{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a0074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005ce4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c902a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0892875e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchvision.models' from 'C:\\\\Users\\\\John\\\\anaconda3\\\\envs\\\\ml10707proj\\\\lib\\\\site-packages\\\\torchvision\\\\models\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d42ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36de45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_train = np.genfromtxt(\"Data/kaggle/mitbih_train.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408bba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87554, 188)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mit_train.shape #should be 87554,188 7*26+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb24a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_train_x_complete = mit_train[:,:-1]\n",
    "mit_train_y_complete = mit_train[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dded89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "train_samp = np.random.choice(mit_train_x_complete.shape[0],replace=False,size=80000)\n",
    "mit_train_x = mit_train_x_complete[train_samp]\n",
    "mit_train_y = mit_train_y_complete[train_samp]\n",
    "\n",
    "mit_val_x = mit_train_x_complete[np.setdiff1d(np.arange(mit_train_x_complete.shape[0]),train_samp)]\n",
    "mit_val_y = mit_train_y_complete[np.setdiff1d(np.arange(mit_train_x_complete.shape[0]),train_samp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae8fff2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 187), (80000,), (7554, 187), (7554,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mit_train_x.shape,mit_train_y.shape,mit_val_x.shape,mit_val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac93940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 66214, 4.0: 5850, 1.0: 2033, 2.0: 5322, 3.0: 581})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(mit_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f2f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9739f966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24c11e01040>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(mit_train_x.shape[1]),mit_train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1a58902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24c094b9070>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(mit_train_x.shape[1]),mit_train_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf72376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mit_train_x.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbaea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mit_train_x.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c33e870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyts\n",
    "import pyts.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d8090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaftransformer = pyts.image.GramianAngularField()\n",
    "mtftransformer = pyts.image.MarkovTransitionField(n_bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f99597c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans1 = gaftransformer.fit_transform([mit_train_x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aabfcc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 187, 187)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26c1d441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b8eaa25e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABSvElEQVR4nO29e7BtW17X9/n9xphzrf06995z+766AWmgaewGbQJF/iBYqFHRpERM1CYpJWIEq6BSqbKMgCm1JFSoRIKpMmKa0hKrFKSCKEUhSlFEMWgElPDuprtpum/f2/dx7nnuvddac47xyx/jMefaZ597b5/HPfvsPb5Va6+15pqPMeee4zd/v+/vJWZGQ0PDxYU+7AE0NDQ8XDQh0NBwwdGEQEPDBUcTAg0NFxxNCDQ0XHA0IdDQcMHxwISAiHyViHxQRD4sIt/yoI7T0NBwb5AHEScgIg74EPD7gOeBnwW+1sx+9b4frKGh4Z7woDSBLwM+bGYfNbMN8APAVz+gYzU0NNwD/APa7zuAT8y+Pw/8x3da+bHLzvafO+DVmwd0Nw13OEAI6UczTtdVDJDp83wlgTtsdDsk/3kzGpEIIrK93eztjsOcj+n1jiOzFe3kstl3kfRBBFQwr8ROCQuIO5Endo551t/iU+M+V2/s0d809GgD8Q7HfjPnPvs/vN7pNpxd3LDXXjWzp04uf1BC4LT7ZOtOE5FvAL4B4LHndviCv/n1fOrHP5N3/NQN5Jc/TAzr0zZ7/SPKbDLXyVJ+V8Q5xClmBtEQp8j+HrJYYKsVdngEqoj3WAjYZsDGYXYGM8FjaZ/5XO44rGJuiUj+HE9bKY8979Pi9vFkUthEBJyb3vseeeyA4bknuPnOHa58kfDUF7/E//z5/5hv+9Af5eY/e5a3/9Rr2Ad/E4vjHUZ5ypjq8W+He/Iysrt7x3NuOJv48Y//jd86bfmDEgLPA585+/4ZwAvzFczsA8AHABaf9Zn2qR//TN7+07dwz78Cjz+GHOylm9A50BOTTBVzMt27CuYceIUYkWCY1/TK25oKceEIC0UiyBiJvbK67Bh3BH9kLK8GTIWwFCRAd3PEH41INCQYcjwg6w0yhjS2LExumyxVEAk4ndZxCs5hTsEMiZbGp5r2GeO0HWDeIcM4HQcgRmwc8z4jjCPxylX89ZtcfvGAvRffxqsvPss3r74WfvoJ3v4zN5EXXkEefwzp+3QtRSDkY+XjV4GZBZKV9QrKuQA33nOZo6eaY+mRw/ecvvhBCYGfBd4lIu8EPgm8H/iv7rRyd9N4x0/dwD3/Cnb5MQ4/53GOnnJIhNiDlfstPxijE8yBWFpmCrEDc+m7Dul77NL6Jum3sDRib0hMkzx2Rrg80u0MDEcd7qrHHMRlREbB3+zxhwskggbwR+APDR1BgyVhEkkT2tJxypggHTt0ktczohdiJ8Q8dol57I60zzHvR8FEiB34VTINyvoSDLdOAkQ3EbcK+Gsr5NpN7Np1+l+4yXNXnuEFucyz/+Ym7oUr2NNPcvg5jzPsaboeChLSPqNL18ZU6rhN8zWfW1t5PYCr7zEWb791/+6WhrcGb6UQMLNRRL4Z+OeAA/6umf3KndZ3hwPyyx+Gx5MAePV3eI6fCxAFW8Q8s0izDMBHcHnWGYgztIuoRsyEMCrOR7p+RNUQwGlkf7nmoF8zRMcQHLvdhs87eJWn+pu8tLnER28+idfI4/0xq+D51OElrh8vCUFZj8qtox45dMggSJA6kSQKWxTFTEOJPg8/CuYh9oa5PLGjYGrgSPsc8zmqYWJYb+ixZkGTjzUKbpUEiw7QHRo7V5bsvrCk/4QSPvki/Nohz+wtcL/+W/DEY9x61+O8+oWe8SDb9VkIQJ7wfhoTln43tYk2trQOmv4P733383zFkx++9xul4S3Ft91h+YPSBDCzHwN+7E2tHAJxXCEHb+foKcfxc4H+mSPMhL4fUTFUjGCCAN4Feh8IUTETvAss/UivgYhwPHTsdAM7fsDnu713gcv9EQd+RTThOPRc8sd87vJlnvI3eKW/xOP+iE4DT/hDjsKC55dP8PJqn030rEfP1dUON4+WjIMjRiEGwaLAXAgYk7ASQ/qYBEAQtA+4LqBqmAlmoGo4FxkGRxhdmuwKKkbXj6xXHTHv36LAqMhaIQi6EYajpF1IWOAOL6FXrxNu3KB76TphvUYO9jh6yrF6LmB7I6KGqBHHzGc4Q33ESR4ToBqT8BSQvNy5iNOIivG+x5/ni3c+dn9vmIaHhgcmBD5tZFJPImnSWJpgMQrikgAoN6mZVAEQoiCihKhEjYxRCSbE/BpxKEa0yBAd0YTBHIMpgzkCSkQJpO9EGMwREYboGPM2kXT8OBtbFQB30gQk/WazSWxRMYnpswmRiIhiUaeJLmlncXYMy9oEMe1fTr4MxKwSkeYUEUHqNc3XtVzvOHk2LErSCAAsjTGNq/xbkldEFUK+frEFm54bnA0hYFkPdS5xAItI34/EKCz7AZfVUMtP2N6PdBrrRPcaWfqBpRsZTfEa2fEDu36DkrSIhY5c8sfsuzWDOToJPOaPuaTH7MqaPV2z79Z0EjjQFZ0ELvljjrsu7VMi686z6j2DGFGV6GfC4IQmYAaihuuSiRKDVBPFuVgFiqqlJ6wmbQCT9LQWY5GvQYxa9xlVsxBKKkMIEHoh9IJ1Ds3knQwj0Qy8EnqSabEIiGatymWvhRreb2snzhVNYCI8vUa8S9LtwK3Y1TUN5wNnQgjUW00lE1LpRhVnuDxJYBICTtIyMUFNcBrTTaoBIowS8RLwElFJqq7XgNdIl82DKJIFRKSXQCcjnQQ6Te+BJEy6vE+voarDqtnVF5R4grQkOwKSh9IQjRA1qfh5cqkkmx80jd0FokmeiNQJ6LJaDhGywBNnyV4n8wn5mpmSPCbFnVjY/8L0ZzNAJb1L1hjK+aRjJ0FQxqgzITD/P3QScHdyKzY8cjizOt38BpxDTll+8oZ9kFCxU8dQIad/rjFGs21P7me+zuse4/WOecKdaiKvH90jJY5h4gC2x9TKz513nAlNYAtZna4cQH4BREv+PidGzMur7R8VLzPb3TStjwKxfo+kV5jJv0BeP78HtK4/5xfstHFlD4WZIFhelk8lpt+N5NI3o3IcxRwonye+gWwmsHWM+bGmV/aOZO8BcRZPkIWBmG2tn+x+m8jLrWNLtv8jlq9xWS/apIkFO7PPjoa7wNn7b94hErc8ke70xL/z8m21VTH0DlGIxTx4E8O7ffnsiXpy2XzD+YP5ZKDh9DR+3SHMNshcyae5/qk/yekayMnr2jSD84ezpQmoEp2AT3aymdD7EZdvvPK+8COdhsyuC04ivY70LuBzyO3SDyzciM+TeqEjCx1Z6oCzSERY6kAvgaUMLDW9OgksZQOattlxA5tyXDfS+5DMfzGCWHqCim5nMhTRKobzMcUDiOB9rBxA8XB4F+gyUWj5aZts9EjnA94HYtSqIYAjeINAYvtLsI8D81qj+gonYDmwCm/V9adqlABi5zIxKFY9Lz4TlXPLwrtAlzmBcp0azgfOiBAobq18wzqrcQCdxkpIxfzI6zTQZzKteAd6F+h1rGq8z4Khq0Jg2Lp5F4UIlBGHTcSgjPQSiCSBsdAp3r5zAe8CIbvXRCAExbJAKj51y6q35BgA1bSec5HeJ4IxmlSyzWskuLTMSMJOxOhcwLlYUyIk5xVEp8kSiEb0hjnJUZQ6IwbzJFVN0ZXO8H6b9S/+/8L8F5W/jEsy1xJN6DTSubTPTgJ9IwbPDc6IEMiPnBIcOIsDiCZInvw2e4+nvjTzA46oIX3PN3xACaYEJHEA+b2g/KY5ZiBYesX8KjEGZVwnbXhMJjO7uAwqDzBtk+IaTnymmPPZ9teIY/peNIT5OyWKcBZNuJXDoHkMmROYNIn0U/ls+bihxC2YIBKS54UUF0AeSxHC6Xq2XMLzgjMiBGbIJFaZJDG7AWHSBBKBN5F1W4KA2SsTeWXbQvzF2QQvkz+to8QZXzDfzxuRg4kYpAY0pXOZTeI4n8RSn7ohCio6219eDlvHqK+834kklJqvIIWBPIEiWKf920S2RsVK3EIhMfN68z2V5dAEwHnDGREChdFO9rQ4w7uASPLVF//8GBUVw0lSYcvE7DXQa2DhRoboEj+gIws3VhIwqfqhmgNBNKm1eZmTWM2BTkYGSQFFXgJBhQWS1eZkDhQjQUSIUlj1082Bkmqsang34wSEyRzQFDMwNwdKEBHk9Gc0mQCacwCCYC5lItYkIJezfGYZickciDUIqORY1HBgsVPNgbkZVmIzVCzHCTSC8LzgbAiBzKqZc8QOtIss/UiIytIPiaiaCYFCAs6FwI5LRGCvIyqRHTewoxv8jMza1Q27uiagqER2dZ1IQQnsSfqtk5GlDKCwqxt2XOIRjsXY9RsOfQ+AC44QYwpXtlOEAIkz6P1IjMqoxrIbE6mZhcAYc0BStrVrnH62x5d+ZOgcY0iqfYjKqPl4QZJFMErNmIydIjkt2MaAiBA6rde0y3yEy0lVkITQokvkazJLkv2vYltCoNdA5xKBuJSBRSMGzw3OhhCANGN8IrFUY0oG0sjSjSkSEPD5iXqSBFy4sQqAwg0Ub0CnExFYyMHBHFEkE4UTMVi+94U8LOuTcgyKxmFOUDGG4GYTfxICc4Kvc5GoqR5B8gSEmugkYjUqMfpso+ftVIyFG1k5X8k5yRNVNSZHZ4yYU6K3lBLsZ5pANgtMU+qyaiIlZWtyQ+diJV/L2DuXoi3n7sByzYHsVWnE4HnB2RECibmqfEAkPSlH02qcxvz88rYd/FNMgGjCaI6x2PtM26pa5gAk8wD5cw6VKN/BExAG8zPOQLc4hzHqFkEYYnYRbtnUieArpFuYbVN4hjmfEXLi0/Z2Wl8xE4xTHkHKWZD6slMLnIjNyVbB6cRFQNIugkb0xFjQKaYinbOhJAGTEq8aL3BecDaEQAmkCYYOEEbleOgIluzwUbZzB4DJC4BUEyBaEhrHoauBPyqGIxJQljE96QfzrMzjiKxix0o2rGLHUVzQSWAlPSvrOIo9x6FjMMdx6FkFz/HQsR594gXypCzRduXJGasLUZE81hgFFccqmzVGmoAu8wHr4FgPPscJJB+9irEZXRUyMSrjqIRRsaAwKoyCDlNREkKqeiTeE82QEOs13Ywe1YgTZQhpQhetIGTBYyYEF3DZ9KrX203egVXsWJUKIw2PPM6GECicgE/2q/ORnW4gmmzVBIg5CmfpB7zE5AbMQmDHJZ9+NEUx9vy62vOwzQkMFlDLnIAmTiD9vqaXwJ6ucRYrJ+AtCZPDsWenG5IpELXyAuWJXtT2og2oGItuTPZ/UBbdyDJzAnG2TadhyydfzIHdbsMw0yBCjIg4YlCis8QJdELsMy/gBbzPnMCYSEs3XdNFN6CZjCzkoHcpxmJOtHb5+zyyspuZA4lLaZzAecHZEAKQbtwsBLp+ZMcnIbDrNzXqrzyJJvt/4gR2dMNCx0r67biBA7eqN3KZ5Hu6YbCAs8iebioxuJSBPd1UYlBJQuLArRjM4ejZ8xuOcnqyi46gMZkUmRysQsCKJmDsdgNjVJwqSz+y2222JpzXlPGoMyFQJuCu37AOvu4/ZOY+RiEEZQTCoJkUFGKv4F0KGAoxRWB2SvTpmi4zAZjIQVdJwBKBWcZePBZzIdC7VLSlXMtlCx8+Nzg7QoDs4pLsSpNQC4JM8fxpgnuJdBKJudqQYltpv8WNpRhdfnqpxEQC5n0FFCexurocyfXoSvoxQifJzFBTOg10GtBMmPkZkVbTmsUSLzBfnid3cbmppHOLKCNax+4tMuTkH8XqdsW7oSYQlSBT2q8AiKWQgVITsBYMLS7C9JtTqxpGcrNOZddcNpsgUShFABRCtgi48n9o7sHzhTMlBCDFwBTLvxQEcfWp8+YY6dNy3U+7cZUkBPTE747IkEkwhzHMt5FS6ow6UUtAEifWK+82W2duZ88nVjlfZNrvyeSdk8tELE/8O1+Lk6nEMtvHybFsHytu/abYLIej1RU6Tzg7QkC0Br24nAsQLbLQcfZESrdecv8NlbXvsk2/kFRYNKA1IUjz0764/pYysCGx231Ww10m4YoZkNafAolyNnIOQgqMeTyKMYqhM3MAYMxhw8XeL8s7TXZ1MgfizBzI5oGTLXOgL9pHZujnJoTk5CVxqUho9JIqHTlFVJKnQHLBEWc1HsFL0kiiy54WmeddJPdqKcAyNweKCQZkt2rDecFdCwER+Uzg7wPPkh7RHzCz/11E/irwZ4FX8qrflouOvv7+nEt9AZbG/nLN5f6IITou+eOqEldOIGcDhlwDoJPArm5Y6EC05AUoJGAn6cbtJfC4O+JAjxnMs7SBPV2zJyO7IgQZOdBjHMaBbhhMOXJHbMwxmGehyUuw6T0LN7IOntE0uzHdHYXAfrdmjMomenb9hj23YeFGQvZueA0sdOQ49ByHMU/ygBNjz22y23NyUa5cV4lJr5EYhXHHExYwLhX6LvU2GMdUrm2ROhMdLNc81h9XobMK6V/vNbJ0QzKvmASDZpMLqORrSabakw1LabrAecG9aAIj8OfN7N+LyAHw8yLyE/m37zazv/7p7EycEhZK7I2Dfl2rApe6f5BuRsVqNmApBpKEwDoLhmTLFxKwcABLGTjQY/ZkIMjIYI6ljOwKdKI5ajBNhl0JDBI5sGM2zmWXYhIC6+jxsmDHuVSIdJZcVIVA1lgU46BbMca03Z5PdQxPEoM1pHmmYahELvlVjYMoMQp9zg4cosPlgKWhj4SlIyyEuOhSB6VxRBcLwkKTYO3XPNav6gRfuCwEJLLjNrWkWtGsvMYts6rTwEIm78BCzo4S2XBvuOv/pJm9CLyYP98UkV8j9SC82/3lyrlyoirwpHgWIeCskHtJCKjEWi04mDKYZ7DAYJMba0OazEFGAsIGR0dIaflmhLw/TIkSclVdn19u6zXatgAoT+pU1Xjyp6sYm+jTU9+UMToGdZXaKNWN0fS5nLfXgEPqsjEHOY3R1UClWlU5VyCWQOqUFOMUMGQxd09KHowhuhQEZOkap0HCmK9xqa6ULzZxVt1EzSpPktabMyUNjzLuizgXkc8Gvhj4f4EvB75ZRP4U8HMkbeHqG+4kGjJGJMAQUnDOkJ/y5WYs0X0RYSFjSgfOUX2QbtqAsjKPWsRZnEUECktLIcNJIDgGHINBEGOwHAknkY0pA8rGXNUAVrFjHTuOQ886eNZxZg7EbQu52O8+ezLKekpS5aOWsQujOHpG1tGzjq5ORMXw0nEcuqpZjFFZhY518AxR2YyOcXSpB8GYAoZkjFjMSUvRkBF0FNbBs8omzNwcKPv2Uia4EFSIopUcrNGZucpI4lQ2d3GnNJxF3LMQEJF94IeA/97MbojI9wDfTgoB+nbgu4CvP2W72pB0yS7ilNgrsUtBMpf8MYM5HvPHt5UDeyNzwGUffzEHlFiDgJYy0hEYSObAcmYOLCUVIVlmMnFP16wscQidBFbWccn3yY6PfksbmKOo7l4ie36dNABTdtzAvlvXeIaSQLTUAUesdnmJG9h366p1FOKwmBIhpmjEYTGw6XvCIpUej71HMicgzhF7ISzSNT3o1sm9qoFOu3QDnGIOLHSskZYldmGRqzBDMq26Rg2eG9yTEBCRjiQA/oGZ/WMAM3tp9vv3Aj962rbzhqSX9LLJ/h6ry45weeTzDl7lc5cvE1Au6fHWE2leIrxoAE4ie5ICfQLKKnY5oGVI/v/satzLHEAABoOlwIF6FtLhdAAGnAgLcQwWcRyxJwObbGoc6DEHumJlSSsICEP0WyYLUDWUTgK7bk3MjU4msnLb/u5k5KhbsLKuCjWVyJ6uudbtpoSnrPEcxZ4biyWjOW6NPa8u9vl4VI5WjtU1x+bJJTsH+4wvv4rs7XL8hCKXV3zewSs829+g05QwtYpJCHQ6ZuE31gKrpcpS4VOAOk6Ap3TNQlpX4vOCe/EOCPB3gF8zs/9ttvy5zBcAfA3wy29ib8hiwbgjdDsDT/U3ecrfIKLsyrpm9ZWklTK5y7JUF3DKcV/JJj/ZC9EGTmBXhE40cQBidKIspKPLqnBQy0FCjg4j6oAyEIGVuWn/1lWuYGO+cgAF88mUwpR9FgLrGpUYc/Wi4ro8jBtWliZmegInIdBJqEJmY551FnCDOXZ0AcD13SWHuzuMO46w42DRp2ChviPsCIudgaf7mzzTXa91E1axS5mROUKyaFaQPClK3BYChPp/WAq45h04N7gXTeDLgT8J/JKI/EJe9m3A14rI+0jmwMeAb3zDPZlhqxX+yBiOOl7aXOKV/hIBzRMhPYFq7kBO+f10NYEgI0sJM00gZA0ABgscxZA0AYxoxs1o3Ixd1QSuhH1eGS9xFBdbRGGY2fFFCyjj2nerqi3sunXtblQ0gRSfsOEobmsCAAfumNfG/Wr2FG7i2rDLYMrhuODV1R7XDneQQ5e7JgdstU6hw+sN/si4ftTzqfVjOeBnEgJz9+rraQLBtF5zgN/mbxBOqWDU8GjiXrwD/5rTY9XeXBPSk/s7PGJ5NeCuej5680ke90cM5u7KRXgUF6/vIszM+1JGYCCocRQDL4VuchEivBJ2uRL2q4vwlfGAlzeXuBUWt3kITnMR3okTKHEPtaiIBI5DMjEKJwBwya94bdjbchEeh44bmyVDdBwPHTdWC46u7rB4zbF8zeiurrBbh0jnsaNjltciXO356K0nOQx9dRFu4ptzEZ7GCXx29yrP2FSAteHRxtlx9uoUMTjPAyitwWDiBIp9qqSagKUkWHpS+Vm035QrMEUCRjAl5GAYJ4JDcCJ0EvPTMrkNSz4BjNXs6DSwsLG69oi5VgFzd1pKa+40pOPl9bpcFq0EOqFT2bNBHYFQJ2HJf/A5YjGUaL68D4BBU6xAaSVuxe5RrU1ITAG1GiTUadpn0arSGHOEoKXzqMRgvh5RZOv/kCoNt3oC5wVnRgiI94SlEJeRx/tjHnPHBCdVfYbJRbiUDX1+cgHVG9Bn7WAlffYEDJVU7Akc6IZdCUQJbExZSmQhjk4cC4xdCZU7GHLkYCkw0ltgcEmtX8jI2pJ3YMjRggGtdQvmcQKlAeoYlX2/zqp3DvjJjVE7CSzjsKWiK8aBW1XzosRN7DiPSmSMLoUwR+XWciQsO8YdiAuP954YjqDvGBeCLQOP90dc7g+TFyKXYytP+KpZVcEwZhNqmxhcSjKdFhLoZPEW3BUNbwXOjBCwEFLAyyisgt+aEDU+oHT1UIhMnEAnI5pJtaK6O4v56T+1NBpMGSSl/w5oSg6ymEhAMwYkv6ekocG0koCbbI8fhQVHsa8TuwQPzVGe2p0G1pnYG6NDQ0rECaJTjIMoQSUXNUn1CwdcfRKvYpcCg7JwScVNuhwz4NkElzoehxwnECKEkPIHQkADMAqrkGIOCtG4zuZAEVjzMXVZOM2JwSha+1WlDIvGCZwXnB0hsBnobo74mz2fOrzE88sn7jp34Cj2p+YOHLkjDrLff2MuFQ/hiKgDN6PxStjdyh34VLjEy+NBFSyvDge8tLmU7PcTuQNzzEOCd/2mBhQt/fAGuQPdbbkDr212t3MHQsetzYIhKuvBc+togd3o6W4I/c2Iu7nGNhuk72Ez0N2K+JuOTx1eqhzEnXIHgBrfUBKXyrJ57sBri09y5G49+Jui4S3B2REC44A/GvGHC64fL3l5tc8YHcddV2PqQ666U27IEl3nJam3RTAch24qKiJTVuDG3FYuwMoSUagM3IwdV8I+TmIyKazj5fGAV8dLDOY4ij1XNvu8vNrnaOwJUdlEN1UbpnYm3yrOcej7Gua70w0c+X6rHqKX1E3paOxZj762VSvJR9fXO1UIhKhsguNo0xGCMmw8w1GHv6X4Q+huReRoRdgMuEv72GbAHwX8oefa8bJ2VPYaWY/pX9+5VKr9ZAl3nXEPZdnCJSFwLe5yaDcewl3S8CBwZoQAlNj31LJrE33NoKuFRm2q95++Z/vbwRhDrQo8mEtFOsyhpinKXTklF8CzQYmQ3YAeGGfrzdbNSUCb6LcEwJ0qC5WOQp3GrAmkun5JoPkpx0CBQC2qWpaZWc4+dFuVhYaghPyKUSEKEkFz7sBU71ynaxrSNR2i1poApdsyUMcNqSzaBvAqxDDFAqgYGku8wmkVGxoeVZwtIRAMDbAetT4VExs+VbiZY+46A7bs5hT22lctgggL7bZyAYp9vMoTfWVdzTEoeQNHsa8C4Dh0bIKrsfshKkOYqg6fLDkeo7HW1JtgDJrLfXtCjFOHpJhy+9fBsx59LUBaqhFtwkwIRGEIKV8gBiWOAqMgI8hoqdDoGFKgkFOIseZjjKNjM/pUSciEzeiJlvo/ArXuoZmknIu43YuwjBeKMH1LbomGtwBnQwjkjptyPOCP4NZRz9XVDmbCuvO1Im6p3bdw28U6i+3d5zp5q+A5HFNNwCIEeh23AnzWMQmEx90hDsuBQAdJMLhETL46HHBls18FwGvrXa6tdlgPPpF1IVUaLr0Ia6vwXE9AcrXgUpF4M3rWg8fXtl+p7FfnAqvBsxl9NSNUjHVw3FotajXjGIUwOsZ1ShpiVNyR4o+E7shwRyO23qRruVgQb97CrUb8ERwdddzMpclUjXHMnhWXmpHO+w54F3C6XcVo3iTltXGfa5nEbHj0cSaEQIpAFmS9wR8acui4ebQkRmHV+9uIwd6H29p7H/q+NvU4Hrpsf29q2fFeA5s+Zf+VEuKXfM+BrgheeWW8xMubS3SatIOjsOClzSVeXu2ziYmFv7ba4frhDuPg0sQMqfY/UWrF5DTQ4pEwxsGlFuJRGLvApnM4Z7VBqOZWZcPgiMGl7sO5Q9B68KxXXTpG7mfIqMhaU+bgRpIAOITuKOIOB1iv07EXaZLKaqQ7NOTQsybvW42YhYA4Q31ud1Y0Gi3di2ddkdwkBF4aLnEl7D34G6PhLcGZEAIFMobk5hqEcXCYCYMYQbXejEBWmafmoKWRhrnUC2A9+loXr6i0Yya2vKRov3XweA2ZIEw9B26FBQsbWchYew4UEnAd0lN8HFyu+y/paVyEwBzlASq5U3FM60YASWZCmnAQS/Xg0RGH1FhEXMy1BvOyImTyfmQQJLsEdQAdDB0MGQKWi46Yd4gIEqZraoNOQUVjvmbRqllQhI04SV2TmIRA6cAMcBy6mufQ8OjjbAiBQquboSEVwYj56Rk1h9uxXcobpk4/I6k3oIrljj2SSLDoUrPPfPOug68VgdbRp3TgTBBWf38kBQJlQVFIwCGTe0UDoLwsF/UwwXK14UkI5ArKufAHIW+buyyVDsyRmVAxSaVPxYhOJ0FTmpyGJACmd7JASAVFrMxUV9qjW11n2g9bgsskXWMrZg25E5SQ26gDpErHAOvYZRK14TzgjPwnJbHZpbpQgJhV7egFMktdW2NLUlWLrQzU5qDFHedC6gtQCDvFtioBlc9bVYNy5Z0hV/AZbeYFyBzASQEgYZpYIrNJZgJqdeJJFCyQOgeRVe+YuBAkbpsVpArBlgVHaUMuRRMIqR25ROp76kRiUzdiLUJgqi6UQp1zEdIiuACiJcGTx26lotBMnplQvQUx5140nA+cESGQkV2EUuzfqg3k3/OEL803LD9RRXL5rFBUV01CwaYuP2Nu/1UFQf3s2VRNILkjaxmxMvmza65oJ8SZAIgkN53V4VED7Sy1DpeiCRh5/k8ahDETKuXpXCoFB93SNIiziV+0gVJaLNlISRMQScfQmWA1qsAyzZ8haWEhq2IzLSZxA0XjAhOrFZ6G6GoUZ8Ojj7MhBKQ8RS3ftJOtbXN7u9ygopiGyspHmRp8hllvwNrrzwTNkX1VE4i5Yk9x1WXhkBqXlsamrgbpFLu4cgB1Uktu+Jl5wfK5TLb5Uzxm4SbTtmBpws5VdS0mA3niT5xAferPhErSBmYxApASicrlna+/9ZLkmNH572V8tnXt67hJWZJNEzg/OBtCYAa57UZl++kklukDqWz21iv/VtyHUrSH2bJ5vEGtT1jq+iOzXAXZigCcPynr5D4xxtuW3fE12fnTCWYV3bJEKeO37f3KiVce5OnXMy+XeszCW2TtZTbuqsrMz6cgb5uu2QkitOGRxpkTAiZMXXXmLyBbsBNrLdPn+iocwIkuPvPv85qFU6rxtG6Jh9PcqquE21I6/pDUY8nsf50SszkkdaB3Op+ktpfYgrK9MR2D3L1IshqeiLrpGtnsM3r6xDSdti3jt8ypGLPBlzHVi8rsxNg6dyenC5yGRxNnzrCT+ZOpvM80gYLqLjxFE4A7P/nLbwVzTaAum12WLU1ga5xy+xhPPLHflCYwh53c7ylPZpuu0ZYmcAfcrglQNYG673qME8c77beGc4ezpQmYVTu3EGLFn16Za8uttQwsavEsJmLcpsi6YhaUyVvi92tG3gn1P2SPgeb2YCnLT2e5AJIiAee2u2V7O09GYy4ABDHLY59vc+Jzqc8Rt1X/dD1mfIBlPmDOMcyFTqwXIW076xkwNyNsvk1+8FevAWyPDZsJYCpxGWwqodbw6OPM/yfnXNf2D3Lb5zLxmb2f1AIK5jkHcXZDl8/lvYT3njqMvG85MbGKvT2fpFsa9J2e3nX9E0/pGbae/Hbi2LetPJGt0/5PCpoTxOvWOjKtM1svNgFwrnC2NIHSQVfJMye5qUr3XYtSbXNRq5qC5BDXVGorIqLVji95B3M+ILUHT3n1itUyZDWPXqZGoq6k2MY5L1DGmR6lplCJAKNqAKm0F8nOr7a21e2SKzDvJ0fyiQmmVq+DaSbyModgaqCS9qlZK8qeRERmT/Q8+UVqy/dSaoy8jdj0PX2RrfHMuYn5/6HT7YIjDY827rXvwMeAm6RS/qOZfamIXAb+EfDZpGrDf/wNOxAZiRF3SuiE6EH6CFFwXUTyRC5PI+cjzsUaJ+BcpPcjnUsTVoBFN7LbDZUk7DSw36056FZsoq9FQHfdml1ds+9W7Pk1nUT23Zq1pPWPx45OI2uNqePP4NKDMUcCmptFCRYNujxRFaxLgUEEMB+hj4hLqoXFLMCc5ZBiTdchT0DtA3HMQgawaIgKkl2FAUNGIfYQe4UcKmyWQ4hjxLwjdhA7sD7OJnk2sxTwEZxNpoZLY5CZeiHOUn4BsOMGdnV9L7dOwxnC/dAEfreZvTr7/i3AT5rZd4rIt+Tvf/FN7WkWLFRiBMyEEi1UbkmJkmppRs0PuxQgFNUIcYoZGHNKrpX8gVzhZ94bMJbehdGndmJKree31fcvTvn79XXCNp/Cn7P7rWgA1U+ft5PJxjbKdhMHQl62FUWYj1UDjyJVGEwxA9OkNdWtYKGyvuUoxjJuk9kY89iL0rJlSQlYaelm0kyCc4QHYQ58NfCV+fP3Af83byQEhBosJDHdpCXGPgZBNJu3mUQzKRGDUzDRqIYEq8FCY1CcTsFCAJu43UNwyEEv0ytFDK6zQNhEPxGK84jBHClYJmQVBFUIzNj4QFonSFbD88SfEW01am8eNgz1GtTQ5JjzBUrkYMkbCKAhqSAlYrCyPbXRKxBm5kYoh5YpVGHGA5gxuTAzexjzok1M5dkazgfuVQgY8C8k6Y3/Z24t9kzpQGRmL4rI029mL8UciF4wn1Rhi4LzEXWlnkAedDUHkkqravR+pPchF+RwLLqRpR8rJ9BpYNdvah8ApctqbapFuOvW7ORae/t+jQZj12847jqG4HJtAM/YhaS6h5QLMHkz2BICZS6bz+ZANg2kj4ibJeuoodkcqJGDLpsDXSSOKfOvBOvEbIZISBckdpLVfdk2B3JjUlSJPpsD3cRB1Ke8GvhkkhRPhujMHCh0hEspzwB7bsNSW1fi84J7FQJfbmYv5In+EyLy6292w62GpLKXHlXOpZu6N1wXsKh0/Zhz21PKrQg4jXnCJ7Xfu8jCp0IjZsJKjKUf2e02lRDsdWTPbVIJcHW1HHhpUnKgq9oYZFc3KKnQ55FP1YmcplTiTefS5A45tr+EAp8wB9JJGmRuw1SQPqJ9IhxLTIOKoS4yiksVfbNgEDV8FxhKzYKssptoIikDQBIAoRdCr1jnoOtgtYIhF2LzLvEsvUGXeiCIGpZTicn1BHL+Vp3wWgjYbLq4LAREjB23Ya9xAucG9yQEzOyF/P6yiPww8GXAS6UfoYg8B7x8h21rQ9LH3NuShuyU6MBcqn5jkp74JZJPXIrgS5VvYn7qpUo4nQu1qMgYlc6FXFk36b1eYy2oSYSYi5WWRialAUetwS/Kwo30mhqahJgr8DjL9QAAYnp6l8m/pVaTJnN+qhsg+Vy0aDFRUJfOMUZJpb7MUnlASZNuzNvnxMNk96thpEYt5pg8BKpojg+QEImWoxodSbtwVouKWLbpCzG5JZjuUFSkaFULTZ2dG84H7qUh6R6gZnYzf/79wF8DfgT4OuA78/s/fdM7NZuCWLKqb5by9NPPyUatwTvFh18+M6vvV6IFax5A3CquuZUjgNbP8wjCsn6JKahJRCUIqb5gCq6ZEYX5s22dk2ydW6ouJLN1syAo9RMKgTjb3zz2YPtltZ6AzcOIq8kyH49Mv+VrliRxOb/04xR3QSVm59e14dHHvWgCzwA/nOPaPfAPzezHReRngR8UkT8DfBz4Y292h6XasOSb3qISY+r3NZkDQiDV5qvVhSRFBJanVinMmaoTay2WOebCpKmJqNTmIaXDzxg1VyVOy8Y4ZRLGKnBmk7oy9eVRPxcIpDz9rczDPNniFOIsWRBEm6n9OUmqEJF10taJPPM4bGUHGrWegMiMbJ22T+Ilb1d2lyMGy3lFEXK1sWoOiCiaU4sHc7WDccOjj3tpSPpR4HeesvwK8Hvvap81+MVyf7+YC2Om2D5x6SacmwMhpsKcPr+iTYU6vcYcHFR+D9UcGMXVZqCdjHX90hYsiKb1Sy/AGFPxTTWiWp10VoMDZArbK3E0ku3vcoK5eKdm9V8yCad5uTnDItUc0Ky6p4uTJnAN5MlkI9UUIHsFSlBB0gpEZCtIqJB+JagomQfJBJjzFCU4qAgB1cks6yQ0c+Ac4exEDKY7jegARyahkovP5wKXRTXtXJrU6YbVWgm3y9WGg2ptvFmKlHqJU2ddhZ6xttvuJbCUTRUAnQSCpj59vQsQIDqhc6Ha77F0GS0FPOYxvjPmvXACCKizygGoFlPAJk4gu+8KKedcnLYv0X+mWBaG5sgcShaeTrY4gbRO4Q4M8YnYE03mEUxBQHrC26InhECqSpz+Dy1i8Hzh7AgBka1Co8PgsKhofroDM/tUCC5WPiDkp1/0kouCuhop6G2qVHwc+q1efI7IUbfgMG44iguOQ8egrjYHPQ49R2OfipcGz2rwDIMjjG6qNDyvCFRQvmuOBDSphUZHcVMxFMsNS6KkXgJjKvFVwoYHgTjolneAUZFBU4zAIOggqdjoSHILFk6gFBodY76mWguNilC9A5ZJzjjjAmIWRPPKQjEXVgFYxVZo9DzhbAiB8iCNMRXMHCGMDgyGwVWmOsbS4GMi/kqd/hLiGkxYD766BoeY8gWiE47DmEqKR8c6OjqJtdrwyjrWsSMQanPQ49CxHlNw0XpMfQFiyBWA55WGi01efOqFgFOSbp95AxMhimYvQOYGXCY6xzRJp+2MoIaN+VjFhz+mSS9RUpDQmIqM6pBjA3K1YXwO5gmxViUOg+YcBYNxKkQaIVcmSkJAXFpHIlOwkMWijKRqw63vwLnB2RACM5JpnvdenoCFjEsPue0KQoW13l52oo6AsPU95rDX5E2Ysgbn6cV1vZnHIbn1yvhksgDyxDdmeQRlmc08BuWJbla9Cxbzk3/Lo5Alyimehts/l2PVC5TPubD/Nm0Dp3yeXe/58Ys3YKZ9levdQobPF86IEMg374zEKuSYzOzTEiykajVj0Cw1KXUa6zLNnwsxWJpw+txp1+d4Ap/tf0dMTUqwygkMuNwdOGZiMPET4nJp75R6l/32U0kwyxOohucWYg9Jvvpybmb5/BIRV/sBWFknR+xpvj7GlPdf4gRmpGAiVWeTM8Sta2ouHY9CEpY6BjOysKYlz69//u9MDUlsui4N5wJnQwjMHmAmUhN+QLZKh5V7XLP670gh8PX37ApU4bbmI0oSHKlLsdSU4tShKHUuLt2Kym9OShDNVJpMgJoSLExuwaLMlIds/X37N9E8+UuxtOrkz5/JYbvCduoymRhUS9GHRTfP+68P57kGAElozMcxHxfkjMUcQBTLcagCQLI5kIRvmviOmNu4NpwHnA0hAElFzmmv1htdPxKjsOjH23oRdj5sN9DUyNKPLNxYm3nudht2/TxsOLDnNlzyq9S1WDr23Zo9Ta8Dd8wlv6KTwIFb4SSmMONunfoYaGQdHOvBp85ATpMqn0ufVxfhXFXXKQfCgqBdxHeh5j1stSETCFomXBJe6Rrk41iOMxhz7EAABMJYwoYFWzjUJS5AxkBMtH69prII9SkfXTI3xMXqtZjnDhTXbLnu8zZk+37dwobPEc6GEMiPVxlG/MrQY609+MpEgUkIeB9mCUSCamToHCuXOwaNqWNQajU2JRAVG3+IjuPQMZrjWrdLJ4HXxn1eG/bwkkyFVex4bbPL9fUOm+jY5Oag61U3EYOndCCCRNpBsvXjOJGHcVSGICkUOHMetZ7AoIkEzMIDsdQ/4djNyMdMDK5SzwMdwB8J/tjwx4auRmxMOQO26JJ3YBhxK9BjITifxqjkXgNgqkRvqfVZIQaLOTKFHCRBlQXM1WGXa60X4bnB2RACSU+uhJREao3/GEtRfCpJFaPOKmclw3YMOmtDNr0KPp0ORMGmeoSjTfvaaj4yc9uVVOKtDkQUXiB9l5BV+VK/v0QC5tOv7sbExiXVv0QjFpfjvI5Avk71PVqKFjzZgSinZ9c6BLMORPXgcbv3QeU7jakXoZRSRDC2iMFzhTMiBDLiRE5ts9UTQ53eJwFghYGnsP6TtyCaoEVwMHkMSn2B4iWYDj8tD0yhx3N35O2s/PbE32Lfy8ItFj5Nxu1cgLkZMQmCejzmx3odL8FJlKIis7Fa9jyUOoa18Mlp+yRHRM7/F7SiIucNZ0sIMD3dahsymybnpAkIIrqlCYRY8gt0qw0ZccodKObAmNuQRZeq5pY2ZMVUGGb5AqUNWYjCbW3IZrkDhQ6Q+YQUSSXB6jY2ZQMWYZAYwK0En1q9Y65tVK1j/vSfv2y7DVnpQFS9FWm/QqkuVMY4e82OX3oSFI0LIdUyILkIWxuy84OzIQQkq9IxIiHVzWPUnPKrNWdgeto5UvBKScgxRtUcb6SMoyLiUpLRzGuwch19CIxRWYUOr5G1edaxyxGCHV4iO85zHHpWoWMTHENQhpAiBcnBO5I5gVq1Z0bi1/JiOqsJGNIkshSzOz11S1rvqOm8jVxANOX8yzgJCIkgoyBjMi9kzMFCuStxMgeKVyCVY5JQArDSvkyT5SWlnkAUrEimSmhK9hpILT9mZlVupGChFjF4XnBGhICAc9g44taGWwmyzgU7ir8dqhAIPrXtLipqzAVGNae+hVGJuRxYIRX9LAEmZNIwmnB1SMTgtWGXG5tlXi+yCh23NguONh0hKOPoGNcOWSsyzNqDlyfsSU0gT6apBBmZ47CcAMQkBJwhg6LDbJnmegOrqR6gGMhIuj4hRQHqmkSmrgOyHrAQEOeIS4+qwhhwm3RNzee5rinakHy4GFL2ZDXDHFO1YSnLpkIkN4Yl18Pug74rGt4inA0hQE57jYXEoj5lLWZdFCZNIFDK3qWnFCkm3sh1B4MSneU6hJmxF2OIU0fioRQRndUXHGJiv8f5OiG9Ypg8AlsdgQtZNxtm9Q4wL/JJrQloTMKjmDipdmAh5nLjkrmgKSZAOX7ep4ZyzYrhni+VzlKJ83FzvlPaV0kCdJm0xLZNmS3hK1P7M6gp2A3nA2dDCGRNAIvoJqYn3CZX0hXduhkRSypsUXuzwIiQSmzHZEpEYIQa8BKj4HMkYTBhMzoEuDX27OiCw3HB8ZASiHoNrEIqJzZsfOIZsomiG0kJOWGalOUpfZomELBqDhTj2txko6PpyStDfjobU1qwkPMEmJkD0zK3ydrAYOgQYRzBIrge6zRd0xDQIV1Tt5EamajZ9DBn4MFCrjOQj286xR1B5mjyvD8eO26G5QO8IRreSpwNIaCC9D2MI24V6A6N4SjX1g/MGmpACYONfiqWUVRVc5qepqNgnaSEmRwhJy4JgiG4mrU3LAZeXewD8OpqjxurBU5SafJNcNw6WjAcdYmgGwV3pPijnMATJoLuNHNA8mSSUaoWUIqCnhQC0ZGyAUdmQiAFAvmjbU1Ax2QCaDB0gO7I8IcBPdpg60168vc9w66n8x4bA/4o4I+Yqh0Lk8BxktORp7GXkmVzL6DllGWA68dLXtnsP/j7ouEtwZkQAuYVeeyAeOUq/tqKnStLYpeFQC/1hpTCX5Ubsk62XHHXp6euDhD7POGyXWvOGHc8Qx+r9rDpez4eleu7S64d7nB0dQcUbi1HYhDsRo+/pfUJ7I+E7jA9eSdNYPb0L6p2mUwKsQdimnSlKGjt/lOeuq6kA9u2EOhTIFAlHvN+/Cqp/zokAdBfWSE3DonHx6AOvXTA8ds8u4/tw9Xr+KvH7L66YLOaXcsqBCD6+fW0WtyF0nW5rOfSttde2+dDO29cRLrh0cCZEAKxU4bnnsBfv4lcu8nuC0skLBCbCQGo9mq6IWXryRW7dJNKNHSkluEu20YvhAWEpUuk2ghhAUcrx+HuDnLoWLzmMDXCssMF6K4L/ihNPBmN7sjoDhPbntj4og1YTSRKB7OqGcQ+RQFqSOXBQ6+3CwHNT/jBtibhJASymy+m4+o6ZCEQ0aNNEgDXrididX+P8PQTHL5DeOzpS/hrN9CrN9h9YUm/35E9qtU8SUVHhOhzPkIWqic1gZiLkwCsL/f8lj75oG+LhrcIZ0IIhAXcfOcOl188wK5dp/+E4g4vpQnRucnnnXkA84q5KRAmaQKK+UQu6mhEL2kClk29MC6VsJAqKEIvrK45xh2HP4Lla8leHnfSpOxvRLrDWCefOxpxhwMyBCTmiJ9QIn9mKN9FUl5/Ie28S+ejOqX+Zp++jBEZ075KNqAtHLoapyjAkCICZT2kbccRW2+Ix8fYOCaT6pm3cetz9zn8nIGbH9/h8gt7xCtX6T/h6JaLnFAk07idYs6Bm+IYcEKtmFQCNp1gPo1r2NvjcNU4gfOCMyEE4k7kyhcJey++jf4XbhI++SJ69XrK3vN5iCXcFcBlZzdkIswhmokws0QkeJ8mYGa3zCksemLvkZgmXOw9myeXhB2HPwx0V1fghLjwSIi4m2vkaJUnXEg293qNhZC8DnGq5LN9QlbHLOX4Zohz4ByaG4SQm4OoUyzMCoIAiKDZbVqEQNrGpvUsT2R16N4u8uxT3HrP27jyhY4v+e0f4Zdefhf7n3gSf+Uq4ZMvpuuU6xCWfUghZefIQkjnzKAKOe+Zy+6z2LmyuIv/dMPDxIfusPxeSo6/m9R4tOBzgL8MPA78WeCVvPzbzOzHXm9fT+wc89QXv8SrLz7Lc1eegV87JNy48ekOaPpc02ynqDbRdLOL9+lpGyPiHDsH+7DosdUau3UIqnjvIQRssyFscqcdO+WJf/K4p8BmWsGpAuMuzlHKpHU90vfopQPC009w83P3ufKFjuUXv8Z/+9xP85d+52WufPJJnn35GewjH8M2m63xbI3v04D/kOPSiy2B6LzgXqoNfxB4H4CIOOCTwA8Dfxr4bjP76292X8/4W3zHu/4J/93v+RO8yGWe3lvQvXQ9+brHMBXIKDZ3yG6DkiQTIzaGNFEB8T49QUOsy4iGjVOWnYhg48j48quUEr/SeYhGDEcp0q/vcZf2kzBxiiwWSWB4l7QR1UllnguDWU6/DGFL5WcYkRBzu/Dix0/dg2tJsGw+yBiwRVfP00TAK3Hpk93eKcOu5/htnsN3CIefM/Alv/0j/Onn/jW/e+cWf+ULfpS/cPRf8ime4qkndtFbm2qGFJNDspZDjFvjLolHFTHW/0O8dp3w6rwHbcOjjPtlDvxe4CNm9lvyBk/G0/DSuM//+Bt/BH76CZ75tzdxv/5bhPU6dRt+k0+q+XFL5x3mlXay2quLRZr0MannsrcLfQfrDXZ0nNbtuyRkNgNWNIEYiTdv3XasrWNsD6hqHHUSZ7W+nJPMtIPTrls9j/lxRFDVqsZ3PnkBHnv6Ejc/vsMvvfwu/vLvvEz4gh/lf/rQf8bO/7PPM//mGnzkE0m7OZFlWL+fwOtpCO5tTyLPPnXH3xvOKO5gD9wvIfB+4Ptn379ZRP4U8HPAnzezq6+38dUbe9z8Z8/y9p+5iXvhCjzxGHKwl55SXitJJaWSrpOaIQeACKHTXHGH1ILLKbFLxKCV2IKFEhaaIuzG5EY8fkIJO4I/NJbXYyIGFylwp7sV8Uchrx9xqxFZjUgIOR13MhEk5oo/5elZSD/v6pMV1do0tKwjcy2hkHUiSetxDhnGafusNTCGyn3YGJIb8NoNLr+wx/4nnuTKJ5/kfzj+L1j+6wOe/Zkb6KeuwOOPJa1CsgYyFwZOq9Aq11NqKfWMGVn42hcccPR0SyB65PCghICI9MAfBr41L/oe4NtJDr1vB74L+PpTtqsNSRc7j/P2n3oNeeEV7KnLHH7eExw+43KcALWlVhEC0Wd31SwoJ8UJTAE15XtxEZqDsDTiIkUc6piadNrlNcvdDdcPe+RqnwKPFgGC4G84/JGvYbf+CLpDqxF7KWR3No4aLDQXTtmNWOIEutnYY4kTYCobXlyEkmIM3CoJmJo5GKzGCchoKRDo6jF69QbxlSv4K1d59tVneVGe4tmfuYZ+6gr2xCUOP+8JNvtZKOoU3xBdvp6nuC23cweSmxDg+nsD++94XbnecBbxN05ffD80gT8I/HszewmgvAOIyPcCP3raRlsNSfunzT74m8jjj3H4uU/wyu/wrJ4LKYGot6lYZ9FQT7bSdhHt4lYCkfORrh9xswSig+Wa/X5dE4h2uw2fd/AKT/c3+dT6MT5660m8Rh7vj1iFjk8dXuLa8bImEB0ddcihfxMJRCWJIE38kjsQO1J3YGdT3sApCURWmpj2hh6fTCCSrQQifwS7ry5SHMAnHOGTL2K/8Zs8/dhOMgEef4xbn/8Er36RZ9y3nEDElECUoy+3IgZLMdITCUTkbM4vevcn+Mon78Q1N5xV/IU7LL8fQuBrmZkCpSNx/vo1wC+/4R6iYTH5uYc9ZTwwbG9MT+xcFw+os0w14v3Utce5SOdDbVe+GT2LbmDZjakCMdC5wGP9MY/1K4boWAXPQbfm2f4Gz3TXUYzD0OMlcrk/TOXHci2CISqb0XNTjDWk/gAlO3DWE6BIgq0nah/TJA5gnaX24K5EEpECd3zEBk1hzkANdV4EgvPb9QTGlA1ITLkAKGxWQr/fpTgA57DNBr21SebComPYVcZ9Y9yPuQIyKReiTG5vOaU5n0upijynPko3JOCZ5U2e6a7f/R3TcKZwT0JARHaB3wd842zx/yIi7yPdTh878dvrI1fGNag9/KS05WJSBEqbLCimttUGJIm0j8mkLhWCxfClH2EuIT6a4iXS6YiTiJOYv6cS5KVVueSqw66U3C7NOyyPN094csnx1FOA+pRP6nfuR1Ci9fL2lisLSy45XlOMS7lvtVwTMBGIUsqMF8eCTtGFKRJQKtkohZSU6fciABDL6r9MJcg1n4Ol76Xicf1fK1MvwnyNGs4H7kkImNkR8OSJZX/yrndY+w5Qn/7zdljF1i717xM3mDQDlwVB6kOQqgOXxqSaJ7HP5cXVZDbhp14DKjH1ItCQKxLHWqhUos6EAGmyxOlzKTQ6lRuzaYLFqQdBKeJZZEbpQ1Am9Vx4SL4W5HRqy+taFj518mfhwomGpNM1na5r7T1g2QNQtA6dmVenCYHZ/6H0Z2g4HzgTEYMVIVYSLo5J5Q7OKiFYbO0xr16beJrU+zXkTMEkGFw1B6ITVsGzcL6aA52mCjnltYmeaMqOG1hHn9KJR5/amAfHOGoaV6kgfMIcSLn6csK2lskcEFK2Y5m9Mf0eLaaCHbU1GIlPcJLbnW2bA5qvkc4+S0jXz0KOS9AUx1Bau2kQ4jgrfFrNgXxpi/ZiYDb1LCyIrjRcIV2b1ovw3ODM+Xnkzu7pLZSio/PP89iW21qRvQ4CWluPpf1sb1MKmN4+1uLqk21CsDyEq6nwhkOYDXz++Q3GPt9/cSO+wfq3FUa9436nashTLceG84izpQkU+1apLbkrJyCG5WKizlnuO5BqCjqXVP/OxdouzLtIl00C4HU5gV5Sy/JiIix0JJrUlucA5qS2Ci+dfC2n1s6LcZbivVWtV5KZwPS9cgL5/Oo+S+3BbEaIizmyb2pzlhqbksosupIFmMaH09s5AdWpVZmbcQIlXUDJRCATJ+Amc6B0IFI3NSNpnMD5wtkQApb9bKrJt+8N9alKkPdhRgKmd+8TaRdNMJeqBS26kU4jId+ovQss/Fh75vUusHQDO27DmGfAjtuwlJGlDOzqhh23oZPAUlOU4NINLNxYCTHvIupzS/RSMDTMUp0LIkCeWD65LU0Ab2gXkoAzKdHK2bVZNittyAx1RvRFOCRzwIjEoMkM8cXPTxYGbkoGGrPN7jRlVPrJC7DVMsAZkpuPFO+AVJ6iELJJ6DqXr6WO9Ro1PPo4G0KgoJBYLjHyppMnYKshqRjeRULmBHwWBE4T6Rc0PfU7DbM2ZCOdxEpoeXGZEBy33r2mdYIoXdYeogmdC9lDEHNqc7GR8+ydaQI1x0FsFhOQtJvagJS0nmpEc6FUqZpAbsTqYpqc836HmdE3DAuTFhC9pKi+ghIRWBuSsqWJVMtBQVycmqTmcRbtq2g58/9DuV4N5wNnSwhAtVe3W41ndjzp2YWYn9n91HVPNgwpKD0HUlOR9OgOaH5J/axmhNKmnO125lM78dx/YEamZRvgBC8gs98ns306t3J+Zb3cBsxIxyj7ve1YldSflTM7wQnMQ5PLPi3N6HkjkfRdqgCwzG9MMi1rLWU7SgOXM0cnNdwlzpYQqExUnkN5UtTJn2/cMnHiTFCc7BRU3/Ouy8QvwuB2IaH1N0ityMJsP/N9npyQ04SaCYC5ZjCb5PNzK59nqQHTfuvk45Tjbb9KRaAtM30rtfrkqxxje3k5t4nfkBz6MF2DacOG84KzJQRmcQKqEcukX+EEJNv3PscFiIQUF6BGl0m8EJXgQlXfi5/fayb+sjkQVFhoMgH6mTmw0JFORzpzLDTQa2ADOIt4FxCNSDEHykQrc6NOeijEoDiboojz9xLeHEWSuaJGzL9hM1KuxBTANMl1KvNV1PwUB5DNgVl6db2mpXBoiQQsDU9mJKC4XKXJJjJ2Hp8x5wRS1eZGDJ4XnCkhUKPfsv0ZSQKgRgzmd82sv+QnZQkKKsy/i5o8ARpRLAUB5aYiXlOh0SgpJNgRUfL2eT8lgEilbCOEmARPmbDU+TGp0ttP/kzCqZVAQsjnotkm18jUBrxEDVoOEsrrTRMRrHoYspZQWP+a8DN/+lvyElTBOgmAKepRKglYeQqZArRORmWW/4Mj0tGChc4Lzo4QqNWASGSVlNd08xX1Xctycvm7IhTE8qTP7xg+Z8qkKLeII+YncPpcQoYd03fNyyoxGLSOowyxROpJ6TQMM9Y9S4Q8zrSuJLZdShCO5fBfq9F4EguVkASHaqzNmpN0oF6jGswzXyZSS4KVlGYrA87Xc/7Z8j7ruNIGVQAU78DJ/0N5bzgfODtC4ASkTpbpOyZbN2LpLjyHfho26/xmTpPfXnf7OmElP2FPXYksA078WsJzi2DIEYblVdfJdrjMthdJ3YFTFyDbdvExURGvG/p1+6WqYymfcw/i2865cDFl3SI4G84Hzp4QmJFUp0cFbr+Xz3X5ids4zgTHdlSgbu0jFGJwXqGIbfJwTpDNo+luGz8w5RXPvlcCUOq2t5GCZRez42wRdlveiIR6mDvNy8JTnFyc92NMHoO5ByG5B4U5IVuuVfMOnB+cDSEwT3ZxYD4RUSJSSUCYwoK9C3QlWIiULdhroHeBMRrRJb9+nwN9FGPhxhQroAG11KW4EIMdKUCofpdAFKXXkT7HGkTyWFxKYQbNhH82B8qknzH6hXBLsidFG86jHUU08RsuEqNSOi2LxlwNLf+W27AjJFedy/77WOIEcp6BkylGoRCDNaIwxx3kIKDoEjmpeTyJp0hP/fJ9ro34HIUJVDK14XzgTAiBeqs5rUEtKTAnRemV0N/yJCqegOKy67I3oNcRxRFNZhM4ZiExstCRhYwMOKJKDXqZewc6TRGEKGl9N6LR1YChzgXMIIilzseVJCiT3ZJfn5LpmIKKItOkdhpTTFAm27wLW0/9FJiTznFwrgqXqLnn4jgPHZ7aiJlXaoXlMHkHkoCwGgQkuXmrYFUIFbKyCoFZGnYJyPI5hHqpQwsWOkc4szrdncgnucPyM0lWbXEa5f31OIc3XueNjlM1gYxKDL7RLgrpeRavY8MDxZnQBARwT17mxnsuc/U9xnvf/Tzve/x5BnMcuNXk2892aHoS5WWkJ/pSBpY6MJhjFTt2dc1SBxyWmf6RPdmwq2uiKRscSxl4StcsBX6bv8Fnd6/SE1lIYEB5bfFJrsVdNuYYzPPauM9LwyWOQ8c6dkQktTrPgUUlbTkFGSmdBnbckLogR8+eS/kJJUFpsBy6rIFV7DgOHdEUnxN09v2aq8Muo7m6z+PQcWNYMkbleOy4frzk2mv7rC/3DHt7XHafhf+QI167jnvbk7z2BQdce+/IF737EzyzvJnMIYx1TP/6TkMOqQ6V//CaPCUq6doF062civ9k74O8uzt+K2+RhgeIMyEEAGR3l6OnlMXbb/EVT36YL975GBFlV9e3Zax1EuiJhPyIcxgLCfSSlq3MsZTAsngSSF21l6IsxBMZCGzocCxkFydKsMgzlpqZd7IgEjlytzi0G0RgMLgWe66EPVbWMZhnMEdAGWpKXkIRVk7iJHTMsdSBPV3TEYg5XLkj4CSyso5V7Ot2SmRP11wLewSSAAgoq9hxPewymONmSN2BP7TzNL+lT3K4WrJzZcGlF/cIr76KPPsUR08rB2+/zlc++SGe6a7XqkmlHoAjVvV+Pu4SB1CEQD/LF3h3d8zbXGs+cl5wZs2BhoaGtwZNCDQ0XHA0IdDQcMHxhkJARP6uiLwsIr88W3ZZRH5CRH4jvz8x++1bReTDIvJBEfkDD2rgDQ0N9wdvRhP4e8BXnVj2LcBPmtm7gJ/M3xGR95Bakr03b/O3crPShoaGM4o3FAJm9q+A104s/mrg+/Ln7wP+yGz5D5jZ2sx+E/gw8GX3Z6gNDQ0PAnfLCTxTugzl96fz8ncAn5it93xe1tDQcEZxv+METotNOz3ZbtaQdCnN59zQ8LBwt5rASyLyHEB+fzkvfx74zNl6nwG8cNoOzOwDZvalZvalPYu7HEZDQ8O94m6FwI8AX5c/fx3wT2fL3y8iCxF5J/Au4N/d2xAbGhoeJN7QHBCR7we+EnibiDwP/BXgO4EfFJE/A3wc+GMAZvYrIvKDwK+SuoV9k5m1nNOGhjOMNxQCZva1d/jp995h/e8AvuNeBtXQ0PDWoUUMNjRccDQh0NBwwdGEQEPDBUcTAg0NFxxNCDQ0XHA0IdDQcMHRhEBDwwVHEwINDRccTQg0NFxwNCHQ0HDB0YRAQ8MFRxMCDQ0XHE0INDRccDQh0NBwwdGEQEPDBUcTAg0NFxxNCDQ0XHA0IdDQcMHRhEBDwwVHEwINDRccTQg0NFxw3G1X4v9VRH5dRH5RRH5YRB7Pyz9bRI5F5Bfy628/wLE3NDTcB9xtV+KfAL7QzH4H8CHgW2e/fcTM3pdff+7+DLOhoeFB4a66EpvZvzCzMX/9t6R2Yw0NDY8g7gcn8PXAP5t9f6eI/AcR+Zci8hV32khEvkFEfk5Efm7D+j4Mo6Gh4W5wT12JReQvkdqN/YO86EXgs8zsioh8CfBPROS9Znbj5LZm9gHgAwCP6ZOndi5uaGh48LhrTUBEvg74z4H/2swMwMzWZnYlf/554CPA59+PgTY0NDwY3JUQEJGvAv4i8IfN7Gi2/CkRcfnz55C6En/0fgy0oaHhweBuuxJ/K7AAfkJEAP5t9gT8LuCvicgIBODPmdlrp+64oaHhTOBuuxL/nTus+0PAD93roBoaGt46tIjBhoYLjiYEGhouOJoQaGi44GhCoKHhgqMJgYaGC44mBBoaLjiaEGhouOBoQqCh4YKjCYGGhguOJgQaGi44mhBoaLjgaEKgoeGCowmBhoYLjiYEGhouOJoQaGi44GhCoKHhgqMJgYaGC44mBBoaLjiaEGhouOBoQqCh4YLjbhuS/lUR+eSs8egfmv32rSLyYRH5oIj8gQc18IaGhvuDu21ICvDds8ajPwYgIu8B3g+8N2/zt0ofgoaGhrOJu2pI+jr4auAHciei3wQ+DHzZPYyvoaHhAeNeOIFvFpFfzObCE3nZO4BPzNZ5Pi+7Da0haUPD2cDdCoHvAT4XeB+pCel35eVyyrqnNhs1sw+Y2Zea2Zf2LO5yGA0NDfeKuxICZvaSmQUzi8D3Mqn8zwOfOVv1M4AX7m2IDQ0NDxJ325D0udnXrwGK5+BHgPeLyEJE3klqSPrv7m2IDQ0NDxJ325D0K0XkfSRV/2PANwKY2a+IyA8CvwqMwDeZWXggI29oaLgvuK8NSfP63wF8x70MqqGh4a1DixhsaLjgaEKgoeGCowmBhoYLjiYEGhouOJoQaGi44GhCoKHhgqMJgYaGC44mBBoaLjiaEGhouOBoQqCh4YKjCYGGhguOJgQaGi44mhBoaLjgaEKgoeGCowmBhoYLjiYEGhouOJoQaGi44GhCoKHhgqMJgYaGC44mBBoaLjjutiHpP5o1I/2YiPxCXv7ZInI8++1vP8CxNzQ03Ae8YbVhUkPSvwn8/bLAzP5E+Swi3wVcn63/ETN7330aX0NDwwPGmyk5/q9E5LNP+01EBPjjwO+5z+NqaGh4i3CvnMBXAC+Z2W/Mlr1TRP6DiPxLEfmKO23YGpI2NJwNvBlz4PXwtcD3z76/CHyWmV0RkS8B/omIvNfMbpzc0Mw+AHwA4DF98tSmpQ0NDQ8ed60JiIgH/ijwj8oyM1ub2ZX8+eeBjwCff6+DbGhoeHC4F3PgPwV+3cyeLwtE5CkRcfnz55Aakn703obY0NDwIPFmXITfD/wb4N0i8ryI/Jn80/vZNgUAfhfwiyLy/wH/F/DnzOy1+znghoaG+4u7bUiKmf03pyz7IeCH7n1YDQ0NbxVaxGBDwwVHEwINDRccTQg0NFxwNCHQ0HDB0YRAQ8MFRxMCDQ0XHE0INDRccDQh0NBwwSFmDz93R0ReAQ6BVx/2WN4CvI12nucJj9J5/jYze+rkwjMhBABE5OfM7Esf9jgeNNp5ni+ch/Ns5kBDwwVHEwINDRccZ0kIfOBhD+AtQjvP84VH/jzPDCfQ0NDwcHCWNIGGhoaHgIcuBETkq0TkgyLyYRH5loc9nvuJ3JPhl3IPhp/Lyy6LyE+IyG/k9yce9jjvBnfoR3HHcxORb83/4w+KyB94OKP+9HGH8/yrIvLJWX+NPzT77ZE7z4cqBHIpsv8D+IPAe4CvFZH3PMwxPQD8bjN738yN9C3AT5rZu4CfzN8fRfw94KtOLDv13PL/9P3Ae/M2f6uUoXsE8Pe4/TwBvjv/X99nZj8Gj+55PmxN4MuAD5vZR81sA/wA8NUPeUwPGl8NfF/+/H3AH3l4Q7l7mNm/Ak6WjrvTuX018AO5EO1vAh8m/e/PPO5wnnfCI3meD1sIvAP4xOz783nZeYEB/0JEfl5EviEve8bMXgTI708/tNHdf9zp3M7j//mbReQXs7lQzJ5H8jwfthCQU5adJ3fFl5vZf0Qyd75JRH7Xwx7QQ8J5+z9/D/C5wPtIvTa+Ky9/JM/zYQuB54HPnH3/DOCFhzSW+w4zeyG/vwz8MEk1fElEngPI7y8/vBHed9zp3M7V/9nMXjKzYGYR+F4mlf+RPM+HLQR+FniXiLxTRHoSqfIjD3lM9wUisiciB+Uz8PuBXyad39fl1b4O+KcPZ4QPBHc6tx8B3i8iCxF5J6kfxb97COO7LyiCLuNrSP9XeETP817bkN0TzGwUkW8G/jnggL9rZr/yMMd0H/EM8MOpZyse+Idm9uMi8rPAD+b+DR8H/thDHONdI/ej+ErgbSLyPPBXgO/klHMzs18RkR8EfhUYgW8ys/BQBv5p4g7n+ZUi8j6Sqv8x4Bvh0T3PFjHY0HDB8bDNgYaGhoeMJgQaGi44mhBoaLjgaEKgoeGCowmBhoYLjiYEGhouOJoQaGi44GhCoKHhguP/B3RALbm6WlSRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trans1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77e4c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans1_10 = gaftransformer.fit_transform(mit_train_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b07725b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 187, 187)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans1_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "033ac266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b8e9c1af0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trans1_10[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c66376a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b8ee51130>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trans1_10[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50fbeff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(trans1[0],trans1_10[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c3a4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_train_x_ims = gaftransformer.fit_transform(mit_train_x)\n",
    "#mit_train_x_ims = mtftransformer.transform(mit_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2949d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_val_x_ims = gaftransformer.fit_transform(mit_val_x)\n",
    "#mit_val_x_ims = mtftransformer.fit_transform(mit_val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceb5ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "try_samp = np.random.choice(mit_train_x.shape[0],replace=False,size=5000)\n",
    "try_samp2 = np.random.choice(np.setdiff1d(np.arange(mit_train_x.shape[0]),try_samp),replace=False,size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7e263f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_train_x_ims_try = gaftransformer.fit_transform(mit_train_x[try_samp])\n",
    "mit_train_y_try = mit_train_y[try_samp]\n",
    "\n",
    "mit_train_x_test = gaftransformer.fit_transform(mit_train_x[try_samp2])\n",
    "mit_train_y_test = mit_train_y[try_samp2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb7b69e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 4158, 4.0: 388, 2.0: 287, 1.0: 120, 3.0: 47})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(mit_train_y_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2aeb3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_b2 = torchvision.models.efficientnet_b2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae0dfd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\" to /home/john/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-bcdf34b7.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030da573f2ab4887b35933fdeb67ea74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36882185.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "efficientnet_b2_2 = torchvision.models.efficientnet_b2(pretrained=True)\n",
    "efficientnet_b2_3 = torchvision.models.efficientnet_b2(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40a0ef9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 4.5455e-02,  7.6288e-02,  3.0048e-02],\n",
       "          [ 5.5432e-01,  2.9662e-01, -3.1593e-02],\n",
       "          [ 7.8309e-01,  2.3778e-01, -7.0381e-02]],\n",
       "\n",
       "         [[-2.9542e-01, -2.2617e-01,  4.4444e-02],\n",
       "          [-1.1007e+00, -7.1409e-01, -5.0454e-02],\n",
       "          [-1.4469e+00, -6.2652e-01, -6.3332e-03]],\n",
       "\n",
       "         [[ 2.2284e-01,  1.2584e-01, -2.9721e-02],\n",
       "          [ 6.2338e-01,  4.4390e-01,  4.5707e-02],\n",
       "          [ 6.7145e-01,  4.5727e-01,  7.3647e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.9284e-02, -2.5299e-02,  1.4641e-02],\n",
       "          [ 5.2415e-02, -2.0318e-01,  1.3882e-01],\n",
       "          [ 9.2027e-02, -9.3667e-01,  9.1768e-01]],\n",
       "\n",
       "         [[-7.7375e-02, -2.6244e-02,  4.9299e-02],\n",
       "          [ 2.1007e-01, -6.1103e-01,  4.3788e-01],\n",
       "          [ 6.8553e-02, -1.9628e+00,  1.9089e+00]],\n",
       "\n",
       "         [[-5.9978e-02,  1.4608e-02,  1.5315e-03],\n",
       "          [ 1.6183e-01, -1.6718e-01,  9.2858e-02],\n",
       "          [-6.7604e-03, -4.1440e-01,  3.6333e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.7562e-03, -2.1970e-02, -1.4113e-02],\n",
       "          [-8.2082e-02, -1.3783e-01, -4.7141e-02],\n",
       "          [-2.7940e-01, -3.6277e-01, -2.3812e-01]],\n",
       "\n",
       "         [[ 1.2188e-02, -1.6955e-02, -3.2000e-02],\n",
       "          [ 6.4081e-02, -3.1379e-01, -1.3303e-01],\n",
       "          [-5.0792e-01, -9.7041e-01, -5.6709e-01]],\n",
       "\n",
       "         [[ 1.1011e-02, -1.5214e-02, -1.5223e-02],\n",
       "          [-2.7066e-02, -9.9056e-02, -1.2986e-01],\n",
       "          [-1.1055e-01, -1.8562e-01, -2.7011e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6226e-02, -5.8355e-02,  1.6767e-02],\n",
       "          [-2.4499e-01, -2.2667e-01, -1.2807e-02],\n",
       "          [-6.2013e-01, -2.0347e-01,  6.3911e-02]],\n",
       "\n",
       "         [[-1.4652e-01,  7.7170e-02, -8.8026e-02],\n",
       "          [ 6.3574e-01,  4.3070e-01, -1.6116e-02],\n",
       "          [ 1.4168e+00,  4.3797e-01, -3.0905e-01]],\n",
       "\n",
       "         [[ 1.2926e-01, -4.9621e-02,  1.2115e-02],\n",
       "          [-2.0691e-01, -2.3254e-01,  1.4823e-02],\n",
       "          [-9.8532e-01, -2.5416e-01,  1.2248e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.5654e-03, -1.4654e-01,  1.0786e-01],\n",
       "          [ 8.8356e-02, -2.7176e-01, -3.5853e-01],\n",
       "          [ 1.3345e-01, -2.5737e-01, -6.2330e-01]],\n",
       "\n",
       "         [[-6.1692e-03, -1.7159e-01,  1.0380e-01],\n",
       "          [ 1.0622e-01, -4.9651e-01, -4.1311e-01],\n",
       "          [ 1.1102e-01, -4.7138e-01, -8.3252e-01]],\n",
       "\n",
       "         [[-2.1845e-02, -1.4307e-01,  6.1010e-02],\n",
       "          [ 4.7625e-02, -1.8282e-01, -2.7660e-01],\n",
       "          [ 1.1679e-01, -9.1711e-02, -3.3707e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.5669e-03,  1.3087e-01,  1.1337e-02],\n",
       "          [ 7.7059e-02, -2.2907e-01, -8.0004e-02],\n",
       "          [ 9.2109e-02, -1.6570e-01,  4.0205e-02]],\n",
       "\n",
       "         [[-1.2674e-01, -9.1433e-02, -2.0497e-01],\n",
       "          [-5.4737e-02, -4.2860e-01, -1.7692e-01],\n",
       "          [-5.6698e-02, -4.8727e-01, -1.1448e-01]],\n",
       "\n",
       "         [[ 2.5169e-02,  1.5508e-01,  4.6205e-02],\n",
       "          [ 8.4115e-02, -1.0285e-01,  6.9053e-02],\n",
       "          [-1.3452e-02, -2.2193e-01, -3.4380e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 6.1950e-02, -1.4591e-01,  9.3395e-02],\n",
       "          [-5.3517e-02, -6.1517e-01,  6.3799e-01],\n",
       "          [ 1.1399e-01, -8.8383e-01,  7.7254e-01]],\n",
       "\n",
       "         [[ 7.9657e-03, -2.5341e-01,  3.1599e-01],\n",
       "          [-3.8451e-02, -1.0760e+00,  1.1028e+00],\n",
       "          [ 2.6621e-01, -1.6640e+00,  1.4228e+00]],\n",
       "\n",
       "         [[ 3.6819e-02, -6.5501e-02,  6.3354e-02],\n",
       "          [ 1.8800e-02, -5.5180e-01,  4.4437e-01],\n",
       "          [ 8.2743e-02, -7.7295e-01,  7.3217e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.5354e-02, -2.0501e-01, -2.6577e-02],\n",
       "          [-9.0355e-02, -4.1631e-01, -4.8026e-01],\n",
       "          [-1.1800e-02,  6.5963e-01,  5.0521e-01]],\n",
       "\n",
       "         [[ 1.2973e-01, -1.7856e-01,  2.5484e-02],\n",
       "          [-5.0095e-02, -1.0308e+00, -9.5188e-01],\n",
       "          [ 2.2359e-02,  1.1624e+00,  8.5877e-01]],\n",
       "\n",
       "         [[ 6.4256e-02, -1.0079e-01, -1.6926e-02],\n",
       "          [-8.2074e-02, -3.5325e-01, -4.9013e-01],\n",
       "          [ 2.0874e-03,  4.3593e-01,  5.4686e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1409e-02,  6.6383e-02,  1.0740e-01],\n",
       "          [-1.8636e-02,  4.7435e-02,  1.6486e-01],\n",
       "          [-7.0551e-02,  1.5408e-01,  2.0916e-01]],\n",
       "\n",
       "         [[ 2.1353e-01, -3.0290e-01, -4.5801e-01],\n",
       "          [-1.1888e-01, -7.2436e-01, -8.2227e-01],\n",
       "          [ 1.0874e-01, -5.7593e-01, -9.9738e-01]],\n",
       "\n",
       "         [[-1.5124e-01,  1.5392e-01,  3.7335e-01],\n",
       "          [ 9.7860e-02,  8.2027e-01,  8.7589e-01],\n",
       "          [-6.5503e-02,  5.8365e-01,  6.1670e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0703e-01,  1.8028e-01,  2.4543e-01],\n",
       "          [ 7.9443e-02, -1.6422e-01, -1.7709e-01],\n",
       "          [ 2.3077e-01,  4.0070e-03, -1.4125e-01]],\n",
       "\n",
       "         [[ 1.7751e-01,  2.1805e-01,  2.0395e-01],\n",
       "          [ 1.1959e-01, -2.5488e-01, -1.3430e-01],\n",
       "          [ 8.2781e-02, -2.6910e-01, -3.1019e-01]],\n",
       "\n",
       "         [[-9.0434e-02, -3.3564e-01, -4.1667e-01],\n",
       "          [-1.7530e-01, -4.0391e-01, -4.6602e-01],\n",
       "          [-2.1842e-01, -4.7778e-01, -4.3031e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.2669e-03, -5.5140e-02, -1.1818e-01],\n",
       "          [ 3.6503e-02,  1.2716e-01,  1.3753e-01],\n",
       "          [ 9.6560e-03,  2.7990e-01,  8.0831e-01]],\n",
       "\n",
       "         [[-4.8611e-02, -1.0334e-01, -6.9644e-02],\n",
       "          [ 1.2982e-04,  4.0877e-01,  4.6546e-01],\n",
       "          [ 8.4115e-02,  7.5370e-01,  8.5458e-01]],\n",
       "\n",
       "         [[-2.4938e-02, -5.7428e-02,  8.4612e-03],\n",
       "          [-1.1295e-02,  5.5190e-02,  1.8033e-01],\n",
       "          [ 3.7924e-02,  2.1070e-01,  2.5825e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.3524e-02, -4.0447e-02, -3.0676e-02],\n",
       "          [-5.0552e-02, -4.5626e-01, -5.4001e-01],\n",
       "          [ 4.4946e-02, -2.1692e-01, -2.4562e-01]],\n",
       "\n",
       "         [[-5.9214e-02, -5.3960e-02, -8.1123e-02],\n",
       "          [-6.0211e-02, -1.9964e-01, -2.5508e-01],\n",
       "          [-1.0010e-01, -8.2371e-02, -2.5655e-01]],\n",
       "\n",
       "         [[ 3.9383e-02,  2.0306e-02,  5.2963e-02],\n",
       "          [ 3.5108e-02, -2.3986e-01, -3.2988e-01],\n",
       "          [ 8.0004e-02,  2.9660e-02, -5.1224e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.3973e-02, -3.9901e-01, -2.1661e-01],\n",
       "          [-3.2942e-01, -8.5829e-01, -6.2310e-01],\n",
       "          [-9.6480e-02, -4.7404e-01, -4.6983e-01]],\n",
       "\n",
       "         [[-2.9271e-02,  4.7167e-01,  5.1756e-01],\n",
       "          [ 3.1754e-01,  1.0400e+00,  9.7523e-01],\n",
       "          [-5.3848e-03,  5.5752e-01,  4.5874e-01]],\n",
       "\n",
       "         [[-1.2324e-02, -1.2747e-01, -2.2177e-01],\n",
       "          [ 7.2159e-02, -1.0253e-01, -2.7674e-01],\n",
       "          [ 7.0885e-02, -6.2571e-02, -1.2283e-01]]],\n",
       "\n",
       "\n",
       "        [[[-7.6580e-02,  1.4209e-01, -9.5393e-02],\n",
       "          [ 3.0133e-01,  3.1930e-01, -6.0185e-01],\n",
       "          [ 3.5865e-01,  2.7892e-01, -5.9598e-01]],\n",
       "\n",
       "         [[-7.5171e-02,  2.7875e-01, -2.7494e-01],\n",
       "          [ 4.9476e-01,  4.6695e-01, -8.6895e-01],\n",
       "          [ 8.3793e-01,  6.0390e-01, -1.3943e+00]],\n",
       "\n",
       "         [[-7.5030e-02,  1.1630e-01, -1.6298e-02],\n",
       "          [ 6.2922e-02,  2.0488e-01, -2.3885e-01],\n",
       "          [ 4.2673e-01,  3.1190e-01, -8.1860e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0735e-02, -8.8667e-02, -1.2922e-01],\n",
       "          [-7.7856e-02,  1.7642e-01,  2.0124e-01],\n",
       "          [-1.2878e-01,  2.2749e-01,  3.2273e-01]],\n",
       "\n",
       "         [[ 6.9320e-02,  1.0982e-01,  1.1875e-01],\n",
       "          [ 4.3025e-02,  1.0876e-01,  1.7587e-01],\n",
       "          [ 1.3171e-01,  2.4686e-01,  2.7539e-01]],\n",
       "\n",
       "         [[-4.6759e-02,  1.5606e-02, -3.9407e-02],\n",
       "          [-1.4865e-02,  3.8278e-01,  4.5508e-01],\n",
       "          [ 6.8536e-04,  4.8489e-01,  6.1078e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0926e-01,  1.1077e+00,  7.1886e-01],\n",
       "          [-1.2461e-01, -1.1466e+00, -7.5873e-01],\n",
       "          [-9.8273e-03,  1.7708e-02,  8.8343e-02]],\n",
       "\n",
       "         [[ 2.7582e-01,  1.8844e+00,  1.1062e+00],\n",
       "          [-2.4614e-01, -2.1252e+00, -1.1122e+00],\n",
       "          [ 2.5406e-02, -1.4614e-02,  2.1135e-01]],\n",
       "\n",
       "         [[ 1.2279e-01,  7.1956e-01,  6.7852e-01],\n",
       "          [-9.9768e-02, -9.1763e-01, -7.8375e-01],\n",
       "          [ 2.8880e-02,  7.3271e-02,  1.5851e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.8159e-02, -1.1526e-01,  1.2950e-01],\n",
       "          [-4.8748e-02, -1.3357e+00, -4.2035e-01],\n",
       "          [ 8.1869e-02,  1.2765e+00,  3.9622e-01]],\n",
       "\n",
       "         [[ 4.4591e-02, -2.8002e-01,  2.3816e-01],\n",
       "          [-1.6210e-01, -2.1381e+00, -7.8078e-01],\n",
       "          [ 4.7243e-02,  2.3531e+00,  6.7534e-01]],\n",
       "\n",
       "         [[ 2.5620e-02, -8.9347e-02,  9.1805e-02],\n",
       "          [ 7.3734e-02, -8.9622e-01, -9.3038e-01],\n",
       "          [-5.0335e-02,  8.5070e-01,  9.2883e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.5924e-02, -2.3744e-03,  5.5290e-02],\n",
       "          [-2.5435e-02,  1.0220e-03, -9.4350e-02],\n",
       "          [-6.8299e-02, -3.2393e-02,  1.0560e+00]],\n",
       "\n",
       "         [[-1.5322e-02, -4.2381e-02,  2.0917e-02],\n",
       "          [-2.0889e-02, -2.3390e-02,  2.4147e-02],\n",
       "          [-2.2232e-02, -5.4420e-02, -1.3959e-02]],\n",
       "\n",
       "         [[ 5.4262e-03,  6.3562e-03,  4.1178e-02],\n",
       "          [ 2.2959e-02, -2.0079e-03,  1.3277e-02],\n",
       "          [ 3.9539e-02, -2.1883e-02, -1.0197e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9683e-01,  2.5801e-01,  2.2689e-01],\n",
       "          [ 3.3833e-01,  1.1797e+00,  9.2336e-01],\n",
       "          [ 2.3269e-01,  8.1019e-01,  5.6826e-01]],\n",
       "\n",
       "         [[-1.0002e-01, -1.3455e-01, -1.6953e-01],\n",
       "          [-1.7710e-01, -4.9712e-01, -4.4494e-01],\n",
       "          [-1.5792e-01, -4.0989e-01, -3.3044e-01]],\n",
       "\n",
       "         [[-8.6857e-02, -1.7519e-01, -1.0466e-01],\n",
       "          [-2.0559e-01, -5.8798e-01, -4.3271e-01],\n",
       "          [-1.4133e-01, -4.0939e-01, -3.1649e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3413e-02, -3.7876e-03,  7.7341e-04],\n",
       "          [-3.2991e-02, -1.0844e-01, -5.7236e-02],\n",
       "          [-2.2720e-02, -1.4122e-01, -4.3147e-02]],\n",
       "\n",
       "         [[ 5.8726e-02,  1.0748e-01, -2.4225e-02],\n",
       "          [ 4.5772e-02, -3.7019e-01, -2.0168e-01],\n",
       "          [-1.4462e-01, -6.5561e-01, -3.8764e-01]],\n",
       "\n",
       "         [[-9.0175e-03,  1.6287e-02, -1.8684e-02],\n",
       "          [-1.6440e-02, -3.2432e-02, -1.3361e-01],\n",
       "          [-8.0087e-03, -9.1388e-02, -1.2588e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.3102e-01, -2.9173e-01, -2.6491e-01],\n",
       "          [-2.6251e-01, -4.6280e-01, -4.4728e-01],\n",
       "          [-2.2203e-01, -4.0276e-01, -4.2865e-01]],\n",
       "\n",
       "         [[ 2.0282e-02, -8.9842e-02, -1.0703e-01],\n",
       "          [ 9.2993e-03, -1.5296e-01, -1.0475e-01],\n",
       "          [ 2.2845e-02, -1.6589e-01, -1.1233e-01]],\n",
       "\n",
       "         [[ 2.0318e-01,  3.7584e-01,  4.0234e-01],\n",
       "          [ 2.9604e-01,  5.6118e-01,  5.8280e-01],\n",
       "          [ 2.4897e-01,  4.6827e-01,  4.7827e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7646e-02, -1.3479e-02,  1.3747e-02],\n",
       "          [ 4.3191e-02, -6.0009e-01,  4.0831e-01],\n",
       "          [-2.6729e-02,  1.4848e-01, -2.6186e-03]],\n",
       "\n",
       "         [[ 8.6852e-02, -9.0645e-02,  6.9199e-02],\n",
       "          [ 6.0397e-02, -1.1608e+00,  6.6590e-01],\n",
       "          [-3.6593e-02,  3.7840e-01, -5.4704e-02]],\n",
       "\n",
       "         [[ 7.1192e-02, -8.7236e-02,  2.4118e-02],\n",
       "          [-7.9343e-02, -4.2738e-01,  6.1607e-01],\n",
       "          [ 3.3402e-02,  1.7762e-01, -2.8076e-01]]],\n",
       "\n",
       "\n",
       "        [[[-9.0258e-02,  3.8909e-02, -1.4717e-02],\n",
       "          [-4.9784e-01,  7.0324e-01, -2.0125e-01],\n",
       "          [-1.0033e+00,  1.2244e+00, -1.5275e-01]],\n",
       "\n",
       "         [[-2.5082e-01,  1.7308e-01,  3.8941e-02],\n",
       "          [-9.2727e-01,  1.1572e+00, -1.3940e-01],\n",
       "          [-2.2013e+00,  2.3799e+00, -1.8207e-01]],\n",
       "\n",
       "         [[-1.7267e-02,  1.9191e-02, -1.2643e-02],\n",
       "          [-4.5520e-01,  4.9704e-01, -1.0956e-01],\n",
       "          [-4.8208e-01,  8.9135e-01, -3.5707e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.2879e-01,  2.8116e-01,  3.4445e-01],\n",
       "          [ 2.8102e-01,  7.6184e-01,  7.8187e-01],\n",
       "          [ 3.2447e-01,  8.8057e-01,  8.5921e-01]],\n",
       "\n",
       "         [[-9.2437e-02, -1.1710e-01, -1.3613e-01],\n",
       "          [-8.6166e-02,  3.8026e-01,  2.3616e-01],\n",
       "          [-2.6110e-01,  1.7046e-01, -7.2559e-02]],\n",
       "\n",
       "         [[ 1.9903e-01, -8.3594e-02, -1.3970e-01],\n",
       "          [-6.5852e-02,  3.6479e-03,  5.7395e-02],\n",
       "          [-6.4513e-02,  4.1476e-02,  3.7333e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.0758e-02,  1.7642e-02, -3.8955e-02],\n",
       "          [ 3.7061e-01,  2.5192e-01, -8.1608e-02],\n",
       "          [-1.2734e-01,  3.5342e-04, -9.4562e-02]],\n",
       "\n",
       "         [[-9.1119e-02,  2.7487e-02,  8.5598e-02],\n",
       "          [ 2.6336e-01,  3.0528e-01, -2.1934e-02],\n",
       "          [-1.8337e-01,  3.0822e-02,  7.5424e-02]],\n",
       "\n",
       "         [[ 8.3421e-02,  5.6799e-02, -1.0166e-01],\n",
       "          [ 6.0076e-01,  2.8780e-01, -1.3468e-01],\n",
       "          [-1.7044e-01, -4.0439e-02, -1.3384e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.5758e-03, -1.1886e-01,  5.2691e-02],\n",
       "          [ 3.8455e-02, -9.5240e-02,  1.0108e+00],\n",
       "          [-9.0729e-03,  9.5743e-02, -9.9512e-01]],\n",
       "\n",
       "         [[ 2.0030e-02, -1.8463e-01,  4.6149e-02],\n",
       "          [ 4.8500e-02, -8.1863e-02,  2.4551e+00],\n",
       "          [-8.8753e-02,  1.9571e-01, -2.3946e+00]],\n",
       "\n",
       "         [[-6.7299e-03, -8.5028e-02,  8.3174e-02],\n",
       "          [ 7.1985e-02, -3.7330e-02, -6.2352e-02],\n",
       "          [-3.3481e-02,  9.9654e-03,  4.4527e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1544e-02,  3.3825e-02,  3.2953e-02],\n",
       "          [ 9.5338e-02,  9.2858e-02,  6.9814e-02],\n",
       "          [ 7.7775e-02,  7.0240e-02,  3.5561e-02]],\n",
       "\n",
       "         [[ 8.0250e-02,  1.1455e-01,  9.1864e-02],\n",
       "          [ 1.3941e-01,  1.7964e-01,  1.4243e-01],\n",
       "          [ 9.0561e-02,  1.1040e-01,  6.2158e-02]],\n",
       "\n",
       "         [[-1.1408e-01, -1.1910e-01, -1.1297e-01],\n",
       "          [-2.1075e-01, -2.3716e-01, -1.8196e-01],\n",
       "          [-1.4555e-01, -1.5185e-01, -1.0213e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.6223e-03, -4.0512e-02, -6.3279e-02],\n",
       "          [-7.8481e-02,  5.3281e-01,  2.0549e-01],\n",
       "          [-6.2342e-02,  6.5127e-02,  1.1698e-01]],\n",
       "\n",
       "         [[ 4.3254e-03, -1.4461e-01, -3.7078e-02],\n",
       "          [-2.3664e-02,  8.4550e-01,  4.1627e-01],\n",
       "          [-8.2809e-02,  1.3983e-01,  1.3596e-01]],\n",
       "\n",
       "         [[-2.2600e-02, -4.6655e-02, -3.2568e-02],\n",
       "          [-3.6126e-02,  3.3198e-01,  1.5211e-01],\n",
       "          [-2.9636e-02,  8.1084e-02,  6.7871e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.4028e-02,  2.2118e-02,  5.3453e-01],\n",
       "          [-2.8720e-03, -6.1138e-03, -6.4067e-01],\n",
       "          [ 4.2600e-02,  3.5242e-02,  4.7357e-02]],\n",
       "\n",
       "         [[-1.1947e-02,  5.3347e-03,  2.0991e+00],\n",
       "          [-7.0220e-02,  1.8368e-02, -2.4034e+00],\n",
       "          [ 3.7795e-02,  1.9104e-01,  1.1874e-01]],\n",
       "\n",
       "         [[-2.8839e-02,  1.7536e-02,  9.0887e-02],\n",
       "          [-2.7015e-02,  7.3476e-03,  2.2504e-02],\n",
       "          [ 9.8820e-04,  6.0223e-02, -1.0806e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.7124e-01, -6.6075e-01, -2.9025e-01],\n",
       "          [-6.0282e-01, -7.8866e-01, -3.8771e-01],\n",
       "          [-2.9037e-01, -4.0949e-01, -2.4665e-01]],\n",
       "\n",
       "         [[ 9.3573e-02,  1.0440e-01,  8.8645e-02],\n",
       "          [ 1.5820e-01,  1.2433e-01,  3.5592e-02],\n",
       "          [ 1.9537e-01,  5.0200e-02,  1.8498e-01]],\n",
       "\n",
       "         [[ 3.6891e-01,  5.3902e-01,  1.9384e-01],\n",
       "          [ 4.5008e-01,  7.0271e-01,  2.9652e-01],\n",
       "          [ 1.2130e-01,  2.2923e-01,  9.2774e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2985e-02,  2.3598e-02,  2.4256e-02],\n",
       "          [ 1.1373e-01,  5.0202e-02,  1.6685e-02],\n",
       "          [-5.3220e-01, -4.1427e-01,  1.3237e-01]],\n",
       "\n",
       "         [[-7.7946e-02, -6.5400e-02,  2.2342e-02],\n",
       "          [ 1.0141e-03, -6.8991e-02, -3.4935e-02],\n",
       "          [-2.7958e-01, -2.7335e-01, -2.3415e-03]],\n",
       "\n",
       "         [[ 1.6454e-02,  2.2998e-02,  1.5887e-02],\n",
       "          [ 1.0922e-01,  6.0327e-02, -1.0632e-02],\n",
       "          [-3.8996e-01, -2.7355e-01,  1.0471e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.2244e-02,  1.2719e-01,  9.4514e-03],\n",
       "          [ 1.0820e-01, -5.4111e-01, -6.8911e-01],\n",
       "          [ 2.1347e-03, -5.1055e-01, -9.2773e-01]],\n",
       "\n",
       "         [[ 6.5479e-02,  1.5716e-02,  7.4090e-02],\n",
       "          [-1.3179e-02,  4.5883e-01,  2.7830e-01],\n",
       "          [-1.1389e-01,  3.0665e-01,  3.7447e-01]],\n",
       "\n",
       "         [[-2.9169e-02, -1.0650e-01, -1.4249e-01],\n",
       "          [-1.2608e-01,  6.2130e-02,  3.6687e-01],\n",
       "          [ 5.8697e-02,  1.2847e-01,  8.0128e-01]]]], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2_2.features[0][0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b7b4b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 4.5455e-02,  7.6288e-02,  3.0048e-02],\n",
       "          [ 5.5432e-01,  2.9662e-01, -3.1593e-02],\n",
       "          [ 7.8309e-01,  2.3778e-01, -7.0381e-02]],\n",
       "\n",
       "         [[-2.9542e-01, -2.2617e-01,  4.4444e-02],\n",
       "          [-1.1007e+00, -7.1409e-01, -5.0454e-02],\n",
       "          [-1.4469e+00, -6.2652e-01, -6.3332e-03]],\n",
       "\n",
       "         [[ 2.2284e-01,  1.2584e-01, -2.9721e-02],\n",
       "          [ 6.2338e-01,  4.4390e-01,  4.5707e-02],\n",
       "          [ 6.7145e-01,  4.5727e-01,  7.3647e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.9284e-02, -2.5299e-02,  1.4641e-02],\n",
       "          [ 5.2415e-02, -2.0318e-01,  1.3882e-01],\n",
       "          [ 9.2027e-02, -9.3667e-01,  9.1768e-01]],\n",
       "\n",
       "         [[-7.7375e-02, -2.6244e-02,  4.9299e-02],\n",
       "          [ 2.1007e-01, -6.1103e-01,  4.3788e-01],\n",
       "          [ 6.8553e-02, -1.9628e+00,  1.9089e+00]],\n",
       "\n",
       "         [[-5.9978e-02,  1.4608e-02,  1.5315e-03],\n",
       "          [ 1.6183e-01, -1.6718e-01,  9.2858e-02],\n",
       "          [-6.7604e-03, -4.1440e-01,  3.6333e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.7562e-03, -2.1970e-02, -1.4113e-02],\n",
       "          [-8.2082e-02, -1.3783e-01, -4.7141e-02],\n",
       "          [-2.7940e-01, -3.6277e-01, -2.3812e-01]],\n",
       "\n",
       "         [[ 1.2188e-02, -1.6955e-02, -3.2000e-02],\n",
       "          [ 6.4081e-02, -3.1379e-01, -1.3303e-01],\n",
       "          [-5.0792e-01, -9.7041e-01, -5.6709e-01]],\n",
       "\n",
       "         [[ 1.1011e-02, -1.5214e-02, -1.5223e-02],\n",
       "          [-2.7066e-02, -9.9056e-02, -1.2986e-01],\n",
       "          [-1.1055e-01, -1.8562e-01, -2.7011e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6226e-02, -5.8355e-02,  1.6767e-02],\n",
       "          [-2.4499e-01, -2.2667e-01, -1.2807e-02],\n",
       "          [-6.2013e-01, -2.0347e-01,  6.3911e-02]],\n",
       "\n",
       "         [[-1.4652e-01,  7.7170e-02, -8.8026e-02],\n",
       "          [ 6.3574e-01,  4.3070e-01, -1.6116e-02],\n",
       "          [ 1.4168e+00,  4.3797e-01, -3.0905e-01]],\n",
       "\n",
       "         [[ 1.2926e-01, -4.9621e-02,  1.2115e-02],\n",
       "          [-2.0691e-01, -2.3254e-01,  1.4823e-02],\n",
       "          [-9.8532e-01, -2.5416e-01,  1.2248e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.5654e-03, -1.4654e-01,  1.0786e-01],\n",
       "          [ 8.8356e-02, -2.7176e-01, -3.5853e-01],\n",
       "          [ 1.3345e-01, -2.5737e-01, -6.2330e-01]],\n",
       "\n",
       "         [[-6.1692e-03, -1.7159e-01,  1.0380e-01],\n",
       "          [ 1.0622e-01, -4.9651e-01, -4.1311e-01],\n",
       "          [ 1.1102e-01, -4.7138e-01, -8.3252e-01]],\n",
       "\n",
       "         [[-2.1845e-02, -1.4307e-01,  6.1010e-02],\n",
       "          [ 4.7625e-02, -1.8282e-01, -2.7660e-01],\n",
       "          [ 1.1679e-01, -9.1711e-02, -3.3707e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.5669e-03,  1.3087e-01,  1.1337e-02],\n",
       "          [ 7.7059e-02, -2.2907e-01, -8.0004e-02],\n",
       "          [ 9.2109e-02, -1.6570e-01,  4.0205e-02]],\n",
       "\n",
       "         [[-1.2674e-01, -9.1433e-02, -2.0497e-01],\n",
       "          [-5.4737e-02, -4.2860e-01, -1.7692e-01],\n",
       "          [-5.6698e-02, -4.8727e-01, -1.1448e-01]],\n",
       "\n",
       "         [[ 2.5169e-02,  1.5508e-01,  4.6205e-02],\n",
       "          [ 8.4115e-02, -1.0285e-01,  6.9053e-02],\n",
       "          [-1.3452e-02, -2.2193e-01, -3.4380e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 6.1950e-02, -1.4591e-01,  9.3395e-02],\n",
       "          [-5.3517e-02, -6.1517e-01,  6.3799e-01],\n",
       "          [ 1.1399e-01, -8.8383e-01,  7.7254e-01]],\n",
       "\n",
       "         [[ 7.9657e-03, -2.5341e-01,  3.1599e-01],\n",
       "          [-3.8451e-02, -1.0760e+00,  1.1028e+00],\n",
       "          [ 2.6621e-01, -1.6640e+00,  1.4228e+00]],\n",
       "\n",
       "         [[ 3.6819e-02, -6.5501e-02,  6.3354e-02],\n",
       "          [ 1.8800e-02, -5.5180e-01,  4.4437e-01],\n",
       "          [ 8.2743e-02, -7.7295e-01,  7.3217e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.5354e-02, -2.0501e-01, -2.6577e-02],\n",
       "          [-9.0355e-02, -4.1631e-01, -4.8026e-01],\n",
       "          [-1.1800e-02,  6.5963e-01,  5.0521e-01]],\n",
       "\n",
       "         [[ 1.2973e-01, -1.7856e-01,  2.5484e-02],\n",
       "          [-5.0095e-02, -1.0308e+00, -9.5188e-01],\n",
       "          [ 2.2359e-02,  1.1624e+00,  8.5877e-01]],\n",
       "\n",
       "         [[ 6.4256e-02, -1.0079e-01, -1.6926e-02],\n",
       "          [-8.2074e-02, -3.5325e-01, -4.9013e-01],\n",
       "          [ 2.0874e-03,  4.3593e-01,  5.4686e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1409e-02,  6.6383e-02,  1.0740e-01],\n",
       "          [-1.8636e-02,  4.7435e-02,  1.6486e-01],\n",
       "          [-7.0551e-02,  1.5408e-01,  2.0916e-01]],\n",
       "\n",
       "         [[ 2.1353e-01, -3.0290e-01, -4.5801e-01],\n",
       "          [-1.1888e-01, -7.2436e-01, -8.2227e-01],\n",
       "          [ 1.0874e-01, -5.7593e-01, -9.9738e-01]],\n",
       "\n",
       "         [[-1.5124e-01,  1.5392e-01,  3.7335e-01],\n",
       "          [ 9.7860e-02,  8.2027e-01,  8.7589e-01],\n",
       "          [-6.5503e-02,  5.8365e-01,  6.1670e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0703e-01,  1.8028e-01,  2.4543e-01],\n",
       "          [ 7.9443e-02, -1.6422e-01, -1.7709e-01],\n",
       "          [ 2.3077e-01,  4.0070e-03, -1.4125e-01]],\n",
       "\n",
       "         [[ 1.7751e-01,  2.1805e-01,  2.0395e-01],\n",
       "          [ 1.1959e-01, -2.5488e-01, -1.3430e-01],\n",
       "          [ 8.2781e-02, -2.6910e-01, -3.1019e-01]],\n",
       "\n",
       "         [[-9.0434e-02, -3.3564e-01, -4.1667e-01],\n",
       "          [-1.7530e-01, -4.0391e-01, -4.6602e-01],\n",
       "          [-2.1842e-01, -4.7778e-01, -4.3031e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.2669e-03, -5.5140e-02, -1.1818e-01],\n",
       "          [ 3.6503e-02,  1.2716e-01,  1.3753e-01],\n",
       "          [ 9.6560e-03,  2.7990e-01,  8.0831e-01]],\n",
       "\n",
       "         [[-4.8611e-02, -1.0334e-01, -6.9644e-02],\n",
       "          [ 1.2982e-04,  4.0877e-01,  4.6546e-01],\n",
       "          [ 8.4115e-02,  7.5370e-01,  8.5458e-01]],\n",
       "\n",
       "         [[-2.4938e-02, -5.7428e-02,  8.4612e-03],\n",
       "          [-1.1295e-02,  5.5190e-02,  1.8033e-01],\n",
       "          [ 3.7924e-02,  2.1070e-01,  2.5825e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.3524e-02, -4.0447e-02, -3.0676e-02],\n",
       "          [-5.0552e-02, -4.5626e-01, -5.4001e-01],\n",
       "          [ 4.4946e-02, -2.1692e-01, -2.4562e-01]],\n",
       "\n",
       "         [[-5.9214e-02, -5.3960e-02, -8.1123e-02],\n",
       "          [-6.0211e-02, -1.9964e-01, -2.5508e-01],\n",
       "          [-1.0010e-01, -8.2371e-02, -2.5655e-01]],\n",
       "\n",
       "         [[ 3.9383e-02,  2.0306e-02,  5.2963e-02],\n",
       "          [ 3.5108e-02, -2.3986e-01, -3.2988e-01],\n",
       "          [ 8.0004e-02,  2.9660e-02, -5.1224e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.3973e-02, -3.9901e-01, -2.1661e-01],\n",
       "          [-3.2942e-01, -8.5829e-01, -6.2310e-01],\n",
       "          [-9.6480e-02, -4.7404e-01, -4.6983e-01]],\n",
       "\n",
       "         [[-2.9271e-02,  4.7167e-01,  5.1756e-01],\n",
       "          [ 3.1754e-01,  1.0400e+00,  9.7523e-01],\n",
       "          [-5.3848e-03,  5.5752e-01,  4.5874e-01]],\n",
       "\n",
       "         [[-1.2324e-02, -1.2747e-01, -2.2177e-01],\n",
       "          [ 7.2159e-02, -1.0253e-01, -2.7674e-01],\n",
       "          [ 7.0885e-02, -6.2571e-02, -1.2283e-01]]],\n",
       "\n",
       "\n",
       "        [[[-7.6580e-02,  1.4209e-01, -9.5393e-02],\n",
       "          [ 3.0133e-01,  3.1930e-01, -6.0185e-01],\n",
       "          [ 3.5865e-01,  2.7892e-01, -5.9598e-01]],\n",
       "\n",
       "         [[-7.5171e-02,  2.7875e-01, -2.7494e-01],\n",
       "          [ 4.9476e-01,  4.6695e-01, -8.6895e-01],\n",
       "          [ 8.3793e-01,  6.0390e-01, -1.3943e+00]],\n",
       "\n",
       "         [[-7.5030e-02,  1.1630e-01, -1.6298e-02],\n",
       "          [ 6.2922e-02,  2.0488e-01, -2.3885e-01],\n",
       "          [ 4.2673e-01,  3.1190e-01, -8.1860e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0735e-02, -8.8667e-02, -1.2922e-01],\n",
       "          [-7.7856e-02,  1.7642e-01,  2.0124e-01],\n",
       "          [-1.2878e-01,  2.2749e-01,  3.2273e-01]],\n",
       "\n",
       "         [[ 6.9320e-02,  1.0982e-01,  1.1875e-01],\n",
       "          [ 4.3025e-02,  1.0876e-01,  1.7587e-01],\n",
       "          [ 1.3171e-01,  2.4686e-01,  2.7539e-01]],\n",
       "\n",
       "         [[-4.6759e-02,  1.5606e-02, -3.9407e-02],\n",
       "          [-1.4865e-02,  3.8278e-01,  4.5508e-01],\n",
       "          [ 6.8536e-04,  4.8489e-01,  6.1078e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0926e-01,  1.1077e+00,  7.1886e-01],\n",
       "          [-1.2461e-01, -1.1466e+00, -7.5873e-01],\n",
       "          [-9.8273e-03,  1.7708e-02,  8.8343e-02]],\n",
       "\n",
       "         [[ 2.7582e-01,  1.8844e+00,  1.1062e+00],\n",
       "          [-2.4614e-01, -2.1252e+00, -1.1122e+00],\n",
       "          [ 2.5406e-02, -1.4614e-02,  2.1135e-01]],\n",
       "\n",
       "         [[ 1.2279e-01,  7.1956e-01,  6.7852e-01],\n",
       "          [-9.9768e-02, -9.1763e-01, -7.8375e-01],\n",
       "          [ 2.8880e-02,  7.3271e-02,  1.5851e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.8159e-02, -1.1526e-01,  1.2950e-01],\n",
       "          [-4.8748e-02, -1.3357e+00, -4.2035e-01],\n",
       "          [ 8.1869e-02,  1.2765e+00,  3.9622e-01]],\n",
       "\n",
       "         [[ 4.4591e-02, -2.8002e-01,  2.3816e-01],\n",
       "          [-1.6210e-01, -2.1381e+00, -7.8078e-01],\n",
       "          [ 4.7243e-02,  2.3531e+00,  6.7534e-01]],\n",
       "\n",
       "         [[ 2.5620e-02, -8.9347e-02,  9.1805e-02],\n",
       "          [ 7.3734e-02, -8.9622e-01, -9.3038e-01],\n",
       "          [-5.0335e-02,  8.5070e-01,  9.2883e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.5924e-02, -2.3744e-03,  5.5290e-02],\n",
       "          [-2.5435e-02,  1.0220e-03, -9.4350e-02],\n",
       "          [-6.8299e-02, -3.2393e-02,  1.0560e+00]],\n",
       "\n",
       "         [[-1.5322e-02, -4.2381e-02,  2.0917e-02],\n",
       "          [-2.0889e-02, -2.3390e-02,  2.4147e-02],\n",
       "          [-2.2232e-02, -5.4420e-02, -1.3959e-02]],\n",
       "\n",
       "         [[ 5.4262e-03,  6.3562e-03,  4.1178e-02],\n",
       "          [ 2.2959e-02, -2.0079e-03,  1.3277e-02],\n",
       "          [ 3.9539e-02, -2.1883e-02, -1.0197e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9683e-01,  2.5801e-01,  2.2689e-01],\n",
       "          [ 3.3833e-01,  1.1797e+00,  9.2336e-01],\n",
       "          [ 2.3269e-01,  8.1019e-01,  5.6826e-01]],\n",
       "\n",
       "         [[-1.0002e-01, -1.3455e-01, -1.6953e-01],\n",
       "          [-1.7710e-01, -4.9712e-01, -4.4494e-01],\n",
       "          [-1.5792e-01, -4.0989e-01, -3.3044e-01]],\n",
       "\n",
       "         [[-8.6857e-02, -1.7519e-01, -1.0466e-01],\n",
       "          [-2.0559e-01, -5.8798e-01, -4.3271e-01],\n",
       "          [-1.4133e-01, -4.0939e-01, -3.1649e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3413e-02, -3.7876e-03,  7.7341e-04],\n",
       "          [-3.2991e-02, -1.0844e-01, -5.7236e-02],\n",
       "          [-2.2720e-02, -1.4122e-01, -4.3147e-02]],\n",
       "\n",
       "         [[ 5.8726e-02,  1.0748e-01, -2.4225e-02],\n",
       "          [ 4.5772e-02, -3.7019e-01, -2.0168e-01],\n",
       "          [-1.4462e-01, -6.5561e-01, -3.8764e-01]],\n",
       "\n",
       "         [[-9.0175e-03,  1.6287e-02, -1.8684e-02],\n",
       "          [-1.6440e-02, -3.2432e-02, -1.3361e-01],\n",
       "          [-8.0087e-03, -9.1388e-02, -1.2588e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.3102e-01, -2.9173e-01, -2.6491e-01],\n",
       "          [-2.6251e-01, -4.6280e-01, -4.4728e-01],\n",
       "          [-2.2203e-01, -4.0276e-01, -4.2865e-01]],\n",
       "\n",
       "         [[ 2.0282e-02, -8.9842e-02, -1.0703e-01],\n",
       "          [ 9.2993e-03, -1.5296e-01, -1.0475e-01],\n",
       "          [ 2.2845e-02, -1.6589e-01, -1.1233e-01]],\n",
       "\n",
       "         [[ 2.0318e-01,  3.7584e-01,  4.0234e-01],\n",
       "          [ 2.9604e-01,  5.6118e-01,  5.8280e-01],\n",
       "          [ 2.4897e-01,  4.6827e-01,  4.7827e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7646e-02, -1.3479e-02,  1.3747e-02],\n",
       "          [ 4.3191e-02, -6.0009e-01,  4.0831e-01],\n",
       "          [-2.6729e-02,  1.4848e-01, -2.6186e-03]],\n",
       "\n",
       "         [[ 8.6852e-02, -9.0645e-02,  6.9199e-02],\n",
       "          [ 6.0397e-02, -1.1608e+00,  6.6590e-01],\n",
       "          [-3.6593e-02,  3.7840e-01, -5.4704e-02]],\n",
       "\n",
       "         [[ 7.1192e-02, -8.7236e-02,  2.4118e-02],\n",
       "          [-7.9343e-02, -4.2738e-01,  6.1607e-01],\n",
       "          [ 3.3402e-02,  1.7762e-01, -2.8076e-01]]],\n",
       "\n",
       "\n",
       "        [[[-9.0258e-02,  3.8909e-02, -1.4717e-02],\n",
       "          [-4.9784e-01,  7.0324e-01, -2.0125e-01],\n",
       "          [-1.0033e+00,  1.2244e+00, -1.5275e-01]],\n",
       "\n",
       "         [[-2.5082e-01,  1.7308e-01,  3.8941e-02],\n",
       "          [-9.2727e-01,  1.1572e+00, -1.3940e-01],\n",
       "          [-2.2013e+00,  2.3799e+00, -1.8207e-01]],\n",
       "\n",
       "         [[-1.7267e-02,  1.9191e-02, -1.2643e-02],\n",
       "          [-4.5520e-01,  4.9704e-01, -1.0956e-01],\n",
       "          [-4.8208e-01,  8.9135e-01, -3.5707e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.2879e-01,  2.8116e-01,  3.4445e-01],\n",
       "          [ 2.8102e-01,  7.6184e-01,  7.8187e-01],\n",
       "          [ 3.2447e-01,  8.8057e-01,  8.5921e-01]],\n",
       "\n",
       "         [[-9.2437e-02, -1.1710e-01, -1.3613e-01],\n",
       "          [-8.6166e-02,  3.8026e-01,  2.3616e-01],\n",
       "          [-2.6110e-01,  1.7046e-01, -7.2559e-02]],\n",
       "\n",
       "         [[ 1.9903e-01, -8.3594e-02, -1.3970e-01],\n",
       "          [-6.5852e-02,  3.6479e-03,  5.7395e-02],\n",
       "          [-6.4513e-02,  4.1476e-02,  3.7333e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.0758e-02,  1.7642e-02, -3.8955e-02],\n",
       "          [ 3.7061e-01,  2.5192e-01, -8.1608e-02],\n",
       "          [-1.2734e-01,  3.5342e-04, -9.4562e-02]],\n",
       "\n",
       "         [[-9.1119e-02,  2.7487e-02,  8.5598e-02],\n",
       "          [ 2.6336e-01,  3.0528e-01, -2.1934e-02],\n",
       "          [-1.8337e-01,  3.0822e-02,  7.5424e-02]],\n",
       "\n",
       "         [[ 8.3421e-02,  5.6799e-02, -1.0166e-01],\n",
       "          [ 6.0076e-01,  2.8780e-01, -1.3468e-01],\n",
       "          [-1.7044e-01, -4.0439e-02, -1.3384e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.5758e-03, -1.1886e-01,  5.2691e-02],\n",
       "          [ 3.8455e-02, -9.5240e-02,  1.0108e+00],\n",
       "          [-9.0729e-03,  9.5743e-02, -9.9512e-01]],\n",
       "\n",
       "         [[ 2.0030e-02, -1.8463e-01,  4.6149e-02],\n",
       "          [ 4.8500e-02, -8.1863e-02,  2.4551e+00],\n",
       "          [-8.8753e-02,  1.9571e-01, -2.3946e+00]],\n",
       "\n",
       "         [[-6.7299e-03, -8.5028e-02,  8.3174e-02],\n",
       "          [ 7.1985e-02, -3.7330e-02, -6.2352e-02],\n",
       "          [-3.3481e-02,  9.9654e-03,  4.4527e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1544e-02,  3.3825e-02,  3.2953e-02],\n",
       "          [ 9.5338e-02,  9.2858e-02,  6.9814e-02],\n",
       "          [ 7.7775e-02,  7.0240e-02,  3.5561e-02]],\n",
       "\n",
       "         [[ 8.0250e-02,  1.1455e-01,  9.1864e-02],\n",
       "          [ 1.3941e-01,  1.7964e-01,  1.4243e-01],\n",
       "          [ 9.0561e-02,  1.1040e-01,  6.2158e-02]],\n",
       "\n",
       "         [[-1.1408e-01, -1.1910e-01, -1.1297e-01],\n",
       "          [-2.1075e-01, -2.3716e-01, -1.8196e-01],\n",
       "          [-1.4555e-01, -1.5185e-01, -1.0213e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.6223e-03, -4.0512e-02, -6.3279e-02],\n",
       "          [-7.8481e-02,  5.3281e-01,  2.0549e-01],\n",
       "          [-6.2342e-02,  6.5127e-02,  1.1698e-01]],\n",
       "\n",
       "         [[ 4.3254e-03, -1.4461e-01, -3.7078e-02],\n",
       "          [-2.3664e-02,  8.4550e-01,  4.1627e-01],\n",
       "          [-8.2809e-02,  1.3983e-01,  1.3596e-01]],\n",
       "\n",
       "         [[-2.2600e-02, -4.6655e-02, -3.2568e-02],\n",
       "          [-3.6126e-02,  3.3198e-01,  1.5211e-01],\n",
       "          [-2.9636e-02,  8.1084e-02,  6.7871e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.4028e-02,  2.2118e-02,  5.3453e-01],\n",
       "          [-2.8720e-03, -6.1138e-03, -6.4067e-01],\n",
       "          [ 4.2600e-02,  3.5242e-02,  4.7357e-02]],\n",
       "\n",
       "         [[-1.1947e-02,  5.3347e-03,  2.0991e+00],\n",
       "          [-7.0220e-02,  1.8368e-02, -2.4034e+00],\n",
       "          [ 3.7795e-02,  1.9104e-01,  1.1874e-01]],\n",
       "\n",
       "         [[-2.8839e-02,  1.7536e-02,  9.0887e-02],\n",
       "          [-2.7015e-02,  7.3476e-03,  2.2504e-02],\n",
       "          [ 9.8820e-04,  6.0223e-02, -1.0806e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.7124e-01, -6.6075e-01, -2.9025e-01],\n",
       "          [-6.0282e-01, -7.8866e-01, -3.8771e-01],\n",
       "          [-2.9037e-01, -4.0949e-01, -2.4665e-01]],\n",
       "\n",
       "         [[ 9.3573e-02,  1.0440e-01,  8.8645e-02],\n",
       "          [ 1.5820e-01,  1.2433e-01,  3.5592e-02],\n",
       "          [ 1.9537e-01,  5.0200e-02,  1.8498e-01]],\n",
       "\n",
       "         [[ 3.6891e-01,  5.3902e-01,  1.9384e-01],\n",
       "          [ 4.5008e-01,  7.0271e-01,  2.9652e-01],\n",
       "          [ 1.2130e-01,  2.2923e-01,  9.2774e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2985e-02,  2.3598e-02,  2.4256e-02],\n",
       "          [ 1.1373e-01,  5.0202e-02,  1.6685e-02],\n",
       "          [-5.3220e-01, -4.1427e-01,  1.3237e-01]],\n",
       "\n",
       "         [[-7.7946e-02, -6.5400e-02,  2.2342e-02],\n",
       "          [ 1.0141e-03, -6.8991e-02, -3.4935e-02],\n",
       "          [-2.7958e-01, -2.7335e-01, -2.3415e-03]],\n",
       "\n",
       "         [[ 1.6454e-02,  2.2998e-02,  1.5887e-02],\n",
       "          [ 1.0922e-01,  6.0327e-02, -1.0632e-02],\n",
       "          [-3.8996e-01, -2.7355e-01,  1.0471e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.2244e-02,  1.2719e-01,  9.4514e-03],\n",
       "          [ 1.0820e-01, -5.4111e-01, -6.8911e-01],\n",
       "          [ 2.1347e-03, -5.1055e-01, -9.2773e-01]],\n",
       "\n",
       "         [[ 6.5479e-02,  1.5716e-02,  7.4090e-02],\n",
       "          [-1.3179e-02,  4.5883e-01,  2.7830e-01],\n",
       "          [-1.1389e-01,  3.0665e-01,  3.7447e-01]],\n",
       "\n",
       "         [[-2.9169e-02, -1.0650e-01, -1.4249e-01],\n",
       "          [-1.2608e-01,  6.2130e-02,  3.6687e-01],\n",
       "          [ 5.8697e-02,  1.2847e-01,  8.0128e-01]]]], requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2_3.features[0][0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48291855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): ConvNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n",
       "            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): ConvNormActivation(\n",
       "      (0): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=True)\n",
       "    (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ae4f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_w1 = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea97eb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvNormActivation(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU(inplace=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (2): ConvNormActivation(\n",
       "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (2): ConvNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "    )\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "    )\n",
       "    (2): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "    )\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "    )\n",
       "    (2): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "    )\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "    )\n",
       "    (2): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "    )\n",
       "    (3): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n",
       "          (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "    )\n",
       "    (2): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "    )\n",
       "    (3): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n",
       "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "    )\n",
       "    (2): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "    )\n",
       "    (3): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "    )\n",
       "    (4): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n",
       "          (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (block): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n",
       "          (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (3): ConvNormActivation(\n",
       "          (0): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "    )\n",
       "  )\n",
       "  (8): ConvNormActivation(\n",
       "    (0): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbd49324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNormActivation(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): SiLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d21828c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2.features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99544f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_conv_forward',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_reversed_padding_repeated_twice',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'dilation',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'groups',\n",
       " 'half',\n",
       " 'in_channels',\n",
       " 'kernel_size',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'out_channels',\n",
       " 'output_padding',\n",
       " 'padding',\n",
       " 'padding_mode',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'stride',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'transposed',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2.features[0][0].__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f403919e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 6.1672e-03, -4.0346e-02,  5.2185e-02],\n",
       "          [ 6.0527e-02, -5.7140e-02, -2.3929e-02],\n",
       "          [ 9.4913e-02,  4.6150e-02,  1.5089e-02]],\n",
       "\n",
       "         [[ 1.9656e-02, -1.2332e-01,  9.4948e-02],\n",
       "          [-1.1030e-01,  2.9495e-02, -2.8500e-02],\n",
       "          [ 1.2329e-01,  1.5252e-01,  1.5829e-01]],\n",
       "\n",
       "         [[-2.0169e-02,  5.1398e-02, -8.2236e-02],\n",
       "          [ 2.5886e-02, -1.5757e-01, -1.8021e-01],\n",
       "          [ 7.2297e-02,  1.2766e-01,  9.0751e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.0675e-02, -2.3334e-02, -8.0036e-02],\n",
       "          [-2.5121e-01, -2.1746e-02,  2.5008e-02],\n",
       "          [-1.5127e-02, -1.0798e-01,  4.7689e-02]],\n",
       "\n",
       "         [[-9.2125e-04,  4.2680e-02, -4.8680e-02],\n",
       "          [-1.1922e-01,  6.4653e-02, -6.4257e-02],\n",
       "          [-2.3509e-01, -3.9835e-02, -1.5604e-02]],\n",
       "\n",
       "         [[-6.4507e-02, -1.9241e-01, -9.7056e-02],\n",
       "          [ 1.0155e-01, -2.7669e-02, -1.1025e-01],\n",
       "          [ 4.1791e-02,  2.5879e-02, -1.1561e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.8486e-02, -7.2743e-02,  4.1630e-02],\n",
       "          [-6.4894e-02, -4.9335e-02, -6.7543e-02],\n",
       "          [-1.5991e-01,  6.3293e-02,  1.9206e-01]],\n",
       "\n",
       "         [[-7.0743e-02,  3.3471e-02,  2.9661e-02],\n",
       "          [-6.0193e-02,  3.9746e-02, -5.0552e-02],\n",
       "          [-2.0988e-01, -2.5637e-02, -7.3719e-02]],\n",
       "\n",
       "         [[-4.3821e-02, -1.0814e-01, -6.7131e-02],\n",
       "          [-5.3568e-02, -4.7392e-02,  1.9104e-02],\n",
       "          [-1.1792e-02, -3.4745e-04,  1.0032e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.8002e-03, -4.9258e-02, -2.6749e-01],\n",
       "          [-6.7901e-03,  3.5500e-02,  5.4865e-02],\n",
       "          [ 1.3044e-01,  2.3133e-02, -1.0892e-02]],\n",
       "\n",
       "         [[-1.9055e-01,  1.3461e-01, -9.7799e-02],\n",
       "          [-1.6624e-01, -1.3998e-02, -2.1879e-02],\n",
       "          [ 3.0394e-02, -4.7305e-02, -4.9664e-02]],\n",
       "\n",
       "         [[-4.3172e-02,  5.2425e-02, -6.8386e-03],\n",
       "          [-1.5723e-01,  7.7479e-02,  2.7578e-02],\n",
       "          [ 1.0974e-02,  5.2928e-02,  2.5612e-03]]],\n",
       "\n",
       "\n",
       "        [[[-4.4042e-02,  8.5584e-03, -1.5864e-02],\n",
       "          [-6.1886e-02, -3.6213e-02, -1.0431e-01],\n",
       "          [-7.0214e-02,  1.1522e-01,  1.4232e-01]],\n",
       "\n",
       "         [[-6.0325e-02, -6.3983e-02, -4.2452e-02],\n",
       "          [-3.3408e-02,  5.9245e-02, -1.0115e-01],\n",
       "          [-9.0378e-02, -2.1487e-02, -6.5587e-02]],\n",
       "\n",
       "         [[-6.0040e-02, -3.9324e-02,  7.6406e-03],\n",
       "          [ 9.6627e-02, -1.8137e-01, -3.2452e-02],\n",
       "          [ 8.3628e-02,  6.2116e-02,  5.3519e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.3520e-02,  3.9235e-02,  1.4340e-01],\n",
       "          [ 3.7361e-02,  3.9069e-02, -2.0276e-01],\n",
       "          [ 2.3993e-01, -2.9042e-02, -2.1615e-02]],\n",
       "\n",
       "         [[ 3.1292e-02, -1.0759e-01, -7.4328e-02],\n",
       "          [ 9.6543e-04, -1.1387e-02, -1.2779e-01],\n",
       "          [-1.3158e-01, -6.6099e-02,  3.3665e-02]],\n",
       "\n",
       "         [[-8.6292e-02,  1.1950e-02, -7.2158e-02],\n",
       "          [-3.0410e-02, -1.1626e-01,  1.9587e-02],\n",
       "          [-4.9371e-03,  7.3330e-02,  1.1026e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1558e-01, -5.9798e-02,  9.6666e-02],\n",
       "          [-2.8823e-02,  4.4282e-03, -1.1935e-01],\n",
       "          [ 2.0240e-02,  5.8739e-02,  1.0168e-01]],\n",
       "\n",
       "         [[ 1.0641e-02, -5.4562e-02,  6.1616e-02],\n",
       "          [ 1.2605e-01,  7.9340e-02, -2.8614e-02],\n",
       "          [ 2.8953e-02,  8.4177e-02, -6.0282e-03]],\n",
       "\n",
       "         [[ 4.9293e-02, -9.3613e-02, -1.0626e-01],\n",
       "          [ 1.1393e-02,  1.3878e-01, -4.0024e-02],\n",
       "          [-2.8666e-02, -5.8117e-02, -1.0949e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1463e-01, -1.9381e-01, -1.0335e-01],\n",
       "          [-2.1656e-01,  1.6113e-01, -1.5979e-02],\n",
       "          [ 1.5042e-02, -1.3088e-01,  9.7298e-02]],\n",
       "\n",
       "         [[ 8.8746e-02, -4.5692e-02,  1.2052e-01],\n",
       "          [-5.1499e-04,  4.6414e-02, -1.5265e-01],\n",
       "          [ 8.7322e-03,  5.2824e-02,  4.2697e-02]],\n",
       "\n",
       "         [[ 5.7523e-02,  1.0431e-01,  1.7639e-02],\n",
       "          [ 6.5512e-02, -5.9885e-02, -7.5349e-02],\n",
       "          [-2.4967e-02,  1.7798e-02,  4.8953e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8548e-02,  9.8283e-02,  2.3743e-02],\n",
       "          [ 8.5996e-02, -5.5053e-02, -4.1353e-02],\n",
       "          [-4.5173e-02,  1.8270e-01,  1.3578e-01]],\n",
       "\n",
       "         [[-2.8148e-03, -1.3761e-01, -2.9561e-02],\n",
       "          [-7.2367e-03, -2.6224e-02, -2.6472e-02],\n",
       "          [ 4.5666e-03,  1.7723e-02,  1.1439e-01]],\n",
       "\n",
       "         [[-1.9216e-02,  1.0576e-01,  1.4888e-01],\n",
       "          [ 4.2491e-02,  3.5611e-02,  5.3122e-02],\n",
       "          [ 6.4013e-02,  1.1319e-01, -1.5513e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.1928e-02, -6.2495e-02, -6.0275e-02],\n",
       "          [ 1.9675e-02, -5.7568e-02,  1.2225e-01],\n",
       "          [ 8.0228e-03,  3.1696e-02,  1.1606e-01]],\n",
       "\n",
       "         [[-5.5253e-03, -1.9892e-01, -1.6741e-02],\n",
       "          [-7.9810e-02,  4.1336e-02,  1.2545e-01],\n",
       "          [ 1.2795e-02,  7.4480e-02,  1.0668e-02]],\n",
       "\n",
       "         [[-8.5685e-02,  1.7542e-02, -6.7897e-02],\n",
       "          [-1.5060e-01,  2.1413e-01, -4.3504e-02],\n",
       "          [-9.1993e-02, -3.2845e-02,  5.8651e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.9348e-02,  1.6293e-02,  1.0076e-01],\n",
       "          [-7.4411e-02, -1.6590e-01, -6.1443e-02],\n",
       "          [-4.1157e-02, -1.1979e-01,  1.2818e-01]],\n",
       "\n",
       "         [[ 4.6157e-03,  9.3988e-02,  1.6818e-02],\n",
       "          [ 9.6763e-02, -8.1652e-02, -1.3196e-01],\n",
       "          [ 1.8345e-02,  7.3748e-02,  6.5660e-03]],\n",
       "\n",
       "         [[-1.2790e-01, -1.4209e-01,  1.8919e-01],\n",
       "          [ 1.4009e-01, -1.0886e-01, -2.5535e-02],\n",
       "          [-3.4355e-02,  1.2977e-02,  1.1019e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0608e-02, -5.1418e-02, -1.8600e-02],\n",
       "          [-3.3033e-02,  1.0964e-01,  6.0905e-02],\n",
       "          [-2.4916e-02, -1.0977e-01, -3.3443e-02]],\n",
       "\n",
       "         [[-1.3313e-01,  1.4439e-02,  3.6833e-02],\n",
       "          [-2.6322e-02,  1.9541e-02, -1.0256e-01],\n",
       "          [ 2.2679e-03, -1.1492e-01, -2.3578e-02]],\n",
       "\n",
       "         [[-3.7271e-02, -3.5777e-02, -5.3780e-02],\n",
       "          [-4.4057e-02, -1.1186e-03, -2.6645e-02],\n",
       "          [-3.0191e-02,  1.0953e-01, -2.7483e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7935e-01,  4.9975e-02,  1.0777e-02],\n",
       "          [-4.3940e-02, -5.2692e-02, -4.6487e-02],\n",
       "          [ 1.6222e-01, -1.0510e-02, -6.5321e-02]],\n",
       "\n",
       "         [[-2.6592e-02,  1.2295e-01,  3.8709e-02],\n",
       "          [ 4.7602e-02, -1.1619e-03, -1.5970e-01],\n",
       "          [ 2.3343e-02, -7.3173e-02, -3.6305e-02]],\n",
       "\n",
       "         [[ 8.5113e-02,  4.6122e-02, -1.0825e-01],\n",
       "          [-1.4891e-01,  5.5107e-04, -1.5273e-01],\n",
       "          [-4.8082e-03,  1.6230e-03, -1.2514e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.8939e-02, -1.3556e-02, -1.1324e-01],\n",
       "          [ 3.1287e-02, -1.5801e-01,  5.0388e-02],\n",
       "          [-2.4574e-02,  6.9720e-02,  4.9256e-02]],\n",
       "\n",
       "         [[-6.4847e-02,  5.0886e-03, -4.4435e-02],\n",
       "          [-6.7500e-02,  1.3800e-01, -3.9797e-02],\n",
       "          [-1.6857e-02,  1.2166e-01, -2.1666e-02]],\n",
       "\n",
       "         [[-4.2683e-02, -1.5527e-01,  3.3838e-02],\n",
       "          [ 7.6957e-02,  1.9717e-02,  6.9141e-04],\n",
       "          [ 9.1917e-03,  1.0516e-01,  2.8046e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.0210e-02,  6.9656e-02,  1.9095e-01],\n",
       "          [-9.4055e-02, -8.4550e-02,  1.0504e-01],\n",
       "          [ 5.2011e-03,  7.1244e-02,  5.5836e-02]],\n",
       "\n",
       "         [[ 4.1973e-02,  6.6746e-02,  4.5277e-02],\n",
       "          [-4.7501e-03,  1.1243e-02, -5.9016e-02],\n",
       "          [-1.7889e-02,  2.4804e-03,  5.9538e-02]],\n",
       "\n",
       "         [[-6.2776e-02, -6.2687e-02, -1.2864e-01],\n",
       "          [-4.6091e-02,  1.0671e-01, -1.3106e-02],\n",
       "          [ 5.1409e-02, -4.4281e-02,  9.8682e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.0918e-03,  9.6477e-02,  6.2196e-02],\n",
       "          [ 6.6390e-02, -1.1839e-02, -1.2529e-02],\n",
       "          [ 1.6609e-02,  1.4529e-03,  3.0178e-02]],\n",
       "\n",
       "         [[-1.0357e-01, -4.8330e-02, -7.0923e-02],\n",
       "          [-9.2744e-02, -9.6910e-02, -1.2124e-01],\n",
       "          [ 1.4913e-02,  2.3020e-02, -1.0932e-02]],\n",
       "\n",
       "         [[-1.4024e-02, -6.2832e-04, -9.3756e-03],\n",
       "          [-8.3802e-02,  1.6440e-01,  1.2053e-01],\n",
       "          [ 8.4272e-02,  4.1067e-02, -9.6003e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 9.1504e-02,  7.4370e-02, -4.1927e-02],\n",
       "          [-1.2652e-01,  6.8948e-02, -5.6153e-02],\n",
       "          [ 1.1903e-01,  3.6076e-02,  3.6371e-02]],\n",
       "\n",
       "         [[ 7.2753e-02, -1.0277e-01, -1.3040e-02],\n",
       "          [-4.4614e-02, -5.0942e-02, -4.2636e-02],\n",
       "          [ 1.7781e-02, -1.4269e-01, -6.7263e-02]],\n",
       "\n",
       "         [[ 1.3135e-02, -1.0995e-01,  1.0649e-01],\n",
       "          [ 2.1214e-02, -4.5519e-03, -2.4666e-02],\n",
       "          [-6.1622e-02,  6.9345e-02, -7.4052e-02]]],\n",
       "\n",
       "\n",
       "        [[[-4.5474e-02,  8.3203e-04,  8.3250e-02],\n",
       "          [ 7.4842e-02, -7.8838e-02, -4.0108e-02],\n",
       "          [-4.6955e-02,  4.0453e-02,  6.7754e-02]],\n",
       "\n",
       "         [[ 1.5508e-01, -3.5984e-02,  8.2283e-02],\n",
       "          [-6.7192e-02,  3.4004e-02, -7.2791e-03],\n",
       "          [ 7.2903e-03,  7.2448e-02, -1.0043e-01]],\n",
       "\n",
       "         [[-6.5698e-02,  2.9413e-02, -6.7281e-02],\n",
       "          [-7.0399e-02, -1.8863e-02, -1.1668e-02],\n",
       "          [ 3.3651e-02,  2.9764e-02,  4.9609e-02]]],\n",
       "\n",
       "\n",
       "        [[[-6.0984e-02,  1.2026e-01, -6.6364e-02],\n",
       "          [ 6.5247e-02, -6.2399e-03,  4.2056e-03],\n",
       "          [-4.2828e-02,  9.6710e-02,  3.2212e-02]],\n",
       "\n",
       "         [[ 2.4146e-02,  7.6564e-03, -6.8798e-02],\n",
       "          [-2.4088e-02, -1.0783e-01, -4.7162e-03],\n",
       "          [ 6.9778e-03,  1.3798e-01,  9.2693e-02]],\n",
       "\n",
       "         [[-4.6652e-02, -3.0268e-02, -2.0589e-02],\n",
       "          [-1.6146e-02,  8.5498e-02,  2.0815e-01],\n",
       "          [ 1.9920e-02,  8.2612e-03, -9.8183e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.3662e-02,  8.8030e-02, -2.6780e-02],\n",
       "          [-3.8740e-03,  1.1898e-01,  3.3790e-02],\n",
       "          [-1.1811e-01,  1.3573e-02,  1.5706e-01]],\n",
       "\n",
       "         [[ 4.8162e-02,  2.8466e-02, -6.7056e-02],\n",
       "          [ 1.0591e-01, -4.0209e-02, -1.9381e-01],\n",
       "          [-7.3987e-02,  5.9347e-03, -5.6461e-02]],\n",
       "\n",
       "         [[ 1.8074e-02,  1.0449e-01, -1.9873e-02],\n",
       "          [-2.8739e-02,  1.9199e-02,  1.2489e-01],\n",
       "          [ 1.0907e-02, -1.1257e-02,  5.1838e-02]]],\n",
       "\n",
       "\n",
       "        [[[-8.4569e-02,  7.3688e-02,  8.2748e-02],\n",
       "          [ 7.4853e-02, -1.2079e-01, -1.1405e-01],\n",
       "          [-5.0263e-02, -6.7096e-02,  1.1174e-01]],\n",
       "\n",
       "         [[-1.2118e-02, -5.2761e-03,  1.0421e-01],\n",
       "          [ 3.0035e-02, -7.4920e-02,  6.5490e-02],\n",
       "          [-2.1893e-01, -4.4592e-02, -1.4892e-01]],\n",
       "\n",
       "         [[ 1.5485e-01, -3.0190e-02,  1.7429e-01],\n",
       "          [-1.5765e-02, -3.2998e-02,  3.1391e-02],\n",
       "          [-6.4940e-02, -2.4408e-02,  5.4200e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7046e-02,  4.1743e-02,  5.6135e-02],\n",
       "          [ 8.4007e-02,  2.9841e-02,  7.3525e-02],\n",
       "          [ 4.1451e-02, -8.3993e-03,  1.6161e-01]],\n",
       "\n",
       "         [[-6.6190e-02,  7.9282e-02,  1.3410e-01],\n",
       "          [ 4.7892e-03,  2.2560e-02, -4.1089e-02],\n",
       "          [ 1.7573e-02, -1.9187e-01, -1.5597e-01]],\n",
       "\n",
       "         [[-9.4759e-02,  1.5596e-01,  1.9103e-03],\n",
       "          [ 7.7483e-02,  2.7703e-02,  8.9542e-02],\n",
       "          [-4.9730e-02,  1.0270e-01,  4.8891e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.4516e-02,  6.4521e-02, -3.0635e-02],\n",
       "          [-9.0938e-02, -3.3208e-02,  1.0580e-01],\n",
       "          [ 5.3308e-02, -1.1476e-01,  1.1568e-01]],\n",
       "\n",
       "         [[ 1.3230e-01, -9.3359e-03, -2.7514e-04],\n",
       "          [ 7.8052e-02,  2.0046e-02, -1.8158e-02],\n",
       "          [ 1.1334e-01, -1.2624e-01,  8.3755e-02]],\n",
       "\n",
       "         [[-1.1424e-01,  8.6029e-02, -9.2510e-02],\n",
       "          [ 6.0368e-03, -8.8930e-02,  6.6338e-02],\n",
       "          [ 9.1975e-02, -1.3824e-01,  3.7257e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.4561e-02, -7.1657e-02, -6.5934e-02],\n",
       "          [-3.7641e-02, -2.3428e-02, -1.0754e-01],\n",
       "          [ 9.9147e-02,  1.2490e-01, -1.8652e-02]],\n",
       "\n",
       "         [[-1.1631e-01, -6.2936e-02, -3.0322e-02],\n",
       "          [-8.0243e-02, -7.0458e-02, -6.9686e-03],\n",
       "          [ 7.7875e-02,  7.8219e-02, -2.8197e-04]],\n",
       "\n",
       "         [[-4.1150e-04, -2.2037e-01,  1.0686e-02],\n",
       "          [-4.5105e-02, -1.9848e-02,  7.5256e-02],\n",
       "          [ 9.3737e-02,  6.5565e-02, -3.0893e-02]]],\n",
       "\n",
       "\n",
       "        [[[-4.2359e-03, -1.1625e-01,  7.7759e-02],\n",
       "          [-1.0090e-02,  3.2790e-02,  1.0117e-01],\n",
       "          [ 1.7569e-02, -1.0216e-01, -5.7281e-02]],\n",
       "\n",
       "         [[ 9.7508e-02,  6.4057e-02,  2.9833e-02],\n",
       "          [-4.1608e-02,  7.6139e-03,  4.8797e-02],\n",
       "          [-2.0522e-02, -4.2685e-03,  9.4439e-02]],\n",
       "\n",
       "         [[-2.4803e-02,  1.2306e-01,  4.3830e-02],\n",
       "          [-9.7703e-03,  4.4612e-02,  1.4449e-01],\n",
       "          [ 6.9558e-02,  1.2912e-02, -8.3194e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.0765e-03,  5.7708e-02, -3.8256e-02],\n",
       "          [ 4.2188e-02,  1.7585e-02, -5.8485e-02],\n",
       "          [ 9.5219e-03, -5.7063e-02,  3.8809e-02]],\n",
       "\n",
       "         [[-1.3925e-01, -7.6877e-03,  1.2301e-01],\n",
       "          [-1.3646e-02,  8.3555e-02, -2.9788e-02],\n",
       "          [-1.8851e-02,  1.1303e-01, -1.2584e-01]],\n",
       "\n",
       "         [[ 7.3718e-02,  1.0528e-01,  1.3740e-01],\n",
       "          [ 1.1045e-01, -4.2378e-02, -2.6656e-04],\n",
       "          [ 1.4678e-01,  7.6650e-02, -3.3007e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.2347e-02,  4.3873e-02, -3.7519e-02],\n",
       "          [ 1.1549e-02,  1.4834e-03, -5.7859e-02],\n",
       "          [ 8.8224e-02, -4.7391e-02, -6.2814e-02]],\n",
       "\n",
       "         [[-5.9464e-02, -4.6572e-02, -9.6007e-03],\n",
       "          [-1.8707e-03, -7.1212e-02,  8.0210e-02],\n",
       "          [-1.6516e-01, -8.9818e-02,  1.4391e-01]],\n",
       "\n",
       "         [[-5.3388e-02, -5.2758e-02, -2.0507e-02],\n",
       "          [-3.6130e-02, -3.5374e-02,  3.9572e-02],\n",
       "          [-1.7959e-01, -8.5818e-02, -7.7204e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 8.4517e-02,  1.3185e-01, -3.1173e-02],\n",
       "          [ 1.9421e-01, -1.1456e-01, -5.0082e-03],\n",
       "          [ 9.1896e-02,  2.3205e-01,  1.7950e-03]],\n",
       "\n",
       "         [[ 5.3158e-02, -4.9053e-04,  1.6618e-02],\n",
       "          [-1.3746e-01,  3.2403e-02,  3.1245e-02],\n",
       "          [ 1.1889e-01, -1.2264e-01, -4.6271e-02]],\n",
       "\n",
       "         [[ 7.6434e-03,  1.1671e-02, -3.6883e-02],\n",
       "          [-6.7881e-03, -1.2133e-02,  1.6452e-01],\n",
       "          [-1.0156e-01, -8.3524e-04, -3.9235e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.8862e-02, -7.3940e-02,  5.9408e-02],\n",
       "          [ 6.5832e-02, -1.0419e-02,  7.5960e-02],\n",
       "          [ 4.1459e-02, -3.3426e-02,  1.4871e-01]],\n",
       "\n",
       "         [[-2.8810e-02, -4.5086e-02, -1.5167e-01],\n",
       "          [-1.1233e-01,  3.0726e-02,  3.3808e-02],\n",
       "          [-8.6135e-02, -7.5557e-02, -2.2560e-02]],\n",
       "\n",
       "         [[-2.0809e-02, -9.0182e-03, -6.6752e-02],\n",
       "          [-1.2790e-02,  1.1577e-01,  4.8188e-02],\n",
       "          [ 7.3955e-02,  8.9406e-02, -1.5687e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.2796e-02,  4.0578e-02, -1.0085e-01],\n",
       "          [ 1.3687e-01,  5.1914e-02, -7.4255e-03],\n",
       "          [ 6.6501e-02, -1.9668e-02, -2.7109e-02]],\n",
       "\n",
       "         [[ 4.2393e-02, -8.2628e-02,  7.9686e-02],\n",
       "          [ 3.7605e-02, -1.4859e-01,  1.6320e-02],\n",
       "          [ 3.1471e-02, -1.4554e-02,  5.0218e-03]],\n",
       "\n",
       "         [[-1.2956e-01,  5.3705e-02, -5.9414e-02],\n",
       "          [ 1.8045e-02, -1.1547e-01,  7.0236e-02],\n",
       "          [ 3.5915e-02,  9.6276e-02, -7.8253e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.8927e-04, -4.3710e-02, -6.2870e-02],\n",
       "          [ 7.6038e-03,  2.4149e-02,  7.7599e-02],\n",
       "          [-6.6225e-02,  6.1183e-02,  7.3953e-02]],\n",
       "\n",
       "         [[-2.3711e-02,  3.7474e-02, -2.9178e-02],\n",
       "          [ 3.4355e-02,  1.4494e-01, -1.0072e-01],\n",
       "          [ 4.9267e-02,  4.6601e-03,  4.0428e-02]],\n",
       "\n",
       "         [[-1.4777e-01, -3.3325e-02,  1.8849e-01],\n",
       "          [-7.1086e-02,  1.2035e-01,  5.5316e-02],\n",
       "          [-7.1937e-02, -2.1654e-01, -1.4550e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.6657e-02, -1.0541e-01,  3.8348e-02],\n",
       "          [-5.3485e-02, -8.9698e-02,  3.7968e-02],\n",
       "          [ 2.5382e-02, -6.1278e-02,  9.4606e-02]],\n",
       "\n",
       "         [[ 3.0389e-02, -2.3522e-02,  7.8921e-02],\n",
       "          [-1.6500e-01,  7.7056e-02, -5.4370e-02],\n",
       "          [-6.8773e-02,  3.1438e-02,  2.3483e-01]],\n",
       "\n",
       "         [[ 1.0183e-01, -2.2823e-02,  4.6954e-02],\n",
       "          [ 2.2206e-02, -2.5845e-02,  5.0658e-03],\n",
       "          [ 1.7108e-01,  5.7720e-02,  3.4286e-02]]]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2.features[0][0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a09d375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2.features[0][0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1f29092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 3, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2.features[0][0].weight.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92c6fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_b2.features[0][0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db057796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 3, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_w1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1d59860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0615, -0.0537, -0.0466],\n",
       "          [-0.0247,  0.0465, -0.1781],\n",
       "          [ 0.2810, -0.0835, -0.1641]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1424, -0.1157, -0.0558],\n",
       "          [-0.0145, -0.1420,  0.1542],\n",
       "          [-0.2403, -0.2228,  0.2960]]],\n",
       "\n",
       "\n",
       "        [[[-0.2917,  0.0035,  0.2290],\n",
       "          [-0.1027, -0.0439,  0.2290],\n",
       "          [ 0.2296, -0.3333, -0.1390]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2919, -0.0461,  0.3322],\n",
       "          [ 0.2258, -0.1242,  0.1748],\n",
       "          [-0.2849, -0.0419, -0.3136]]],\n",
       "\n",
       "\n",
       "        [[[-0.1182,  0.0855, -0.1332],\n",
       "          [-0.2125,  0.1873, -0.1740],\n",
       "          [ 0.0627, -0.2435, -0.0882]]],\n",
       "\n",
       "\n",
       "        [[[-0.1536, -0.1660,  0.0065],\n",
       "          [ 0.0087,  0.1616,  0.0227],\n",
       "          [ 0.0982,  0.1337, -0.2575]]],\n",
       "\n",
       "\n",
       "        [[[-0.0559,  0.2469,  0.0044],\n",
       "          [ 0.1068, -0.1701, -0.0903],\n",
       "          [-0.2588, -0.2869,  0.2261]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2443, -0.0101, -0.1579],\n",
       "          [ 0.3205,  0.0143, -0.0734],\n",
       "          [-0.2044,  0.0720,  0.2064]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0095,  0.1810,  0.2731],\n",
       "          [ 0.1995,  0.0997,  0.0530],\n",
       "          [-0.3303, -0.1393, -0.1713]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1164,  0.3129, -0.3275],\n",
       "          [-0.0951,  0.3087, -0.3023],\n",
       "          [-0.1672, -0.2946,  0.1057]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3078,  0.2621, -0.2521],\n",
       "          [ 0.1385, -0.0494, -0.0757],\n",
       "          [ 0.1062, -0.1010, -0.3276]]],\n",
       "\n",
       "\n",
       "        [[[-0.2274, -0.0265, -0.0710],\n",
       "          [ 0.0622, -0.0254, -0.1802],\n",
       "          [-0.0510,  0.1182, -0.0653]]],\n",
       "\n",
       "\n",
       "        [[[-0.2204,  0.3218, -0.1363],\n",
       "          [-0.0561, -0.2929,  0.1228],\n",
       "          [ 0.2937,  0.3312, -0.2538]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0065,  0.0994, -0.2021],\n",
       "          [ 0.1551, -0.3167, -0.0031],\n",
       "          [ 0.2259, -0.2466,  0.0460]]],\n",
       "\n",
       "\n",
       "        [[[-0.1749,  0.2967,  0.0072],\n",
       "          [-0.1952, -0.0238,  0.1944],\n",
       "          [ 0.3233,  0.2606, -0.0175]]],\n",
       "\n",
       "\n",
       "        [[[-0.2859, -0.2546,  0.1648],\n",
       "          [ 0.0665, -0.0063,  0.3120],\n",
       "          [-0.1022,  0.1082, -0.2712]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0764,  0.2686, -0.0047],\n",
       "          [ 0.1254,  0.2568,  0.3267],\n",
       "          [ 0.2668,  0.2457, -0.2612]]],\n",
       "\n",
       "\n",
       "        [[[-0.0419, -0.2933,  0.3316],\n",
       "          [-0.2980, -0.2172, -0.1675],\n",
       "          [-0.1841,  0.2459,  0.1126]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2101,  0.0993, -0.2483],\n",
       "          [-0.1717, -0.0370,  0.3207],\n",
       "          [-0.0891,  0.1672,  0.1548]]],\n",
       "\n",
       "\n",
       "        [[[-0.0565, -0.0444,  0.0534],\n",
       "          [-0.1855,  0.2020,  0.2384],\n",
       "          [-0.0198, -0.0403,  0.0637]]],\n",
       "\n",
       "\n",
       "        [[[-0.0219,  0.0956, -0.1734],\n",
       "          [-0.0596, -0.1292,  0.2298],\n",
       "          [ 0.0668,  0.1535,  0.2068]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3028,  0.2083,  0.0438],\n",
       "          [ 0.1793, -0.1543, -0.3314],\n",
       "          [-0.2614, -0.1594,  0.0045]]],\n",
       "\n",
       "\n",
       "        [[[-0.2044, -0.0920, -0.0625],\n",
       "          [ 0.2240, -0.0723,  0.1342],\n",
       "          [-0.1794,  0.1897, -0.2195]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1462,  0.3301,  0.3287],\n",
       "          [ 0.2280, -0.3133, -0.0873],\n",
       "          [ 0.0269,  0.1492, -0.1304]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0564, -0.0241,  0.0208],\n",
       "          [-0.0730, -0.0761,  0.0907],\n",
       "          [ 0.0824,  0.2336,  0.1940]]],\n",
       "\n",
       "\n",
       "        [[[-0.3217,  0.1065, -0.2544],\n",
       "          [-0.1439,  0.0823, -0.0383],\n",
       "          [-0.2285,  0.0276,  0.2226]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2242,  0.2320,  0.0339],\n",
       "          [ 0.1214,  0.1919, -0.1836],\n",
       "          [-0.1240, -0.2355,  0.1431]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3288, -0.2068,  0.3001],\n",
       "          [ 0.0736,  0.0456, -0.2497],\n",
       "          [ 0.1380,  0.0130, -0.1295]]],\n",
       "\n",
       "\n",
       "        [[[-0.3017,  0.1984,  0.2417],\n",
       "          [ 0.2838, -0.3030, -0.1279],\n",
       "          [-0.1587, -0.1129, -0.3118]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1822,  0.1919, -0.0318],\n",
       "          [ 0.0650,  0.1561,  0.1450],\n",
       "          [ 0.2419, -0.2550, -0.1811]]],\n",
       "\n",
       "\n",
       "        [[[-0.1713, -0.1221, -0.3056],\n",
       "          [ 0.0989, -0.1267,  0.0872],\n",
       "          [-0.0628,  0.0083, -0.2607]]],\n",
       "\n",
       "\n",
       "        [[[-0.3255, -0.1850,  0.1669],\n",
       "          [-0.0358,  0.2905, -0.2187],\n",
       "          [ 0.2722, -0.1305, -0.2846]]]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_w1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ef45357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__dlpack__',\n",
       " '__dlpack_device__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_coalesced_',\n",
       " '_conj',\n",
       " '_conj_physical',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_fix_weakref',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_indices',\n",
       " '_is_view',\n",
       " '_make_subclass',\n",
       " '_neg_view',\n",
       " '_nnz',\n",
       " '_python_dispatch',\n",
       " '_reduce_ex_internal',\n",
       " '_update_names',\n",
       " '_values',\n",
       " '_version',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'aminmax',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_left_shift',\n",
       " 'bitwise_left_shift_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_right_shift',\n",
       " 'bitwise_right_shift_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'conj_physical',\n",
       " 'conj_physical_',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'corrcoef',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'histogram',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_conj',\n",
       " 'is_contiguous',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_inference',\n",
       " 'is_leaf',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mlc',\n",
       " 'is_neg',\n",
       " 'is_nonzero',\n",
       " 'is_ort',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'resolve_conj',\n",
       " 'resolve_neg',\n",
       " 'retain_grad',\n",
       " 'retains_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'select',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_sparse',\n",
       " 'to_sparse_csr',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_w1.weight.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35365b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_w1_3filt = efficientnet_b2.features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29dab6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff_w1_summed = eff_w1_3filt.weight.detach().numpy().sum(axis=1)\n",
    "eff_w1_summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bb7001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.65476343e-03, -1.12271488e-01,  6.48978949e-02],\n",
       "        [-2.38831677e-02, -1.85218126e-01, -2.32642844e-01],\n",
       "        [ 2.90502012e-01,  3.26332033e-01,  2.64128149e-01]],\n",
       "\n",
       "       [[ 5.24765998e-03, -1.73063427e-01, -2.25773171e-01],\n",
       "        [-2.68877029e-01,  1.52380783e-02, -1.49500623e-01],\n",
       "        [-2.08426178e-01, -1.21940762e-01, -8.35250840e-02]],\n",
       "\n",
       "       [[-1.63049474e-01, -1.47412598e-01,  4.16045636e-03],\n",
       "        [-1.78654969e-01, -5.69806807e-02, -9.89911258e-02],\n",
       "        [-3.81579310e-01,  3.73084210e-02,  2.18664229e-01]],\n",
       "\n",
       "       [[-2.25917503e-01,  1.37775764e-01, -3.72130454e-01],\n",
       "        [-3.30269456e-01,  9.89804566e-02,  6.05632402e-02],\n",
       "        [ 1.71805725e-01,  2.87561230e-02, -5.79947978e-02]],\n",
       "\n",
       "       [[-1.64407164e-01, -9.47486013e-02, -5.06752208e-02],\n",
       "        [ 1.33232772e-03, -1.58334598e-01, -2.37912238e-01],\n",
       "        [-7.69642442e-02,  1.55851707e-01,  1.30250350e-01]],\n",
       "\n",
       "       [[-8.85201395e-02, -5.63997366e-02, -3.08958441e-03],\n",
       "        [ 7.91610777e-03, -8.85828510e-02, -3.10961515e-01],\n",
       "        [ 1.03414610e-01, -2.18106732e-02,  1.22306496e-01]],\n",
       "\n",
       "       [[-5.56477532e-02, -2.07972512e-01,  5.20255640e-02],\n",
       "        [ 1.08622789e-01,  2.22549528e-01, -1.87984973e-01],\n",
       "        [ 2.05268189e-02,  8.47992748e-02, -1.38429478e-02]],\n",
       "\n",
       "       [[ 3.16353068e-02, -1.35187715e-01,  3.48080620e-02],\n",
       "        [-1.51563108e-01,  1.47661015e-01, -2.43976891e-01],\n",
       "        [-1.19272619e-03, -6.02589324e-02,  1.88947812e-01]],\n",
       "\n",
       "       [[ 3.65170985e-02,  6.64401799e-02,  1.43057004e-01],\n",
       "        [ 1.21250130e-01, -4.56659347e-02, -1.47025213e-02],\n",
       "        [ 2.34068930e-02,  3.13614488e-01,  9.50442851e-02]],\n",
       "\n",
       "       [[-1.73137993e-01, -2.43872166e-01, -1.44913226e-01],\n",
       "        [-2.10731819e-01,  1.97902143e-01,  2.04198301e-01],\n",
       "        [-7.11756498e-02,  7.33309090e-02,  1.85377866e-01]],\n",
       "\n",
       "       [[-2.02636629e-01, -3.18101943e-02,  3.06765407e-01],\n",
       "        [ 1.62446022e-01, -3.56420875e-01, -2.18942016e-01],\n",
       "        [-5.71670234e-02, -3.30702104e-02,  2.44937792e-01]],\n",
       "\n",
       "       [[-1.81010291e-01, -7.27558360e-02, -3.55468839e-02],\n",
       "        [-1.03412047e-01,  1.28059000e-01, -6.82957545e-02],\n",
       "        [-5.28394580e-02, -1.15169078e-01, -8.45042765e-02]],\n",
       "\n",
       "       [[ 2.37875268e-01,  2.19051152e-01, -5.87641448e-02],\n",
       "        [-1.45246372e-01, -5.33031076e-02, -3.58921587e-01],\n",
       "        [ 1.80752560e-01, -8.20605457e-02, -2.26766557e-01]],\n",
       "\n",
       "       [[-1.66468874e-01, -1.63739160e-01, -1.23840541e-01],\n",
       "        [ 4.07434702e-02, -2.93280929e-04,  1.12829544e-02],\n",
       "        [-3.22384797e-02,  2.96537668e-01,  5.56356460e-02]],\n",
       "\n",
       "       [[-1.11013606e-01,  7.37150237e-02,  1.07585222e-01],\n",
       "        [-1.44895673e-01,  3.34019512e-02,  3.29152830e-02],\n",
       "        [ 3.87212634e-02,  2.94429362e-02,  2.14056104e-01]],\n",
       "\n",
       "       [[-1.19686313e-01,  4.75190990e-02, -1.81027316e-02],\n",
       "        [-1.10156342e-01,  5.56528717e-02, -1.32403076e-02],\n",
       "        [ 1.15793586e-01,  6.55391365e-02,  9.64516215e-03]],\n",
       "\n",
       "       [[ 1.77393004e-01, -1.38344288e-01,  5.15187979e-02],\n",
       "        [-1.49919793e-01,  1.34534426e-02, -1.23454392e-01],\n",
       "        [ 7.51886517e-02, -3.72717828e-02, -1.04944035e-01]],\n",
       "\n",
       "       [[ 4.39125597e-02, -5.73926419e-03,  9.82512236e-02],\n",
       "        [-6.27490804e-02, -6.36970401e-02, -5.90547882e-02],\n",
       "        [-6.01352751e-03,  1.42664671e-01,  1.69300213e-02]],\n",
       "\n",
       "       [[-8.34903568e-02,  9.76433158e-02, -1.55752048e-01],\n",
       "        [ 2.50138640e-02, -2.85698473e-02,  2.07643598e-01],\n",
       "        [-1.59302689e-02,  2.42946878e-01,  2.67221332e-02]],\n",
       "\n",
       "       [[ 1.09897487e-01,  2.20986530e-01, -1.13709807e-01],\n",
       "        [ 7.33008087e-02,  9.79719013e-02, -3.51294354e-02],\n",
       "        [-1.81191832e-01,  8.25113617e-03,  1.52442008e-01]],\n",
       "\n",
       "       [[ 5.81618994e-02,  3.82222608e-02,  3.61245155e-01],\n",
       "        [ 8.91239718e-02, -2.28710473e-01, -1.71738788e-02],\n",
       "        [-3.34132880e-01, -1.36095837e-01, -3.17552872e-02]],\n",
       "\n",
       "       [[-1.43903658e-01,  2.76983142e-01,  1.92148373e-01],\n",
       "        [ 1.66278630e-01,  8.01044703e-02,  1.21977568e-01],\n",
       "        [ 9.29483399e-03, -9.75613967e-02,  5.45367002e-02]],\n",
       "\n",
       "       [[-6.45745546e-03,  1.41214028e-01, -1.23420544e-01],\n",
       "        [-6.84906216e-03, -1.02092177e-01,  1.53975934e-01],\n",
       "        [ 2.58619130e-01, -3.79241943e-01,  2.36690164e-01]],\n",
       "\n",
       "       [[-7.21565112e-02, -3.54964256e-01, -8.55696872e-02],\n",
       "        [-1.62988573e-01, -1.13734171e-01, -3.92543823e-02],\n",
       "        [ 2.70758986e-01,  2.68680871e-01, -4.98276763e-02]],\n",
       "\n",
       "       [[ 6.84689805e-02,  7.08618686e-02,  1.51421607e-01],\n",
       "        [-6.14682138e-02,  8.50156769e-02,  2.94452816e-01],\n",
       "        [ 6.66049272e-02, -9.35163572e-02, -4.60365638e-02]],\n",
       "\n",
       "       [[-7.06059039e-02,  1.55301273e-01,  2.22147673e-01],\n",
       "        [ 1.38994262e-01,  5.87633699e-02, -8.85396749e-02],\n",
       "        [ 1.37446433e-01,  1.32615402e-01, -1.20033704e-01]],\n",
       "\n",
       "       [[-1.25198349e-01, -5.54568022e-02, -6.76265135e-02],\n",
       "        [-2.64522657e-02, -1.05102353e-01,  6.19234778e-02],\n",
       "        [-2.56518573e-01, -2.23026782e-01,  3.89114022e-03]],\n",
       "\n",
       "       [[ 1.45319283e-01,  1.43027067e-01, -5.14372587e-02],\n",
       "        [ 4.99625951e-02, -9.42917019e-02,  1.90753952e-01],\n",
       "        [ 1.09220311e-01,  1.08574830e-01, -4.83995788e-02]],\n",
       "\n",
       "       [[-1.08481050e-01, -1.28044084e-01, -1.59009263e-01],\n",
       "        [-5.92853203e-02,  1.36081308e-01,  1.57956123e-01],\n",
       "        [ 2.92786062e-02, -1.95773691e-02,  1.10458568e-01]],\n",
       "\n",
       "       [[-2.43715197e-02,  1.16548128e-02, -8.05745125e-02],\n",
       "        [ 1.92524880e-01, -2.12137908e-01,  7.91303888e-02],\n",
       "        [ 1.33887023e-01,  6.20540902e-02, -1.00340046e-01]],\n",
       "\n",
       "       [[-1.71767265e-01, -3.95615101e-02,  9.64418650e-02],\n",
       "        [-2.91271992e-02,  2.89438814e-01,  3.21943164e-02],\n",
       "        [-8.88956040e-02, -1.50692284e-01, -3.11199874e-02]],\n",
       "\n",
       "       [[ 9.55596715e-02, -1.51756316e-01,  1.64223611e-01],\n",
       "        [-1.96275979e-01, -3.84863913e-02, -1.13359476e-02],\n",
       "        [ 1.27693504e-01,  2.78797746e-02,  3.63724500e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff_w1_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe9c675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_w1_summed = eff_w1_summed.reshape((32,1,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac682766",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_w1.weight = torch.nn.Parameter(torch.tensor(eff_w1_summed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b11f0389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 5.6548e-03, -1.1227e-01,  6.4898e-02],\n",
       "          [-2.3883e-02, -1.8522e-01, -2.3264e-01],\n",
       "          [ 2.9050e-01,  3.2633e-01,  2.6413e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.2477e-03, -1.7306e-01, -2.2577e-01],\n",
       "          [-2.6888e-01,  1.5238e-02, -1.4950e-01],\n",
       "          [-2.0843e-01, -1.2194e-01, -8.3525e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.6305e-01, -1.4741e-01,  4.1605e-03],\n",
       "          [-1.7865e-01, -5.6981e-02, -9.8991e-02],\n",
       "          [-3.8158e-01,  3.7308e-02,  2.1866e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.2592e-01,  1.3778e-01, -3.7213e-01],\n",
       "          [-3.3027e-01,  9.8980e-02,  6.0563e-02],\n",
       "          [ 1.7181e-01,  2.8756e-02, -5.7995e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.6441e-01, -9.4749e-02, -5.0675e-02],\n",
       "          [ 1.3323e-03, -1.5833e-01, -2.3791e-01],\n",
       "          [-7.6964e-02,  1.5585e-01,  1.3025e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.8520e-02, -5.6400e-02, -3.0896e-03],\n",
       "          [ 7.9161e-03, -8.8583e-02, -3.1096e-01],\n",
       "          [ 1.0341e-01, -2.1811e-02,  1.2231e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.5648e-02, -2.0797e-01,  5.2026e-02],\n",
       "          [ 1.0862e-01,  2.2255e-01, -1.8798e-01],\n",
       "          [ 2.0527e-02,  8.4799e-02, -1.3843e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.1635e-02, -1.3519e-01,  3.4808e-02],\n",
       "          [-1.5156e-01,  1.4766e-01, -2.4398e-01],\n",
       "          [-1.1927e-03, -6.0259e-02,  1.8895e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.6517e-02,  6.6440e-02,  1.4306e-01],\n",
       "          [ 1.2125e-01, -4.5666e-02, -1.4703e-02],\n",
       "          [ 2.3407e-02,  3.1361e-01,  9.5044e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.7314e-01, -2.4387e-01, -1.4491e-01],\n",
       "          [-2.1073e-01,  1.9790e-01,  2.0420e-01],\n",
       "          [-7.1176e-02,  7.3331e-02,  1.8538e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0264e-01, -3.1810e-02,  3.0677e-01],\n",
       "          [ 1.6245e-01, -3.5642e-01, -2.1894e-01],\n",
       "          [-5.7167e-02, -3.3070e-02,  2.4494e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.8101e-01, -7.2756e-02, -3.5547e-02],\n",
       "          [-1.0341e-01,  1.2806e-01, -6.8296e-02],\n",
       "          [-5.2839e-02, -1.1517e-01, -8.4504e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.3788e-01,  2.1905e-01, -5.8764e-02],\n",
       "          [-1.4525e-01, -5.3303e-02, -3.5892e-01],\n",
       "          [ 1.8075e-01, -8.2061e-02, -2.2677e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.6647e-01, -1.6374e-01, -1.2384e-01],\n",
       "          [ 4.0743e-02, -2.9328e-04,  1.1283e-02],\n",
       "          [-3.2238e-02,  2.9654e-01,  5.5636e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.1101e-01,  7.3715e-02,  1.0759e-01],\n",
       "          [-1.4490e-01,  3.3402e-02,  3.2915e-02],\n",
       "          [ 3.8721e-02,  2.9443e-02,  2.1406e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1969e-01,  4.7519e-02, -1.8103e-02],\n",
       "          [-1.1016e-01,  5.5653e-02, -1.3240e-02],\n",
       "          [ 1.1579e-01,  6.5539e-02,  9.6452e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7739e-01, -1.3834e-01,  5.1519e-02],\n",
       "          [-1.4992e-01,  1.3453e-02, -1.2345e-01],\n",
       "          [ 7.5189e-02, -3.7272e-02, -1.0494e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.3913e-02, -5.7393e-03,  9.8251e-02],\n",
       "          [-6.2749e-02, -6.3697e-02, -5.9055e-02],\n",
       "          [-6.0135e-03,  1.4266e-01,  1.6930e-02]]],\n",
       "\n",
       "\n",
       "        [[[-8.3490e-02,  9.7643e-02, -1.5575e-01],\n",
       "          [ 2.5014e-02, -2.8570e-02,  2.0764e-01],\n",
       "          [-1.5930e-02,  2.4295e-01,  2.6722e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0990e-01,  2.2099e-01, -1.1371e-01],\n",
       "          [ 7.3301e-02,  9.7972e-02, -3.5129e-02],\n",
       "          [-1.8119e-01,  8.2511e-03,  1.5244e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8162e-02,  3.8222e-02,  3.6125e-01],\n",
       "          [ 8.9124e-02, -2.2871e-01, -1.7174e-02],\n",
       "          [-3.3413e-01, -1.3610e-01, -3.1755e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.4390e-01,  2.7698e-01,  1.9215e-01],\n",
       "          [ 1.6628e-01,  8.0104e-02,  1.2198e-01],\n",
       "          [ 9.2948e-03, -9.7561e-02,  5.4537e-02]]],\n",
       "\n",
       "\n",
       "        [[[-6.4575e-03,  1.4121e-01, -1.2342e-01],\n",
       "          [-6.8491e-03, -1.0209e-01,  1.5398e-01],\n",
       "          [ 2.5862e-01, -3.7924e-01,  2.3669e-01]]],\n",
       "\n",
       "\n",
       "        [[[-7.2157e-02, -3.5496e-01, -8.5570e-02],\n",
       "          [-1.6299e-01, -1.1373e-01, -3.9254e-02],\n",
       "          [ 2.7076e-01,  2.6868e-01, -4.9828e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.8469e-02,  7.0862e-02,  1.5142e-01],\n",
       "          [-6.1468e-02,  8.5016e-02,  2.9445e-01],\n",
       "          [ 6.6605e-02, -9.3516e-02, -4.6037e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.0606e-02,  1.5530e-01,  2.2215e-01],\n",
       "          [ 1.3899e-01,  5.8763e-02, -8.8540e-02],\n",
       "          [ 1.3745e-01,  1.3262e-01, -1.2003e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.2520e-01, -5.5457e-02, -6.7627e-02],\n",
       "          [-2.6452e-02, -1.0510e-01,  6.1923e-02],\n",
       "          [-2.5652e-01, -2.2303e-01,  3.8911e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4532e-01,  1.4303e-01, -5.1437e-02],\n",
       "          [ 4.9963e-02, -9.4292e-02,  1.9075e-01],\n",
       "          [ 1.0922e-01,  1.0857e-01, -4.8400e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.0848e-01, -1.2804e-01, -1.5901e-01],\n",
       "          [-5.9285e-02,  1.3608e-01,  1.5796e-01],\n",
       "          [ 2.9279e-02, -1.9577e-02,  1.1046e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.4372e-02,  1.1655e-02, -8.0575e-02],\n",
       "          [ 1.9252e-01, -2.1214e-01,  7.9130e-02],\n",
       "          [ 1.3389e-01,  6.2054e-02, -1.0034e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.7177e-01, -3.9562e-02,  9.6442e-02],\n",
       "          [-2.9127e-02,  2.8944e-01,  3.2194e-02],\n",
       "          [-8.8896e-02, -1.5069e-01, -3.1120e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 9.5560e-02, -1.5176e-01,  1.6422e-01],\n",
       "          [-1.9628e-01, -3.8486e-02, -1.1336e-02],\n",
       "          [ 1.2769e-01,  2.7880e-02,  3.6372e-01]]]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_w1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b7a0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_b2.features[0][0] = replace_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b2ab340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2175, -0.3077, -0.3479,  ..., -0.0277, -0.0338, -0.5428],\n",
       "        [-0.0759,  0.1436, -0.0688,  ...,  0.0072, -0.0605,  0.0826],\n",
       "        [-0.0910,  0.0133,  0.0941,  ..., -0.0172, -0.0575,  0.1283],\n",
       "        ...,\n",
       "        [-0.1029, -0.0997, -0.1490,  ...,  0.0033, -0.0185,  0.1192],\n",
       "        [ 0.0550, -0.0164, -0.0291,  ...,  0.0238,  0.0876,  0.0421],\n",
       "        [ 0.0172,  0.1080, -0.0502,  ...,  0.0259, -0.1375,  0.1516]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2(torch.tensor(mit_train_x_ims_try[:10][:,np.newaxis,:,:].astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "032b1ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1560,  0.1285,  0.0634,  ..., -0.0536, -0.0613,  0.1545],\n",
       "        [ 0.1051,  0.0723,  0.0930,  ...,  0.0665,  0.1560,  0.0930],\n",
       "        [ 0.0796, -0.0356, -0.0873,  ..., -0.0084, -0.0303,  0.0722],\n",
       "        ...,\n",
       "        [ 0.2359,  0.0269,  0.0226,  ..., -0.1524, -0.0278, -0.0023],\n",
       "        [ 1.4316,  0.0830,  0.1379,  ..., -0.5529,  0.4594,  0.4878],\n",
       "        [ 0.0886,  0.1926,  0.0405,  ...,  0.0340,  0.1898,  0.1965]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2(torch.tensor(mit_train_x_ims_try[:10][:,np.newaxis,:,:].astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3bd29e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2(torch.tensor(mit_train_x_ims_try[:10][:,np.newaxis,:,:].astype(np.float32))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "927df663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1772,  0.1209, -0.0273,  ...,  0.2825,  0.2800,  0.3685],\n",
       "        [ 0.1105,  0.0788, -0.0377,  ..., -0.0034, -0.0530,  0.0091],\n",
       "        [ 0.0831,  0.1188, -0.1801,  ...,  0.1031, -0.1485, -0.0471],\n",
       "        [ 0.5326,  0.3849,  0.1610,  ...,  0.0548,  0.0137,  0.0774],\n",
       "        [ 0.1714,  0.0553,  0.0257,  ..., -0.0219,  0.0258,  0.1898]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2(torch.randn(5,1,187,187))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6d46414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eff_back(torch.nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(eff_back, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        backbone = torchvision.models.efficientnet_b2()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        eff_w1_3filt = self.backbone.features[0][0]\n",
    "        eff_w1_summed = eff_w1_3filt.weight.detach().numpy().sum(axis=1)\n",
    "        eff_w1_summed = eff_w1_summed.reshape((32,1,3,3))\n",
    "        replace_w1 = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        #replace_w1.weight = torch.tensor(eff_w1_summed)\n",
    "        replace_w1.load_state_dict({'weight':torch.tensor(eff_w1_summed)})\n",
    "        self.backbone.features[0][0] = replace_w1\n",
    "\n",
    "        self.classifier = torch.nn.Linear(in_features=1000, out_features=num_classes, bias=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out_1000 = self.backbone(x)\n",
    "        return self.classifier(out_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0434ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_model = eff_back(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f399433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0695, -0.6848,  0.8272,  ...,  0.0446,  0.5079, -1.4220],\n",
       "        [-0.6411, -1.9489, -1.5704,  ...,  0.0890,  1.6234, -0.9699],\n",
       "        [-1.9601, -0.0302,  0.2935,  ..., -1.4343, -2.2644, -0.4214],\n",
       "        [-1.2045, -0.3208, -1.1385,  ..., -1.4037, -1.3862, -0.0593],\n",
       "        [-2.3702, -1.4064, -0.1412,  ..., -1.1034, -3.0565,  0.7341]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff_model.backbone(torch.randn(5,1,187,187))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2285768a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00583787, 0.19013707, 0.07263222, 0.66531616, 0.0660767 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_freqs = Counter(mit_train_y)\n",
    "loss_weights = np.array([1.0/train_label_freqs[i] for i in range(5)])\n",
    "loss_weights = (loss_weights/np.sum(loss_weights)).astype(np.float32)\n",
    "loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02ca6fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 0 loss 1.5825790166854858\n",
      "\tbatch 10 loss 1.3889881372451782\n",
      "\tbatch 20 loss 1.6891754865646362\n",
      "\tbatch 30 loss 1.527307391166687\n",
      "\tbatch 40 loss 5.098392009735107\n",
      "\tbatch 50 loss 1.2870780229568481\n",
      "\tbatch 60 loss 1.0577658414840698\n",
      "\tbatch 70 loss 1.0164605379104614\n",
      "\tbatch 80 loss 1.8294843435287476\n",
      "\tbatch 90 loss 1.6917091608047485\n",
      "\tbatch 100 loss 1.2937698364257812\n",
      "\tbatch 110 loss 1.603596806526184\n",
      "\tbatch 120 loss 1.668884515762329\n",
      "\tbatch 130 loss 1.5953449010849\n",
      "\tbatch 140 loss 1.6099371910095215\n",
      "\tbatch 150 loss 1.502029299736023\n",
      "\tbatch 160 loss 1.413860559463501\n",
      "\tbatch 170 loss 1.423665165901184\n",
      "\tbatch 180 loss 1.6570087671279907\n",
      "\tbatch 190 loss 1.5151045322418213\n",
      "\tbatch 200 loss 1.361348032951355\n",
      "\tbatch 210 loss 2.709449529647827\n",
      "\tbatch 220 loss 1.410272240638733\n",
      "\tbatch 230 loss 1.26264488697052\n",
      "\tbatch 240 loss 1.1888104677200317\n",
      "\tbatch 250 loss 1.3854002952575684\n",
      "\tbatch 260 loss 1.677490472793579\n",
      "\tbatch 270 loss 2.3743984699249268\n",
      "\tbatch 280 loss 1.3250924348831177\n",
      "\tbatch 290 loss 1.225060224533081\n",
      "\tbatch 300 loss 1.2498908042907715\n",
      "\tbatch 310 loss 1.0649714469909668\n",
      "\tbatch 320 loss 0.7726039886474609\n",
      "\tbatch 330 loss 1.221659779548645\n",
      "\tbatch 340 loss 1.1170504093170166\n",
      "\tbatch 350 loss 1.5686264038085938\n",
      "\tbatch 360 loss 0.6485365033149719\n",
      "\tbatch 370 loss 1.1101549863815308\n",
      "\tbatch 380 loss 1.2578696012496948\n",
      "\tbatch 390 loss 1.4802807569503784\n",
      "\tbatch 400 loss 1.0432049036026\n",
      "\tbatch 410 loss 1.4253175258636475\n",
      "\tbatch 420 loss 1.001684308052063\n",
      "\tbatch 430 loss 1.0691115856170654\n",
      "\tbatch 440 loss 1.231289029121399\n",
      "\tbatch 450 loss 0.9525490999221802\n",
      "\tbatch 460 loss 0.8888896107673645\n",
      "\tbatch 470 loss 1.2653239965438843\n",
      "\tbatch 480 loss 0.8132883310317993\n",
      "\tbatch 490 loss 0.8545147180557251\n",
      "\tbatch 500 loss 0.7670184373855591\n",
      "\tbatch 510 loss 0.5871872901916504\n",
      "\tbatch 520 loss 0.6823073625564575\n",
      "\tbatch 530 loss 0.5638203620910645\n",
      "\tbatch 540 loss 0.9512006044387817\n",
      "\tbatch 550 loss 0.6572308540344238\n",
      "\tbatch 560 loss 0.629258930683136\n",
      "\tbatch 570 loss 0.9063339233398438\n",
      "\tbatch 580 loss 0.7546036839485168\n",
      "\tbatch 590 loss 0.8205814361572266\n",
      "\tbatch 600 loss 0.7499526739120483\n",
      "\tbatch 610 loss 0.6289397478103638\n",
      "\tbatch 620 loss 1.3118690252304077\n",
      "\tbatch 630 loss 0.5188677906990051\n",
      "\tbatch 640 loss 0.8261600732803345\n",
      "\tbatch 650 loss 0.577261209487915\n",
      "\tbatch 660 loss 0.9612244367599487\n",
      "\tbatch 670 loss 1.402651071548462\n",
      "\tbatch 680 loss 0.4406692087650299\n",
      "\tbatch 690 loss 1.5871983766555786\n",
      "\tbatch 700 loss 0.5963467359542847\n",
      "\tbatch 710 loss 0.6359025239944458\n",
      "\tbatch 720 loss 1.0680350065231323\n",
      "\tbatch 730 loss 0.9656890630722046\n",
      "\tbatch 740 loss 1.40409255027771\n",
      "\tbatch 750 loss 0.4919763505458832\n",
      "\tbatch 760 loss 1.4612990617752075\n",
      "\tbatch 770 loss 0.7599959969520569\n",
      "\tbatch 780 loss 1.0748932361602783\n",
      "\tbatch 790 loss 0.8680022954940796\n",
      "\tbatch 800 loss 1.2722392082214355\n",
      "\tbatch 810 loss 0.5780068635940552\n",
      "\tbatch 820 loss 0.6635806560516357\n",
      "\tbatch 830 loss 0.6799803376197815\n",
      "\tbatch 840 loss 1.04718816280365\n",
      "\tbatch 850 loss 0.5732864141464233\n",
      "\tbatch 860 loss 0.9215329885482788\n",
      "\tbatch 870 loss 0.9521740078926086\n",
      "\tbatch 880 loss 1.1232377290725708\n",
      "\tbatch 890 loss 1.134316325187683\n",
      "\tbatch 900 loss 0.6032447218894958\n",
      "\tbatch 910 loss 0.32870081067085266\n",
      "\tbatch 920 loss 0.731734037399292\n",
      "\tbatch 930 loss 0.4378206729888916\n",
      "\tbatch 940 loss 0.5415463447570801\n",
      "\tbatch 950 loss 0.5128027200698853\n",
      "\tbatch 960 loss 0.47358596324920654\n",
      "\tbatch 970 loss 0.5609912872314453\n",
      "\tbatch 980 loss 0.345763623714447\n",
      "\tbatch 990 loss 0.5512932538986206\n",
      "\tbatch 1000 loss 0.4466576874256134\n",
      "\tbatch 1010 loss 0.35448145866394043\n",
      "\tbatch 1020 loss 0.5358895659446716\n",
      "\tbatch 1030 loss 0.8541601300239563\n",
      "\tbatch 1040 loss 0.5432226657867432\n",
      "\tbatch 1050 loss 0.43681907653808594\n",
      "\tbatch 1060 loss 0.37966257333755493\n",
      "\tbatch 1070 loss 0.9245986938476562\n",
      "\tbatch 1080 loss 0.3614804744720459\n",
      "\tbatch 1090 loss 0.20009464025497437\n",
      "\tbatch 1100 loss 0.7164349555969238\n",
      "\tbatch 1110 loss 0.5063284635543823\n",
      "\tbatch 1120 loss 0.4309980869293213\n",
      "\tbatch 1130 loss 1.1369657516479492\n",
      "\tbatch 1140 loss 0.6674721837043762\n",
      "\tbatch 1150 loss 0.4876459538936615\n",
      "\tbatch 1160 loss 0.7718300819396973\n",
      "\tbatch 1170 loss 0.690578281879425\n",
      "\tbatch 1180 loss 0.7763797640800476\n",
      "\tbatch 1190 loss 0.8411943316459656\n",
      "\tbatch 1200 loss 0.5285505056381226\n",
      "\tbatch 1210 loss 0.22104178369045258\n",
      "\tbatch 1220 loss 0.6260603070259094\n",
      "\tbatch 1230 loss 0.45607736706733704\n",
      "\tbatch 1240 loss 0.6834550499916077\n",
      "epoch 0 average_loss 1.0100749810695648\n",
      "\tbatch 0 loss 0.46505358815193176\n",
      "\tbatch 10 loss 1.0634714365005493\n",
      "\tbatch 20 loss 0.44241786003112793\n",
      "\tbatch 30 loss 0.4074455797672272\n",
      "\tbatch 40 loss 0.3404020071029663\n",
      "\tbatch 50 loss 0.3716460168361664\n",
      "\tbatch 60 loss 0.3040224015712738\n",
      "\tbatch 70 loss 0.324641615152359\n",
      "\tbatch 80 loss 1.3598307371139526\n",
      "\tbatch 90 loss 0.4108940064907074\n",
      "\tbatch 100 loss 0.17787006497383118\n",
      "\tbatch 110 loss 0.7375770807266235\n",
      "\tbatch 120 loss 0.4572398364543915\n",
      "\tbatch 130 loss 0.19800116121768951\n",
      "\tbatch 140 loss 0.6088880300521851\n",
      "\tbatch 150 loss 0.847572386264801\n",
      "\tbatch 160 loss 0.9168440103530884\n",
      "\tbatch 170 loss 2.1287901401519775\n",
      "\tbatch 180 loss 0.837710976600647\n",
      "\tbatch 190 loss 0.6534149646759033\n",
      "\tbatch 200 loss 0.2067623734474182\n",
      "\tbatch 210 loss 0.47020649909973145\n",
      "\tbatch 220 loss 0.6867400407791138\n",
      "\tbatch 230 loss 1.1636735200881958\n",
      "\tbatch 240 loss 0.46504175662994385\n",
      "\tbatch 250 loss 0.5571812391281128\n",
      "\tbatch 260 loss 1.049817681312561\n",
      "\tbatch 270 loss 0.3190082311630249\n",
      "\tbatch 280 loss 0.6255160570144653\n",
      "\tbatch 290 loss 0.5980297327041626\n",
      "\tbatch 300 loss 1.0466190576553345\n",
      "\tbatch 310 loss 0.42481398582458496\n",
      "\tbatch 320 loss 0.5484950542449951\n",
      "\tbatch 330 loss 0.8382581472396851\n",
      "\tbatch 340 loss 0.8503153920173645\n",
      "\tbatch 350 loss 0.5911298990249634\n",
      "\tbatch 360 loss 0.384797602891922\n",
      "\tbatch 370 loss 0.656684935092926\n",
      "\tbatch 380 loss 0.43900904059410095\n",
      "\tbatch 390 loss 1.9132804870605469\n",
      "\tbatch 400 loss 0.593651533126831\n",
      "\tbatch 410 loss 0.5330597162246704\n",
      "\tbatch 420 loss 0.6210176944732666\n",
      "\tbatch 430 loss 0.4054643213748932\n",
      "\tbatch 440 loss 0.6097601056098938\n",
      "\tbatch 450 loss 0.4616824984550476\n",
      "\tbatch 460 loss 0.39195236563682556\n",
      "\tbatch 470 loss 0.4656708538532257\n",
      "\tbatch 480 loss 0.5298378467559814\n",
      "\tbatch 490 loss 0.41008642315864563\n",
      "\tbatch 500 loss 0.18653956055641174\n",
      "\tbatch 510 loss 0.40084946155548096\n",
      "\tbatch 520 loss 0.7305742502212524\n",
      "\tbatch 530 loss 0.3643338978290558\n",
      "\tbatch 540 loss 0.7214264869689941\n",
      "\tbatch 550 loss 0.48234987258911133\n",
      "\tbatch 560 loss 0.36373695731163025\n",
      "\tbatch 570 loss 0.2528507113456726\n",
      "\tbatch 580 loss 0.5559176206588745\n",
      "\tbatch 590 loss 0.12124189734458923\n",
      "\tbatch 600 loss 0.36650344729423523\n",
      "\tbatch 610 loss 0.6331191658973694\n",
      "\tbatch 620 loss 1.0967563390731812\n",
      "\tbatch 630 loss 0.2651352882385254\n",
      "\tbatch 640 loss 0.6621890068054199\n",
      "\tbatch 650 loss 0.32977965474128723\n",
      "\tbatch 660 loss 0.4985959827899933\n",
      "\tbatch 670 loss 1.4464964866638184\n",
      "\tbatch 680 loss 0.21437546610832214\n",
      "\tbatch 690 loss 1.1873362064361572\n",
      "\tbatch 700 loss 0.34202849864959717\n",
      "\tbatch 710 loss 0.3261502683162689\n",
      "\tbatch 720 loss 0.610809326171875\n",
      "\tbatch 730 loss 0.5999510884284973\n",
      "\tbatch 740 loss 0.49547991156578064\n",
      "\tbatch 750 loss 0.6597306728363037\n",
      "\tbatch 760 loss 0.46271029114723206\n",
      "\tbatch 770 loss 0.5119046568870544\n",
      "\tbatch 780 loss 1.5214014053344727\n",
      "\tbatch 790 loss 1.1723663806915283\n",
      "\tbatch 800 loss 1.308892846107483\n",
      "\tbatch 810 loss 0.5566187500953674\n",
      "\tbatch 820 loss 0.591106653213501\n",
      "\tbatch 830 loss 0.6320329308509827\n",
      "\tbatch 840 loss 0.472167432308197\n",
      "\tbatch 850 loss 0.5816308259963989\n",
      "\tbatch 860 loss 0.4800952970981598\n",
      "\tbatch 870 loss 0.5261948108673096\n",
      "\tbatch 880 loss 1.0181483030319214\n",
      "\tbatch 890 loss 0.8686416745185852\n",
      "\tbatch 900 loss 0.25053688883781433\n",
      "\tbatch 910 loss 0.2334936261177063\n",
      "\tbatch 920 loss 0.42010098695755005\n",
      "\tbatch 930 loss 0.33443424105644226\n",
      "\tbatch 940 loss 0.1295294314622879\n",
      "\tbatch 950 loss 0.37654441595077515\n",
      "\tbatch 960 loss 0.2837177813053131\n",
      "\tbatch 970 loss 0.7003830075263977\n",
      "\tbatch 980 loss 0.25365114212036133\n",
      "\tbatch 990 loss 0.4821372628211975\n",
      "\tbatch 1000 loss 0.31512928009033203\n",
      "\tbatch 1010 loss 0.29977378249168396\n",
      "\tbatch 1020 loss 0.40485358238220215\n",
      "\tbatch 1030 loss 0.9151756763458252\n",
      "\tbatch 1040 loss 0.3449084460735321\n",
      "\tbatch 1050 loss 0.49937745928764343\n",
      "\tbatch 1060 loss 0.283798485994339\n",
      "\tbatch 1070 loss 0.3258706331253052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 1080 loss 0.39040490984916687\n",
      "\tbatch 1090 loss 0.18175636231899261\n",
      "\tbatch 1100 loss 0.47436967492103577\n",
      "\tbatch 1110 loss 0.43534550070762634\n",
      "\tbatch 1120 loss 0.2625860571861267\n",
      "\tbatch 1130 loss 0.9949484467506409\n",
      "\tbatch 1140 loss 0.7313365340232849\n",
      "\tbatch 1150 loss 0.24865540862083435\n",
      "\tbatch 1160 loss 0.5231189131736755\n",
      "\tbatch 1170 loss 0.36061182618141174\n",
      "\tbatch 1180 loss 0.5134764909744263\n",
      "\tbatch 1190 loss 0.5546749830245972\n",
      "\tbatch 1200 loss 0.34571388363838196\n",
      "\tbatch 1210 loss 0.11103647947311401\n",
      "\tbatch 1220 loss 0.3869810998439789\n",
      "\tbatch 1230 loss 0.6347973346710205\n",
      "\tbatch 1240 loss 0.5677187442779541\n",
      "epoch 1 average_loss 0.6029774132311344\n",
      "\tbatch 0 loss 0.33735474944114685\n",
      "\tbatch 10 loss 0.7894651889801025\n",
      "\tbatch 20 loss 0.2773275077342987\n",
      "\tbatch 30 loss 0.2204233705997467\n",
      "\tbatch 40 loss 0.15675921738147736\n",
      "\tbatch 50 loss 0.2661752998828888\n",
      "\tbatch 60 loss 0.2240375429391861\n",
      "\tbatch 70 loss 0.19789469242095947\n",
      "\tbatch 80 loss 1.2687612771987915\n",
      "\tbatch 90 loss 0.31990304589271545\n",
      "\tbatch 100 loss 0.16070879995822906\n",
      "\tbatch 110 loss 0.38099750876426697\n",
      "\tbatch 120 loss 0.38432934880256653\n",
      "\tbatch 130 loss 0.2450098991394043\n",
      "\tbatch 140 loss 0.7208662033081055\n",
      "\tbatch 150 loss 0.5127511620521545\n",
      "\tbatch 160 loss 0.904596209526062\n",
      "\tbatch 170 loss 2.856624126434326\n",
      "\tbatch 180 loss 0.30267655849456787\n",
      "\tbatch 190 loss 0.41062772274017334\n",
      "\tbatch 200 loss 0.14778858423233032\n",
      "\tbatch 210 loss 0.30770444869995117\n",
      "\tbatch 220 loss 0.7771522998809814\n",
      "\tbatch 230 loss 0.5124614834785461\n",
      "\tbatch 240 loss 0.2866564691066742\n",
      "\tbatch 250 loss 0.49060502648353577\n",
      "\tbatch 260 loss 0.9749364852905273\n",
      "\tbatch 270 loss 0.3767889142036438\n",
      "\tbatch 280 loss 0.6139635443687439\n",
      "\tbatch 290 loss 0.6266933083534241\n",
      "\tbatch 300 loss 0.7491477727890015\n",
      "\tbatch 310 loss 0.4271150529384613\n",
      "\tbatch 320 loss 0.2791944146156311\n",
      "\tbatch 330 loss 0.4358054995536804\n",
      "\tbatch 340 loss 0.8939694166183472\n",
      "\tbatch 350 loss 0.3286188542842865\n",
      "\tbatch 360 loss 0.12305902689695358\n",
      "\tbatch 370 loss 0.3746233284473419\n",
      "\tbatch 380 loss 0.4790939688682556\n",
      "\tbatch 390 loss 2.115658760070801\n",
      "\tbatch 400 loss 0.7549998760223389\n",
      "\tbatch 410 loss 0.4508391320705414\n",
      "\tbatch 420 loss 0.6167541146278381\n",
      "\tbatch 430 loss 0.5469254851341248\n",
      "\tbatch 440 loss 0.3128136694431305\n",
      "\tbatch 450 loss 0.4532315731048584\n",
      "\tbatch 460 loss 0.2683403789997101\n",
      "\tbatch 470 loss 0.2772108316421509\n",
      "\tbatch 480 loss 0.4142068922519684\n",
      "\tbatch 490 loss 0.3575819730758667\n",
      "\tbatch 500 loss 0.16137638688087463\n",
      "\tbatch 510 loss 0.38734427094459534\n",
      "\tbatch 520 loss 0.7319520115852356\n",
      "\tbatch 530 loss 0.5751681923866272\n",
      "\tbatch 540 loss 0.5309126973152161\n",
      "\tbatch 550 loss 0.4397222101688385\n",
      "\tbatch 560 loss 0.4237527549266815\n",
      "\tbatch 570 loss 0.26088300347328186\n",
      "\tbatch 580 loss 0.499583899974823\n",
      "\tbatch 590 loss 0.12403713911771774\n",
      "\tbatch 600 loss 0.39164453744888306\n",
      "\tbatch 610 loss 0.4264945387840271\n",
      "\tbatch 620 loss 0.5578926205635071\n",
      "\tbatch 630 loss 0.4914115071296692\n",
      "\tbatch 640 loss 0.4008708894252777\n",
      "\tbatch 650 loss 0.28758105635643005\n",
      "\tbatch 660 loss 0.5485042333602905\n",
      "\tbatch 670 loss 0.7853966951370239\n",
      "\tbatch 680 loss 0.1600354015827179\n",
      "\tbatch 690 loss 0.8853222131729126\n",
      "\tbatch 700 loss 0.18416883051395416\n",
      "\tbatch 710 loss 0.2709181010723114\n",
      "\tbatch 720 loss 0.3911249041557312\n",
      "\tbatch 730 loss 0.14748559892177582\n",
      "\tbatch 740 loss 0.4658164083957672\n",
      "\tbatch 750 loss 0.2567468583583832\n",
      "\tbatch 760 loss 0.4184616208076477\n",
      "\tbatch 770 loss 0.17742303013801575\n",
      "\tbatch 780 loss 0.8818891644477844\n",
      "\tbatch 790 loss 0.2384437620639801\n",
      "\tbatch 800 loss 0.8422124981880188\n",
      "\tbatch 810 loss 0.2668435275554657\n",
      "\tbatch 820 loss 0.43491679430007935\n",
      "\tbatch 830 loss 0.1677333116531372\n",
      "\tbatch 840 loss 0.3099883198738098\n",
      "\tbatch 850 loss 0.4091033339500427\n",
      "\tbatch 860 loss 0.34868091344833374\n",
      "\tbatch 870 loss 0.3733178675174713\n",
      "\tbatch 880 loss 0.37311890721321106\n",
      "\tbatch 890 loss 0.41702863574028015\n",
      "\tbatch 900 loss 0.12493592500686646\n",
      "\tbatch 910 loss 0.1631700098514557\n",
      "\tbatch 920 loss 1.1102721691131592\n",
      "\tbatch 930 loss 0.19308963418006897\n",
      "\tbatch 940 loss 0.16328147053718567\n",
      "\tbatch 950 loss 0.34721022844314575\n",
      "\tbatch 960 loss 0.192290261387825\n",
      "\tbatch 970 loss 0.4668732285499573\n",
      "\tbatch 980 loss 0.1382066011428833\n",
      "\tbatch 990 loss 0.6072055101394653\n",
      "\tbatch 1000 loss 0.28128886222839355\n",
      "\tbatch 1010 loss 0.29514700174331665\n",
      "\tbatch 1020 loss 0.28063616156578064\n",
      "\tbatch 1030 loss 0.4121261239051819\n",
      "\tbatch 1040 loss 0.2919484078884125\n",
      "\tbatch 1050 loss 0.42297863960266113\n",
      "\tbatch 1060 loss 0.32978808879852295\n",
      "\tbatch 1070 loss 0.14333799481391907\n",
      "\tbatch 1080 loss 0.12391025573015213\n",
      "\tbatch 1090 loss 0.17675001919269562\n",
      "\tbatch 1100 loss 0.29013851284980774\n",
      "\tbatch 1110 loss 0.09939274936914444\n",
      "\tbatch 1120 loss 0.12709693610668182\n",
      "\tbatch 1130 loss 0.44346752762794495\n",
      "\tbatch 1140 loss 0.464985191822052\n",
      "\tbatch 1150 loss 0.09255610406398773\n",
      "\tbatch 1160 loss 0.3900728225708008\n",
      "\tbatch 1170 loss 0.3646444082260132\n",
      "\tbatch 1180 loss 0.41402968764305115\n",
      "\tbatch 1190 loss 0.32589057087898254\n",
      "\tbatch 1200 loss 0.2837778925895691\n",
      "\tbatch 1210 loss 0.0986875519156456\n",
      "\tbatch 1220 loss 0.3485052287578583\n",
      "\tbatch 1230 loss 0.3619906008243561\n",
      "\tbatch 1240 loss 0.3960561454296112\n",
      "epoch 2 average_loss 0.4335246032208204\n",
      "\tbatch 0 loss 0.32733023166656494\n",
      "\tbatch 10 loss 0.6571778655052185\n",
      "\tbatch 20 loss 0.12928307056427002\n",
      "\tbatch 30 loss 0.18307913839817047\n",
      "\tbatch 40 loss 0.2003687471151352\n",
      "\tbatch 50 loss 0.17076954245567322\n",
      "\tbatch 60 loss 0.3181897699832916\n",
      "\tbatch 70 loss 0.17074979841709137\n",
      "\tbatch 80 loss 1.056544303894043\n",
      "\tbatch 90 loss 0.3032202422618866\n",
      "\tbatch 100 loss 0.15180066227912903\n",
      "\tbatch 110 loss 0.350209504365921\n",
      "\tbatch 120 loss 0.30768072605133057\n",
      "\tbatch 130 loss 0.10398309677839279\n",
      "\tbatch 140 loss 0.3759227991104126\n",
      "\tbatch 150 loss 0.359540194272995\n",
      "\tbatch 160 loss 1.161902904510498\n",
      "\tbatch 170 loss 1.2839348316192627\n",
      "\tbatch 180 loss 0.12098203599452972\n",
      "\tbatch 190 loss 0.5012038946151733\n",
      "\tbatch 200 loss 0.08908476680517197\n",
      "\tbatch 210 loss 0.3789187967777252\n",
      "\tbatch 220 loss 0.12316877394914627\n",
      "\tbatch 230 loss 0.5602598786354065\n",
      "\tbatch 240 loss 0.29927676916122437\n",
      "\tbatch 250 loss 0.4723392426967621\n",
      "\tbatch 260 loss 0.39660322666168213\n",
      "\tbatch 270 loss 0.1315678060054779\n",
      "\tbatch 280 loss 0.3598964810371399\n",
      "\tbatch 290 loss 0.26927563548088074\n",
      "\tbatch 300 loss 0.24723193049430847\n",
      "\tbatch 310 loss 0.16074258089065552\n",
      "\tbatch 320 loss 0.3131636679172516\n",
      "\tbatch 330 loss 0.32178404927253723\n",
      "\tbatch 340 loss 1.1595616340637207\n",
      "\tbatch 350 loss 0.3596004545688629\n",
      "\tbatch 360 loss 0.15380142629146576\n",
      "\tbatch 370 loss 0.5385082364082336\n",
      "\tbatch 380 loss 0.5896808505058289\n",
      "\tbatch 390 loss 2.1276259422302246\n",
      "\tbatch 400 loss 0.5795667171478271\n",
      "\tbatch 410 loss 0.4100375473499298\n",
      "\tbatch 420 loss 0.45848286151885986\n",
      "\tbatch 430 loss 0.5418182611465454\n",
      "\tbatch 440 loss 0.3682630658149719\n",
      "\tbatch 450 loss 0.17772038280963898\n",
      "\tbatch 460 loss 0.31704381108283997\n",
      "\tbatch 470 loss 0.1771886646747589\n",
      "\tbatch 480 loss 0.19556556642055511\n",
      "\tbatch 490 loss 0.2517849802970886\n",
      "\tbatch 500 loss 0.5712623000144958\n",
      "\tbatch 510 loss 0.40854987502098083\n",
      "\tbatch 520 loss 0.5316219329833984\n",
      "\tbatch 530 loss 0.18779990077018738\n",
      "\tbatch 540 loss 0.6078038215637207\n",
      "\tbatch 550 loss 0.26852455735206604\n",
      "\tbatch 560 loss 0.29451286792755127\n",
      "\tbatch 570 loss 0.13338902592658997\n",
      "\tbatch 580 loss 0.34666088223457336\n",
      "\tbatch 590 loss 0.10687316209077835\n",
      "\tbatch 600 loss 0.14778654277324677\n",
      "\tbatch 610 loss 0.39225199818611145\n",
      "\tbatch 620 loss 0.6873893737792969\n",
      "\tbatch 630 loss 0.39255744218826294\n",
      "\tbatch 640 loss 0.39011138677597046\n",
      "\tbatch 650 loss 0.22294999659061432\n",
      "\tbatch 660 loss 0.2604537606239319\n",
      "\tbatch 670 loss 0.8156561255455017\n",
      "\tbatch 680 loss 0.08762992173433304\n",
      "\tbatch 690 loss 0.9863294363021851\n",
      "\tbatch 700 loss 0.21736016869544983\n",
      "\tbatch 710 loss 0.3320198059082031\n",
      "\tbatch 720 loss 0.27232828736305237\n",
      "\tbatch 730 loss 0.2935853898525238\n",
      "\tbatch 740 loss 0.20856085419654846\n",
      "\tbatch 750 loss 0.23540598154067993\n",
      "\tbatch 760 loss 0.4472384750843048\n",
      "\tbatch 770 loss 0.20743653178215027\n",
      "\tbatch 780 loss 0.5128122568130493\n",
      "\tbatch 790 loss 0.20391862094402313\n",
      "\tbatch 800 loss 0.5281570553779602\n",
      "\tbatch 810 loss 0.1762348860502243\n",
      "\tbatch 820 loss 0.669462263584137\n",
      "\tbatch 830 loss 0.09338834136724472\n",
      "\tbatch 840 loss 0.08758596330881119\n",
      "\tbatch 850 loss 0.24386991560459137\n",
      "\tbatch 860 loss 0.2813376486301422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 870 loss 0.17766547203063965\n",
      "\tbatch 880 loss 0.5657104849815369\n",
      "\tbatch 890 loss 0.5572872757911682\n",
      "\tbatch 900 loss 0.05735976994037628\n",
      "\tbatch 910 loss 0.1286243349313736\n",
      "\tbatch 920 loss 0.3250236511230469\n",
      "\tbatch 930 loss 0.11744191497564316\n",
      "\tbatch 940 loss 0.09914986044168472\n",
      "\tbatch 950 loss 0.3142937123775482\n",
      "\tbatch 960 loss 0.12553085386753082\n",
      "\tbatch 970 loss 0.3371073007583618\n",
      "\tbatch 980 loss 0.19664400815963745\n",
      "\tbatch 990 loss 0.34613385796546936\n",
      "\tbatch 1000 loss 0.1299595683813095\n",
      "\tbatch 1010 loss 0.25620537996292114\n",
      "\tbatch 1020 loss 0.21991318464279175\n",
      "\tbatch 1030 loss 0.4050872027873993\n",
      "\tbatch 1040 loss 0.1001347228884697\n",
      "\tbatch 1050 loss 0.45135197043418884\n",
      "\tbatch 1060 loss 0.15114842355251312\n",
      "\tbatch 1070 loss 0.20417766273021698\n",
      "\tbatch 1080 loss 0.10642587393522263\n",
      "\tbatch 1090 loss 0.14291666448116302\n",
      "\tbatch 1100 loss 0.3068506717681885\n",
      "\tbatch 1110 loss 0.17509816586971283\n",
      "\tbatch 1120 loss 0.175991952419281\n",
      "\tbatch 1130 loss 0.42801761627197266\n",
      "\tbatch 1140 loss 0.683531641960144\n",
      "\tbatch 1150 loss 0.1224115788936615\n",
      "\tbatch 1160 loss 0.5936232209205627\n",
      "\tbatch 1170 loss 0.22459878027439117\n",
      "\tbatch 1180 loss 0.2976648807525635\n",
      "\tbatch 1190 loss 0.35311588644981384\n",
      "\tbatch 1200 loss 0.35879987478256226\n",
      "\tbatch 1210 loss 0.2926409840583801\n",
      "\tbatch 1220 loss 0.28273725509643555\n",
      "\tbatch 1230 loss 0.7114863991737366\n",
      "\tbatch 1240 loss 0.2835150957107544\n",
      "epoch 3 average_loss 0.356678809094429\n",
      "\tbatch 0 loss 0.303431898355484\n",
      "\tbatch 10 loss 0.5399001240730286\n",
      "\tbatch 20 loss 0.32441550493240356\n",
      "\tbatch 30 loss 0.1198817789554596\n",
      "\tbatch 40 loss 0.2319067418575287\n",
      "\tbatch 50 loss 0.33975639939308167\n",
      "\tbatch 60 loss 0.3125572204589844\n",
      "\tbatch 70 loss 0.33447083830833435\n",
      "\tbatch 80 loss 0.8277732729911804\n",
      "\tbatch 90 loss 0.3911761939525604\n",
      "\tbatch 100 loss 0.2604477107524872\n",
      "\tbatch 110 loss 0.36542272567749023\n",
      "\tbatch 120 loss 0.1203475221991539\n",
      "\tbatch 130 loss 0.18557226657867432\n",
      "\tbatch 140 loss 0.7151710391044617\n",
      "\tbatch 150 loss 0.5820934772491455\n",
      "\tbatch 160 loss 0.8676289916038513\n",
      "\tbatch 170 loss 1.6128026247024536\n",
      "\tbatch 180 loss 0.22439250349998474\n",
      "\tbatch 190 loss 0.6248045563697815\n",
      "\tbatch 200 loss 0.14385570585727692\n",
      "\tbatch 210 loss 0.40875083208084106\n",
      "\tbatch 220 loss 0.1488569676876068\n",
      "\tbatch 230 loss 0.37430471181869507\n",
      "\tbatch 240 loss 0.20830070972442627\n",
      "\tbatch 250 loss 0.8487202525138855\n",
      "\tbatch 260 loss 0.2632156014442444\n",
      "\tbatch 270 loss 0.17176178097724915\n",
      "\tbatch 280 loss 0.39835959672927856\n",
      "\tbatch 290 loss 0.44412028789520264\n",
      "\tbatch 300 loss 0.3289901614189148\n",
      "\tbatch 310 loss 0.20791424810886383\n",
      "\tbatch 320 loss 0.16337352991104126\n",
      "\tbatch 330 loss 0.31735897064208984\n",
      "\tbatch 340 loss 0.6690056324005127\n",
      "\tbatch 350 loss 0.4685664176940918\n",
      "\tbatch 360 loss 0.10412435978651047\n",
      "\tbatch 370 loss 0.40851929783821106\n",
      "\tbatch 380 loss 0.404623806476593\n",
      "\tbatch 390 loss 1.7155922651290894\n",
      "\tbatch 400 loss 0.20216599106788635\n",
      "\tbatch 410 loss 0.3812950849533081\n",
      "\tbatch 420 loss 0.4123106598854065\n",
      "\tbatch 430 loss 0.4451450705528259\n",
      "\tbatch 440 loss 0.2923898696899414\n",
      "\tbatch 450 loss 0.22937311232089996\n",
      "\tbatch 460 loss 0.24023737013339996\n",
      "\tbatch 470 loss 0.20034362375736237\n",
      "\tbatch 480 loss 0.16782590746879578\n",
      "\tbatch 490 loss 0.15628856420516968\n",
      "\tbatch 500 loss 0.11374928057193756\n",
      "\tbatch 510 loss 0.1518876850605011\n",
      "\tbatch 520 loss 0.4121738374233246\n",
      "\tbatch 530 loss 0.3250660002231598\n",
      "\tbatch 540 loss 0.510532021522522\n",
      "\tbatch 550 loss 0.30577582120895386\n",
      "\tbatch 560 loss 0.41524359583854675\n",
      "\tbatch 570 loss 0.17385394871234894\n",
      "\tbatch 580 loss 0.4283769130706787\n",
      "\tbatch 590 loss 0.19240190088748932\n",
      "\tbatch 600 loss 0.29284992814064026\n",
      "\tbatch 610 loss 0.3133869469165802\n",
      "\tbatch 620 loss 0.5517702698707581\n",
      "\tbatch 630 loss 0.19057142734527588\n",
      "\tbatch 640 loss 0.4973161816596985\n",
      "\tbatch 650 loss 0.14984264969825745\n",
      "\tbatch 660 loss 0.3055853247642517\n",
      "\tbatch 670 loss 0.7078947424888611\n",
      "\tbatch 680 loss 0.10811153799295425\n",
      "\tbatch 690 loss 1.005207896232605\n",
      "\tbatch 700 loss 0.15363895893096924\n",
      "\tbatch 710 loss 0.26713645458221436\n",
      "\tbatch 720 loss 0.27082231640815735\n",
      "\tbatch 730 loss 0.1422153264284134\n",
      "\tbatch 740 loss 0.47391477227211\n",
      "\tbatch 750 loss 0.13121812045574188\n",
      "\tbatch 760 loss 0.3734479546546936\n",
      "\tbatch 770 loss 0.12184365838766098\n",
      "\tbatch 780 loss 0.34435001015663147\n",
      "\tbatch 790 loss 0.11498363316059113\n",
      "\tbatch 800 loss 0.44386741518974304\n",
      "\tbatch 810 loss 0.1076127216219902\n",
      "\tbatch 820 loss 0.3477481007575989\n",
      "\tbatch 830 loss 0.11358056217432022\n",
      "\tbatch 840 loss 0.10827144980430603\n",
      "\tbatch 850 loss 0.2193749099969864\n",
      "\tbatch 860 loss 0.16408956050872803\n",
      "\tbatch 870 loss 0.18074271082878113\n",
      "\tbatch 880 loss 0.2585142254829407\n",
      "\tbatch 890 loss 0.2686990201473236\n",
      "\tbatch 900 loss 0.14652414619922638\n",
      "\tbatch 910 loss 0.13561998307704926\n",
      "\tbatch 920 loss 0.1943301409482956\n",
      "\tbatch 930 loss 0.06566145271062851\n",
      "\tbatch 940 loss 0.10972439497709274\n",
      "\tbatch 950 loss 0.24163347482681274\n",
      "\tbatch 960 loss 0.1083654910326004\n",
      "\tbatch 970 loss 0.5788260102272034\n",
      "\tbatch 980 loss 0.22214587032794952\n",
      "\tbatch 990 loss 0.2796693742275238\n",
      "\tbatch 1000 loss 0.1626363843679428\n",
      "\tbatch 1010 loss 0.2196129858493805\n",
      "\tbatch 1020 loss 0.16740933060646057\n",
      "\tbatch 1030 loss 0.2077626734972\n",
      "\tbatch 1040 loss 0.24943311512470245\n",
      "\tbatch 1050 loss 0.2814874053001404\n",
      "\tbatch 1060 loss 0.10631009191274643\n",
      "\tbatch 1070 loss 0.0912545695900917\n",
      "\tbatch 1080 loss 0.20229385793209076\n",
      "\tbatch 1090 loss 0.0690498948097229\n",
      "\tbatch 1100 loss 0.20996679365634918\n",
      "\tbatch 1110 loss 0.09945996105670929\n",
      "\tbatch 1120 loss 0.18952210247516632\n",
      "\tbatch 1130 loss 0.44834354519844055\n",
      "\tbatch 1140 loss 0.5247552990913391\n",
      "\tbatch 1150 loss 0.11311636865139008\n",
      "\tbatch 1160 loss 0.321014404296875\n",
      "\tbatch 1170 loss 0.2995935380458832\n",
      "\tbatch 1180 loss 0.2302331179380417\n",
      "\tbatch 1190 loss 0.25173264741897583\n",
      "\tbatch 1200 loss 0.552442193031311\n",
      "\tbatch 1210 loss 0.12223772704601288\n",
      "\tbatch 1220 loss 0.4581186771392822\n",
      "\tbatch 1230 loss 0.5044320225715637\n",
      "\tbatch 1240 loss 0.25980696082115173\n",
      "epoch 4 average_loss 0.3309391396328807\n",
      "\tbatch 0 loss 0.21082298457622528\n",
      "\tbatch 10 loss 0.22756771743297577\n",
      "\tbatch 20 loss 0.1065894216299057\n",
      "\tbatch 30 loss 0.06989683210849762\n",
      "\tbatch 40 loss 0.1765173226594925\n",
      "\tbatch 50 loss 0.13803471624851227\n",
      "\tbatch 60 loss 0.07996174693107605\n",
      "\tbatch 70 loss 0.16749994456768036\n",
      "\tbatch 80 loss 1.0267646312713623\n",
      "\tbatch 90 loss 0.2909567058086395\n",
      "\tbatch 100 loss 0.12221865355968475\n",
      "\tbatch 110 loss 0.24175004661083221\n",
      "\tbatch 120 loss 0.11104564368724823\n",
      "\tbatch 130 loss 0.04295865073800087\n",
      "\tbatch 140 loss 0.32326772809028625\n",
      "\tbatch 150 loss 0.28719189763069153\n",
      "\tbatch 160 loss 0.7102340459823608\n",
      "\tbatch 170 loss 1.723402738571167\n",
      "\tbatch 180 loss 0.2498755156993866\n",
      "\tbatch 190 loss 0.5486986041069031\n",
      "\tbatch 200 loss 0.15388092398643494\n",
      "\tbatch 210 loss 0.18250499665737152\n",
      "\tbatch 220 loss 0.10094722360372543\n",
      "\tbatch 230 loss 0.685562789440155\n",
      "\tbatch 240 loss 0.10929159075021744\n",
      "\tbatch 250 loss 0.1953953206539154\n",
      "\tbatch 260 loss 1.7098617553710938\n",
      "\tbatch 270 loss 0.10123816132545471\n",
      "\tbatch 280 loss 0.40269702672958374\n",
      "\tbatch 290 loss 0.4751112163066864\n",
      "\tbatch 300 loss 0.23370197415351868\n",
      "\tbatch 310 loss 0.24972257018089294\n",
      "\tbatch 320 loss 0.15917238593101501\n",
      "\tbatch 330 loss 0.5098503232002258\n",
      "\tbatch 340 loss 0.5747567415237427\n",
      "\tbatch 350 loss 0.34460458159446716\n",
      "\tbatch 360 loss 0.08011855185031891\n",
      "\tbatch 370 loss 0.24575640261173248\n",
      "\tbatch 380 loss 0.2834070324897766\n",
      "\tbatch 390 loss 2.0833866596221924\n",
      "\tbatch 400 loss 0.301106721162796\n",
      "\tbatch 410 loss 0.29513636231422424\n",
      "\tbatch 420 loss 0.5464023351669312\n",
      "\tbatch 430 loss 0.4631921350955963\n",
      "\tbatch 440 loss 0.282174289226532\n",
      "\tbatch 450 loss 0.16119761765003204\n",
      "\tbatch 460 loss 0.10523618012666702\n",
      "\tbatch 470 loss 0.14444661140441895\n",
      "\tbatch 480 loss 0.11036647856235504\n",
      "\tbatch 490 loss 0.19439053535461426\n",
      "\tbatch 500 loss 0.2706635296344757\n",
      "\tbatch 510 loss 0.33151254057884216\n",
      "\tbatch 520 loss 0.5598047971725464\n",
      "\tbatch 530 loss 0.11389605700969696\n",
      "\tbatch 540 loss 0.45557427406311035\n",
      "\tbatch 550 loss 0.14882659912109375\n",
      "\tbatch 560 loss 0.3667118549346924\n",
      "\tbatch 570 loss 0.1541796624660492\n",
      "\tbatch 580 loss 0.1607402265071869\n",
      "\tbatch 590 loss 0.07939418405294418\n",
      "\tbatch 600 loss 0.1676112562417984\n",
      "\tbatch 610 loss 0.16090138256549835\n",
      "\tbatch 620 loss 0.47071829438209534\n",
      "\tbatch 630 loss 0.3174542486667633\n",
      "\tbatch 640 loss 0.26841047406196594\n",
      "\tbatch 650 loss 0.17949458956718445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 660 loss 0.18044893443584442\n",
      "\tbatch 670 loss 0.7616437673568726\n",
      "\tbatch 680 loss 0.16184471547603607\n",
      "\tbatch 690 loss 0.6059863567352295\n",
      "\tbatch 700 loss 0.16168761253356934\n",
      "\tbatch 710 loss 0.24320170283317566\n",
      "\tbatch 720 loss 0.16078412532806396\n",
      "\tbatch 730 loss 0.31655699014663696\n",
      "\tbatch 740 loss 0.26823484897613525\n",
      "\tbatch 750 loss 0.13912390172481537\n",
      "\tbatch 760 loss 0.26303815841674805\n",
      "\tbatch 770 loss 0.10429435968399048\n",
      "\tbatch 780 loss 0.23925134539604187\n",
      "\tbatch 790 loss 0.13801126182079315\n",
      "\tbatch 800 loss 0.36881089210510254\n",
      "\tbatch 810 loss 0.10326389223337173\n",
      "\tbatch 820 loss 0.374003529548645\n",
      "\tbatch 830 loss 0.13586291670799255\n",
      "\tbatch 840 loss 0.10758095234632492\n",
      "\tbatch 850 loss 0.48613518476486206\n",
      "\tbatch 860 loss 0.16186101734638214\n",
      "\tbatch 870 loss 0.3840857148170471\n",
      "\tbatch 880 loss 0.3654175102710724\n",
      "\tbatch 890 loss 0.2680565118789673\n",
      "\tbatch 900 loss 0.12471349537372589\n",
      "\tbatch 910 loss 0.10053553432226181\n",
      "\tbatch 920 loss 0.3398781418800354\n",
      "\tbatch 930 loss 0.14305470883846283\n",
      "\tbatch 940 loss 0.08043497800827026\n",
      "\tbatch 950 loss 0.31315380334854126\n",
      "\tbatch 960 loss 0.1812363862991333\n",
      "\tbatch 970 loss 0.587925136089325\n",
      "\tbatch 980 loss 0.18274250626564026\n",
      "\tbatch 990 loss 0.3092564642429352\n",
      "\tbatch 1000 loss 0.2507322132587433\n",
      "\tbatch 1010 loss 0.2178308665752411\n",
      "\tbatch 1020 loss 0.19688761234283447\n",
      "\tbatch 1030 loss 0.14608201384544373\n",
      "\tbatch 1040 loss 0.22217316925525665\n",
      "\tbatch 1050 loss 0.41925638914108276\n",
      "\tbatch 1060 loss 0.14704424142837524\n",
      "\tbatch 1070 loss 0.13762830197811127\n",
      "\tbatch 1080 loss 0.6008893251419067\n",
      "\tbatch 1090 loss 0.12348954379558563\n",
      "\tbatch 1100 loss 0.34300199151039124\n",
      "\tbatch 1110 loss 0.12657950818538666\n",
      "\tbatch 1120 loss 0.094570092856884\n",
      "\tbatch 1130 loss 0.2291652262210846\n",
      "\tbatch 1140 loss 0.5542097091674805\n",
      "\tbatch 1150 loss 0.11886907368898392\n",
      "\tbatch 1160 loss 0.3570789694786072\n",
      "\tbatch 1170 loss 0.27299192547798157\n",
      "\tbatch 1180 loss 0.3720358908176422\n",
      "\tbatch 1190 loss 0.1767023801803589\n",
      "\tbatch 1200 loss 0.3279675841331482\n",
      "\tbatch 1210 loss 0.07465695589780807\n",
      "\tbatch 1220 loss 0.3864568769931793\n",
      "\tbatch 1230 loss 0.48228171467781067\n",
      "\tbatch 1240 loss 0.29601022601127625\n",
      "epoch 5 average_loss 0.2997315985664725\n",
      "\tbatch 0 loss 0.23149697482585907\n",
      "\tbatch 10 loss 0.3497803509235382\n",
      "\tbatch 20 loss 0.09132646769285202\n",
      "\tbatch 30 loss 0.05665523186326027\n",
      "\tbatch 40 loss 0.10020191222429276\n",
      "\tbatch 50 loss 0.12405087053775787\n",
      "\tbatch 60 loss 0.16022969782352448\n",
      "\tbatch 70 loss 0.24256743490695953\n",
      "\tbatch 80 loss 1.2578927278518677\n",
      "\tbatch 90 loss 0.3682514727115631\n",
      "\tbatch 100 loss 0.167522594332695\n",
      "\tbatch 110 loss 0.4362698495388031\n",
      "\tbatch 120 loss 0.19367828965187073\n",
      "\tbatch 130 loss 0.07618596404790878\n",
      "\tbatch 140 loss 0.3816891312599182\n",
      "\tbatch 150 loss 0.37493276596069336\n",
      "\tbatch 160 loss 0.9723979234695435\n",
      "\tbatch 170 loss 1.7341784238815308\n",
      "\tbatch 180 loss 0.5433658361434937\n",
      "\tbatch 190 loss 0.3737872242927551\n",
      "\tbatch 200 loss 0.08789314329624176\n",
      "\tbatch 210 loss 0.12700878083705902\n",
      "\tbatch 220 loss 0.13403558731079102\n",
      "\tbatch 230 loss 0.32891055941581726\n",
      "\tbatch 240 loss 0.1316569745540619\n",
      "\tbatch 250 loss 0.2826480567455292\n",
      "\tbatch 260 loss 1.0893921852111816\n",
      "\tbatch 270 loss 0.09464461356401443\n",
      "\tbatch 280 loss 0.40327346324920654\n",
      "\tbatch 290 loss 0.33403125405311584\n",
      "\tbatch 300 loss 0.15229763090610504\n",
      "\tbatch 310 loss 0.09592269361019135\n",
      "\tbatch 320 loss 0.10058221220970154\n",
      "\tbatch 330 loss 0.2404075562953949\n",
      "\tbatch 340 loss 1.1186200380325317\n",
      "\tbatch 350 loss 0.5260483026504517\n",
      "\tbatch 360 loss 0.13216698169708252\n",
      "\tbatch 370 loss 0.4244261682033539\n",
      "\tbatch 380 loss 0.39479860663414\n",
      "\tbatch 390 loss 1.9301981925964355\n",
      "\tbatch 400 loss 0.7549598813056946\n",
      "\tbatch 410 loss 0.40345194935798645\n",
      "\tbatch 420 loss 0.5930779576301575\n",
      "\tbatch 430 loss 0.3270980715751648\n",
      "\tbatch 440 loss 0.13077500462532043\n",
      "\tbatch 450 loss 0.13557179272174835\n",
      "\tbatch 460 loss 0.09587086737155914\n",
      "\tbatch 470 loss 0.18748508393764496\n",
      "\tbatch 480 loss 0.30023229122161865\n",
      "\tbatch 490 loss 0.21049003303050995\n",
      "\tbatch 500 loss 0.3337503969669342\n",
      "\tbatch 510 loss 0.35252493619918823\n",
      "\tbatch 520 loss 0.6688945889472961\n",
      "\tbatch 530 loss 0.1375270038843155\n",
      "\tbatch 540 loss 0.18448255956172943\n",
      "\tbatch 550 loss 0.8122588396072388\n",
      "\tbatch 560 loss 0.27280429005622864\n",
      "\tbatch 570 loss 0.1792701929807663\n",
      "\tbatch 580 loss 0.23836766183376312\n",
      "\tbatch 590 loss 0.08454836159944534\n",
      "\tbatch 600 loss 0.32632043957710266\n",
      "\tbatch 610 loss 0.2272181659936905\n",
      "\tbatch 620 loss 0.783575713634491\n",
      "\tbatch 630 loss 0.8414773941040039\n",
      "\tbatch 640 loss 0.14451850950717926\n",
      "\tbatch 650 loss 0.14741162955760956\n",
      "\tbatch 660 loss 0.2868231236934662\n",
      "\tbatch 670 loss 0.5169722437858582\n",
      "\tbatch 680 loss 0.10232037305831909\n",
      "\tbatch 690 loss 0.5680524706840515\n",
      "\tbatch 700 loss 0.11924821138381958\n",
      "\tbatch 710 loss 0.16412097215652466\n",
      "\tbatch 720 loss 0.2167125791311264\n",
      "\tbatch 730 loss 0.2288137823343277\n",
      "\tbatch 740 loss 0.4104628562927246\n",
      "\tbatch 750 loss 0.13456086814403534\n",
      "\tbatch 760 loss 0.2405213862657547\n",
      "\tbatch 770 loss 0.08268561214208603\n",
      "\tbatch 780 loss 0.18860551714897156\n",
      "\tbatch 790 loss 0.1202455386519432\n",
      "\tbatch 800 loss 0.30172744393348694\n",
      "\tbatch 810 loss 0.12045768648386002\n",
      "\tbatch 820 loss 0.31730204820632935\n",
      "\tbatch 830 loss 0.08150933682918549\n",
      "\tbatch 840 loss 0.11075901985168457\n",
      "\tbatch 850 loss 0.28534215688705444\n",
      "\tbatch 860 loss 0.19129754602909088\n",
      "\tbatch 870 loss 0.16499002277851105\n",
      "\tbatch 880 loss 0.120903879404068\n",
      "\tbatch 890 loss 0.2833128273487091\n",
      "\tbatch 900 loss 0.14509284496307373\n",
      "\tbatch 910 loss 0.11485524475574493\n",
      "\tbatch 920 loss 0.35114020109176636\n",
      "\tbatch 930 loss 0.1618676632642746\n",
      "\tbatch 940 loss 0.2760898470878601\n",
      "\tbatch 950 loss 0.32297226786613464\n",
      "\tbatch 960 loss 0.08779314160346985\n",
      "\tbatch 970 loss 0.3263638913631439\n",
      "\tbatch 980 loss 0.25706684589385986\n",
      "\tbatch 990 loss 0.25717028975486755\n",
      "\tbatch 1000 loss 0.17506632208824158\n",
      "\tbatch 1010 loss 0.10173703730106354\n",
      "\tbatch 1020 loss 0.15054334700107574\n",
      "\tbatch 1030 loss 0.14048679172992706\n",
      "\tbatch 1040 loss 0.20886561274528503\n",
      "\tbatch 1050 loss 0.4059566557407379\n",
      "\tbatch 1060 loss 0.31290969252586365\n",
      "\tbatch 1070 loss 0.14072872698307037\n",
      "\tbatch 1080 loss 0.40044736862182617\n",
      "\tbatch 1090 loss 0.07459534704685211\n",
      "\tbatch 1100 loss 0.2267073541879654\n",
      "\tbatch 1110 loss 0.06362880766391754\n",
      "\tbatch 1120 loss 0.057999152690172195\n",
      "\tbatch 1130 loss 0.1073703020811081\n",
      "\tbatch 1140 loss 0.6961128115653992\n",
      "\tbatch 1150 loss 0.1436759978532791\n",
      "\tbatch 1160 loss 0.4100901484489441\n",
      "\tbatch 1170 loss 0.30362769961357117\n",
      "\tbatch 1180 loss 0.18143300712108612\n",
      "\tbatch 1190 loss 0.15672890841960907\n",
      "\tbatch 1200 loss 0.2486085593700409\n",
      "\tbatch 1210 loss 0.30135872960090637\n",
      "\tbatch 1220 loss 0.22063764929771423\n",
      "\tbatch 1230 loss 0.3928152322769165\n",
      "\tbatch 1240 loss 0.2692033350467682\n",
      "epoch 6 average_loss 0.28256005033403636\n",
      "\tbatch 0 loss 0.35970523953437805\n",
      "\tbatch 10 loss 0.31650176644325256\n",
      "\tbatch 20 loss 0.06256502121686935\n",
      "\tbatch 30 loss 0.08340748399496078\n",
      "\tbatch 40 loss 0.08346307277679443\n",
      "\tbatch 50 loss 0.07992815226316452\n",
      "\tbatch 60 loss 0.10872514545917511\n",
      "\tbatch 70 loss 0.23270578682422638\n",
      "\tbatch 80 loss 0.9366128444671631\n",
      "\tbatch 90 loss 0.30091533064842224\n",
      "\tbatch 100 loss 0.12035150080919266\n",
      "\tbatch 110 loss 0.3162482678890228\n",
      "\tbatch 120 loss 0.06942172348499298\n",
      "\tbatch 130 loss 0.05779529735445976\n",
      "\tbatch 140 loss 0.18097038567066193\n",
      "\tbatch 150 loss 0.17536373436450958\n",
      "\tbatch 160 loss 0.5455499887466431\n",
      "\tbatch 170 loss 1.3930662870407104\n",
      "\tbatch 180 loss 0.14345960319042206\n",
      "\tbatch 190 loss 0.31624835729599\n",
      "\tbatch 200 loss 0.12668253481388092\n",
      "\tbatch 210 loss 0.05622044950723648\n",
      "\tbatch 220 loss 0.14818553626537323\n",
      "\tbatch 230 loss 0.18475186824798584\n",
      "\tbatch 240 loss 0.06113223731517792\n",
      "\tbatch 250 loss 0.2208680361509323\n",
      "\tbatch 260 loss 0.1969171017408371\n",
      "\tbatch 270 loss 0.11408863961696625\n",
      "\tbatch 280 loss 0.31585586071014404\n",
      "\tbatch 290 loss 0.14614561200141907\n",
      "\tbatch 300 loss 0.15832975506782532\n",
      "\tbatch 310 loss 0.2508704960346222\n",
      "\tbatch 320 loss 0.0833420678973198\n",
      "\tbatch 330 loss 0.2015109360218048\n",
      "\tbatch 340 loss 0.6925299763679504\n",
      "\tbatch 350 loss 0.3471297025680542\n",
      "\tbatch 360 loss 0.05103924870491028\n",
      "\tbatch 370 loss 0.08599966019392014\n",
      "\tbatch 380 loss 0.10282247513532639\n",
      "\tbatch 390 loss 2.0419762134552\n",
      "\tbatch 400 loss 0.5349391102790833\n",
      "\tbatch 410 loss 0.4772196412086487\n",
      "\tbatch 420 loss 0.24256011843681335\n",
      "\tbatch 430 loss 0.12453543394804001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 440 loss 0.12469185888767242\n",
      "\tbatch 450 loss 0.3831814229488373\n",
      "\tbatch 460 loss 0.18920452892780304\n",
      "\tbatch 470 loss 0.21841415762901306\n",
      "\tbatch 480 loss 0.17228898406028748\n",
      "\tbatch 490 loss 0.27460986375808716\n",
      "\tbatch 500 loss 0.14259348809719086\n",
      "\tbatch 510 loss 0.3265920579433441\n",
      "\tbatch 520 loss 0.4515770971775055\n",
      "\tbatch 530 loss 0.1875361055135727\n",
      "\tbatch 540 loss 0.13719378411769867\n",
      "\tbatch 550 loss 0.19186164438724518\n",
      "\tbatch 560 loss 0.3007044494152069\n",
      "\tbatch 570 loss 0.10927463322877884\n",
      "\tbatch 580 loss 0.13818173110485077\n",
      "\tbatch 590 loss 0.09450472146272659\n",
      "\tbatch 600 loss 0.2083120495080948\n",
      "\tbatch 610 loss 0.20860172808170319\n",
      "\tbatch 620 loss 0.7686324715614319\n",
      "\tbatch 630 loss 0.13229306042194366\n",
      "\tbatch 640 loss 0.5198361277580261\n",
      "\tbatch 650 loss 0.16092243790626526\n",
      "\tbatch 660 loss 0.17797516286373138\n",
      "\tbatch 670 loss 0.4191100001335144\n",
      "\tbatch 680 loss 0.1033569723367691\n",
      "\tbatch 690 loss 0.49030601978302\n",
      "\tbatch 700 loss 0.13312233984470367\n",
      "\tbatch 710 loss 0.15946145355701447\n",
      "\tbatch 720 loss 0.21384674310684204\n",
      "\tbatch 730 loss 0.3815367817878723\n",
      "\tbatch 740 loss 0.12052936851978302\n",
      "\tbatch 750 loss 0.18104572594165802\n",
      "\tbatch 760 loss 0.2113160789012909\n",
      "\tbatch 770 loss 0.08730316907167435\n",
      "\tbatch 780 loss 0.19481630623340607\n",
      "\tbatch 790 loss 0.13679473102092743\n",
      "\tbatch 800 loss 0.36708757281303406\n",
      "\tbatch 810 loss 0.13673974573612213\n",
      "\tbatch 820 loss 0.29771503806114197\n",
      "\tbatch 830 loss 0.07668279111385345\n",
      "\tbatch 840 loss 0.0735713317990303\n",
      "\tbatch 850 loss 0.5420727133750916\n",
      "\tbatch 860 loss 0.06345460563898087\n",
      "\tbatch 870 loss 0.17838791012763977\n",
      "\tbatch 880 loss 0.29837244749069214\n",
      "\tbatch 890 loss 0.12301591038703918\n",
      "\tbatch 900 loss 0.07272800803184509\n",
      "\tbatch 910 loss 0.07180888950824738\n",
      "\tbatch 920 loss 0.24561287462711334\n",
      "\tbatch 930 loss 0.7340192794799805\n",
      "\tbatch 940 loss 0.0659438893198967\n",
      "\tbatch 950 loss 0.2592797875404358\n",
      "\tbatch 960 loss 0.12855982780456543\n",
      "\tbatch 970 loss 0.2560122609138489\n",
      "\tbatch 980 loss 0.39723771810531616\n",
      "\tbatch 990 loss 0.40521305799484253\n",
      "\tbatch 1000 loss 0.09481444209814072\n",
      "\tbatch 1010 loss 0.1617051661014557\n",
      "\tbatch 1020 loss 0.17371930181980133\n",
      "\tbatch 1030 loss 0.16447396576404572\n",
      "\tbatch 1040 loss 0.10678111016750336\n",
      "\tbatch 1050 loss 0.3067910671234131\n",
      "\tbatch 1060 loss 0.20100584626197815\n",
      "\tbatch 1070 loss 0.1789635419845581\n",
      "\tbatch 1080 loss 0.13386096060276031\n",
      "\tbatch 1090 loss 0.08595918118953705\n",
      "\tbatch 1100 loss 0.5038003325462341\n",
      "\tbatch 1110 loss 0.08721546828746796\n",
      "\tbatch 1120 loss 0.19329647719860077\n",
      "\tbatch 1130 loss 0.22781546413898468\n",
      "\tbatch 1140 loss 0.4915958642959595\n",
      "\tbatch 1150 loss 0.07985934615135193\n",
      "\tbatch 1160 loss 0.3060663938522339\n",
      "\tbatch 1170 loss 0.25330299139022827\n",
      "\tbatch 1180 loss 0.1466728299856186\n",
      "\tbatch 1190 loss 0.13606108725070953\n",
      "\tbatch 1200 loss 0.18680836260318756\n",
      "\tbatch 1210 loss 0.048893898725509644\n",
      "\tbatch 1220 loss 0.3785651922225952\n",
      "\tbatch 1230 loss 0.24644018709659576\n",
      "\tbatch 1240 loss 0.2156948447227478\n",
      "epoch 7 average_loss 0.2484205048620701\n",
      "\tbatch 0 loss 0.12810435891151428\n",
      "\tbatch 10 loss 0.212607279419899\n",
      "\tbatch 20 loss 0.0659516304731369\n",
      "\tbatch 30 loss 0.09936913847923279\n",
      "\tbatch 40 loss 0.06368298828601837\n",
      "\tbatch 50 loss 0.08911856263875961\n",
      "\tbatch 60 loss 0.12794643640518188\n",
      "\tbatch 70 loss 0.21245066821575165\n",
      "\tbatch 80 loss 0.607728898525238\n",
      "\tbatch 90 loss 0.23402459919452667\n",
      "\tbatch 100 loss 0.08194175362586975\n",
      "\tbatch 110 loss 0.34306275844573975\n",
      "\tbatch 120 loss 0.0725412666797638\n",
      "\tbatch 130 loss 0.06009115278720856\n",
      "\tbatch 140 loss 0.16519570350646973\n",
      "\tbatch 150 loss 0.1064784973859787\n",
      "\tbatch 160 loss 0.6996550559997559\n",
      "\tbatch 170 loss 1.2409188747406006\n",
      "\tbatch 180 loss 0.11956118792295456\n",
      "\tbatch 190 loss 0.4938625395298004\n",
      "\tbatch 200 loss 0.12063639611005783\n",
      "\tbatch 210 loss 0.1262928694486618\n",
      "\tbatch 220 loss 0.25001800060272217\n",
      "\tbatch 230 loss 0.21330475807189941\n",
      "\tbatch 240 loss 0.08772234618663788\n",
      "\tbatch 250 loss 0.19218765199184418\n",
      "\tbatch 260 loss 0.7110615372657776\n",
      "\tbatch 270 loss 0.08920889347791672\n",
      "\tbatch 280 loss 0.2763744294643402\n",
      "\tbatch 290 loss 0.08321783691644669\n",
      "\tbatch 300 loss 0.2561662495136261\n",
      "\tbatch 310 loss 0.14674243330955505\n",
      "\tbatch 320 loss 0.07346774637699127\n",
      "\tbatch 330 loss 0.31277886033058167\n",
      "\tbatch 340 loss 0.547603189945221\n",
      "\tbatch 350 loss 0.39993759989738464\n",
      "\tbatch 360 loss 0.046145133674144745\n",
      "\tbatch 370 loss 0.06359603255987167\n",
      "\tbatch 380 loss 0.17863686382770538\n",
      "\tbatch 390 loss 0.850055456161499\n",
      "\tbatch 400 loss 0.11772468686103821\n",
      "\tbatch 410 loss 0.5475114583969116\n",
      "\tbatch 420 loss 0.35604286193847656\n",
      "\tbatch 430 loss 0.29332301020622253\n",
      "\tbatch 440 loss 0.12447977066040039\n",
      "\tbatch 450 loss 0.07833298295736313\n",
      "\tbatch 460 loss 0.08873991668224335\n",
      "\tbatch 470 loss 0.11280301213264465\n",
      "\tbatch 480 loss 0.18673522770404816\n",
      "\tbatch 490 loss 0.17556291818618774\n",
      "\tbatch 500 loss 0.10759858042001724\n",
      "\tbatch 510 loss 0.18148820102214813\n",
      "\tbatch 520 loss 0.5127527117729187\n",
      "\tbatch 530 loss 0.10953589528799057\n",
      "\tbatch 540 loss 0.09326004981994629\n",
      "\tbatch 550 loss 0.4135952889919281\n",
      "\tbatch 560 loss 0.3733506202697754\n",
      "\tbatch 570 loss 0.09258738160133362\n",
      "\tbatch 580 loss 0.07046059519052505\n",
      "\tbatch 590 loss 0.07338063418865204\n",
      "\tbatch 600 loss 0.09815877676010132\n",
      "\tbatch 610 loss 0.09028132259845734\n",
      "\tbatch 620 loss 0.32290583848953247\n",
      "\tbatch 630 loss 0.3885910212993622\n",
      "\tbatch 640 loss 0.12996654212474823\n",
      "\tbatch 650 loss 0.14400914311408997\n",
      "\tbatch 660 loss 0.2412288784980774\n",
      "\tbatch 670 loss 1.0177664756774902\n",
      "\tbatch 680 loss 0.11150407791137695\n",
      "\tbatch 690 loss 0.3899296820163727\n",
      "\tbatch 700 loss 0.12958510220050812\n",
      "\tbatch 710 loss 0.17615579068660736\n",
      "\tbatch 720 loss 0.06845319271087646\n",
      "\tbatch 730 loss 0.09920965880155563\n",
      "\tbatch 740 loss 0.2547760605812073\n",
      "\tbatch 750 loss 0.10010728985071182\n",
      "\tbatch 760 loss 0.2495071291923523\n",
      "\tbatch 770 loss 0.10165335983037949\n",
      "\tbatch 780 loss 0.09690899401903152\n",
      "\tbatch 790 loss 0.08727896958589554\n",
      "\tbatch 800 loss 0.36385539174079895\n",
      "\tbatch 810 loss 0.08329153060913086\n",
      "\tbatch 820 loss 0.345030277967453\n",
      "\tbatch 830 loss 0.09827388823032379\n",
      "\tbatch 840 loss 0.07286620140075684\n",
      "\tbatch 850 loss 0.30023694038391113\n",
      "\tbatch 860 loss 0.1123540997505188\n",
      "\tbatch 870 loss 0.16148266196250916\n",
      "\tbatch 880 loss 0.26517218351364136\n",
      "\tbatch 890 loss 0.15148529410362244\n",
      "\tbatch 900 loss 0.1847083419561386\n",
      "\tbatch 910 loss 0.10340873152017593\n",
      "\tbatch 920 loss 0.15249137580394745\n",
      "\tbatch 930 loss 0.08237806707620621\n",
      "\tbatch 940 loss 0.12193380296230316\n",
      "\tbatch 950 loss 0.2408597618341446\n",
      "\tbatch 960 loss 0.17884710431098938\n",
      "\tbatch 970 loss 0.14455768465995789\n",
      "\tbatch 980 loss 0.11854690313339233\n",
      "\tbatch 990 loss 0.24801261723041534\n",
      "\tbatch 1000 loss 0.07995013147592545\n",
      "\tbatch 1010 loss 0.04855016618967056\n",
      "\tbatch 1020 loss 0.07187271863222122\n",
      "\tbatch 1030 loss 0.08040839433670044\n",
      "\tbatch 1040 loss 0.059167683124542236\n",
      "\tbatch 1050 loss 0.2301967740058899\n",
      "\tbatch 1060 loss 0.1878928542137146\n",
      "\tbatch 1070 loss 0.16925612092018127\n",
      "\tbatch 1080 loss 0.126255065202713\n",
      "\tbatch 1090 loss 0.048913080245256424\n",
      "\tbatch 1100 loss 0.3162318170070648\n",
      "\tbatch 1110 loss 0.06388592720031738\n",
      "\tbatch 1120 loss 0.11693509668111801\n",
      "\tbatch 1130 loss 0.1557178497314453\n",
      "\tbatch 1140 loss 0.5114392638206482\n",
      "\tbatch 1150 loss 0.06862911581993103\n",
      "\tbatch 1160 loss 0.25790122151374817\n",
      "\tbatch 1170 loss 0.23795932531356812\n",
      "\tbatch 1180 loss 0.11642833054065704\n",
      "\tbatch 1190 loss 0.15536276996135712\n",
      "\tbatch 1200 loss 0.14076325297355652\n",
      "\tbatch 1210 loss 0.08515723794698715\n",
      "\tbatch 1220 loss 0.15702427923679352\n",
      "\tbatch 1230 loss 0.34644511342048645\n",
      "\tbatch 1240 loss 0.1937371790409088\n",
      "epoch 8 average_loss 0.21774259823560715\n",
      "\tbatch 0 loss 0.08308247476816177\n",
      "\tbatch 10 loss 0.13074375689029694\n",
      "\tbatch 20 loss 0.07575411349534988\n",
      "\tbatch 30 loss 0.06403004378080368\n",
      "\tbatch 40 loss 0.04336144030094147\n",
      "\tbatch 50 loss 0.08123637735843658\n",
      "\tbatch 60 loss 0.04072790965437889\n",
      "\tbatch 70 loss 0.16821736097335815\n",
      "\tbatch 80 loss 0.6148834824562073\n",
      "\tbatch 90 loss 0.256111741065979\n",
      "\tbatch 100 loss 0.08558695018291473\n",
      "\tbatch 110 loss 0.3620196580886841\n",
      "\tbatch 120 loss 0.05195178836584091\n",
      "\tbatch 130 loss 0.043153755366802216\n",
      "\tbatch 140 loss 0.11839870363473892\n",
      "\tbatch 150 loss 0.12095442414283752\n",
      "\tbatch 160 loss 0.4939742386341095\n",
      "\tbatch 170 loss 2.084735155105591\n",
      "\tbatch 180 loss 0.14860787987709045\n",
      "\tbatch 190 loss 0.24252113699913025\n",
      "\tbatch 200 loss 0.06583334505558014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 210 loss 0.2503192722797394\n",
      "\tbatch 220 loss 0.06442634761333466\n",
      "\tbatch 230 loss 0.20193476974964142\n",
      "\tbatch 240 loss 0.06408196687698364\n",
      "\tbatch 250 loss 0.3021225333213806\n",
      "\tbatch 260 loss 0.1415543556213379\n",
      "\tbatch 270 loss 0.07356710731983185\n",
      "\tbatch 280 loss 0.30655437707901\n",
      "\tbatch 290 loss 0.18126896023750305\n",
      "\tbatch 300 loss 0.2924920618534088\n",
      "\tbatch 310 loss 0.3535711169242859\n",
      "\tbatch 320 loss 0.12742222845554352\n",
      "\tbatch 330 loss 0.1808663308620453\n",
      "\tbatch 340 loss 0.42845916748046875\n",
      "\tbatch 350 loss 0.46285295486450195\n",
      "\tbatch 360 loss 0.07064713537693024\n",
      "\tbatch 370 loss 0.08737208694219589\n",
      "\tbatch 380 loss 0.2204989790916443\n",
      "\tbatch 390 loss 0.7253632545471191\n",
      "\tbatch 400 loss 0.2041158378124237\n",
      "\tbatch 410 loss 0.5177321434020996\n",
      "\tbatch 420 loss 0.2184102088212967\n",
      "\tbatch 430 loss 0.21866640448570251\n",
      "\tbatch 440 loss 0.22540943324565887\n",
      "\tbatch 450 loss 0.1571485698223114\n",
      "\tbatch 460 loss 0.053552448749542236\n",
      "\tbatch 470 loss 0.14947456121444702\n",
      "\tbatch 480 loss 0.12872938811779022\n",
      "\tbatch 490 loss 0.09770242869853973\n",
      "\tbatch 500 loss 0.22848929464817047\n",
      "\tbatch 510 loss 0.14929881691932678\n",
      "\tbatch 520 loss 0.5142644047737122\n",
      "\tbatch 530 loss 0.09242241829633713\n",
      "\tbatch 540 loss 0.3026181161403656\n",
      "\tbatch 550 loss 0.08500842750072479\n",
      "\tbatch 560 loss 0.3134523034095764\n",
      "\tbatch 570 loss 0.17177511751651764\n",
      "\tbatch 580 loss 0.06993162631988525\n",
      "\tbatch 590 loss 0.10950236767530441\n",
      "\tbatch 600 loss 0.20806944370269775\n",
      "\tbatch 610 loss 0.12353768944740295\n",
      "\tbatch 620 loss 0.43204420804977417\n",
      "\tbatch 630 loss 0.2917402386665344\n",
      "\tbatch 640 loss 0.12619519233703613\n",
      "\tbatch 650 loss 0.12380708009004593\n",
      "\tbatch 660 loss 0.1272575855255127\n",
      "\tbatch 670 loss 0.43588787317276\n",
      "\tbatch 680 loss 0.06381692737340927\n",
      "\tbatch 690 loss 0.4191349744796753\n",
      "\tbatch 700 loss 0.1555940955877304\n",
      "\tbatch 710 loss 0.22451086342334747\n",
      "\tbatch 720 loss 0.2255222350358963\n",
      "\tbatch 730 loss 0.0826474130153656\n",
      "\tbatch 740 loss 0.1176920160651207\n",
      "\tbatch 750 loss 0.13697698712348938\n",
      "\tbatch 760 loss 0.3267470896244049\n",
      "\tbatch 770 loss 0.07108693569898605\n",
      "\tbatch 780 loss 0.17069120705127716\n",
      "\tbatch 790 loss 0.07068339735269547\n",
      "\tbatch 800 loss 0.2472553700208664\n",
      "\tbatch 810 loss 0.1188681572675705\n",
      "\tbatch 820 loss 0.2170971930027008\n",
      "\tbatch 830 loss 0.06278000771999359\n",
      "\tbatch 840 loss 0.08899835497140884\n",
      "\tbatch 850 loss 0.425387978553772\n",
      "\tbatch 860 loss 0.1127467006444931\n",
      "\tbatch 870 loss 0.1400454044342041\n",
      "\tbatch 880 loss 0.1559351086616516\n",
      "\tbatch 890 loss 0.12080979347229004\n",
      "\tbatch 900 loss 0.13369210064411163\n",
      "\tbatch 910 loss 0.09429531544446945\n",
      "\tbatch 920 loss 0.4503403902053833\n",
      "\tbatch 930 loss 0.06929264962673187\n",
      "\tbatch 940 loss 0.1574377566576004\n",
      "\tbatch 950 loss 0.1770837903022766\n",
      "\tbatch 960 loss 0.09236839413642883\n",
      "\tbatch 970 loss 0.2373143434524536\n",
      "\tbatch 980 loss 0.3759387731552124\n",
      "\tbatch 990 loss 0.3033978044986725\n",
      "\tbatch 1000 loss 0.08928146958351135\n",
      "\tbatch 1010 loss 0.05446873605251312\n",
      "\tbatch 1020 loss 0.12826083600521088\n",
      "\tbatch 1030 loss 0.0676538422703743\n",
      "\tbatch 1040 loss 0.046149417757987976\n",
      "\tbatch 1050 loss 0.19293533265590668\n",
      "\tbatch 1060 loss 0.03311372920870781\n",
      "\tbatch 1070 loss 0.1545337736606598\n",
      "\tbatch 1080 loss 0.23923681676387787\n",
      "\tbatch 1090 loss 0.0626349151134491\n",
      "\tbatch 1100 loss 0.27292096614837646\n",
      "\tbatch 1110 loss 0.024480924010276794\n",
      "\tbatch 1120 loss 0.07990332692861557\n",
      "\tbatch 1130 loss 0.1325623095035553\n",
      "\tbatch 1140 loss 0.6184132695198059\n",
      "\tbatch 1150 loss 0.06608325242996216\n",
      "\tbatch 1160 loss 0.17754743993282318\n",
      "\tbatch 1170 loss 0.36149540543556213\n",
      "\tbatch 1180 loss 0.10733561962842941\n",
      "\tbatch 1190 loss 0.15206696093082428\n",
      "\tbatch 1200 loss 0.17643438279628754\n",
      "\tbatch 1210 loss 0.06930826604366302\n",
      "\tbatch 1220 loss 0.16592644155025482\n",
      "\tbatch 1230 loss 0.4252956807613373\n",
      "\tbatch 1240 loss 0.16963635385036469\n",
      "epoch 9 average_loss 0.2021804259389639\n"
     ]
    }
   ],
   "source": [
    "eff_model.to(device)\n",
    "#ce_loss = torch.nn.CrossEntropyLoss(torch.tensor([1,10.0,10.0,10,10]).to(device))\n",
    "#ce_loss = torch.nn.CrossEntropyLoss()\n",
    "ce_loss = torch.nn.CrossEntropyLoss(torch.tensor(loss_weights).to(device))\n",
    "my_opt = torch.optim.Adam(eff_model.parameters())\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "for epoch_index in range(num_epochs):\n",
    "    train_loss_sum = 0\n",
    "    train_loss_weight = 0\n",
    "    for batch_index in range(0,mit_train_x_ims.shape[0],batch_size):\n",
    "        my_opt.zero_grad()\n",
    "        input_batch = torch.tensor(mit_train_x_ims[batch_index:batch_index+batch_size][:,np.newaxis,:,:].astype(np.float32)).to(device)\n",
    "        pred = eff_model(input_batch)\n",
    "        target = torch.tensor(mit_train_y[batch_index:batch_index+batch_size].astype(np.int64)).to(device)\n",
    "        batch_loss = ce_loss(pred,target)\n",
    "        batch_loss.backward()\n",
    "        my_opt.step()\n",
    "        if batch_index//batch_size % 10 == 0:\n",
    "            print(\"\\tbatch {} loss {}\".format(batch_index//batch_size,batch_loss))\n",
    "        train_loss_sum += batch_loss.detach().cpu().numpy() * mit_train_x_ims[batch_index:batch_index+batch_size].shape[0]\n",
    "        train_loss_weight += mit_train_x_ims[batch_index:batch_index+batch_size].shape[0]\n",
    "    print(\"epoch {} average_loss {}\".format(epoch_index, train_loss_sum/train_loss_weight))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b095b258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 0 loss 0.0035141671542078257\n",
      "\tbatch 10 loss 0.01854134537279606\n",
      "\tbatch 20 loss 0.00045384978875517845\n",
      "\tbatch 30 loss 0.002022574422881007\n",
      "\tbatch 40 loss 0.0013687842292711139\n",
      "\tbatch 50 loss 0.0015725827543064952\n",
      "\tbatch 60 loss 0.008659813553094864\n",
      "\tbatch 70 loss 0.0033818413503468037\n",
      "\tbatch 80 loss 0.010327216237783432\n",
      "\tbatch 90 loss 0.0036964919418096542\n",
      "\tbatch 100 loss 0.0011205151677131653\n",
      "\tbatch 110 loss 0.033017292618751526\n",
      "\tbatch 120 loss 0.011109746061265469\n",
      "\tbatch 130 loss 0.0004525554249994457\n",
      "\tbatch 140 loss 0.004301168955862522\n",
      "\tbatch 150 loss 0.006896864157170057\n",
      "\tbatch 160 loss 0.07514199614524841\n",
      "\tbatch 170 loss 0.04325104132294655\n",
      "\tbatch 180 loss 0.013537371531128883\n",
      "\tbatch 190 loss 0.0008877898217178881\n",
      "\tbatch 200 loss 0.031601689755916595\n",
      "\tbatch 210 loss 0.002508217468857765\n",
      "\tbatch 220 loss 0.023071937263011932\n",
      "\tbatch 230 loss 0.07115601748228073\n",
      "\tbatch 240 loss 0.00021397312229964882\n",
      "\tbatch 250 loss 0.0017995324451476336\n",
      "\tbatch 260 loss 0.001319920178502798\n",
      "\tbatch 270 loss 0.0001935226027853787\n",
      "\tbatch 280 loss 0.037002116441726685\n",
      "\tbatch 290 loss 0.011390700936317444\n",
      "\tbatch 300 loss 0.004199100658297539\n",
      "\tbatch 310 loss 0.004100603051483631\n",
      "\tbatch 320 loss 0.002876443089917302\n",
      "\tbatch 330 loss 0.0006677318597212434\n",
      "\tbatch 340 loss 0.04630324989557266\n",
      "\tbatch 350 loss 0.003722842549905181\n",
      "\tbatch 360 loss 0.00285119260661304\n",
      "\tbatch 370 loss 0.003245343454182148\n",
      "\tbatch 380 loss 0.008466544561088085\n",
      "\tbatch 390 loss 0.046177543699741364\n",
      "\tbatch 400 loss 7.674490188946947e-05\n",
      "\tbatch 410 loss 0.003497988684102893\n",
      "\tbatch 420 loss 0.0028840559534728527\n",
      "\tbatch 430 loss 0.0006050384836271405\n",
      "\tbatch 440 loss 0.011416364461183548\n",
      "\tbatch 450 loss 0.0011797602055594325\n",
      "\tbatch 460 loss 0.06601652503013611\n",
      "\tbatch 470 loss 0.003035439644008875\n",
      "\tbatch 480 loss 0.0030532791279256344\n",
      "\tbatch 490 loss 0.0010073977755382657\n",
      "\tbatch 500 loss 0.019525818526744843\n",
      "\tbatch 510 loss 0.07716597616672516\n",
      "\tbatch 520 loss 0.038257285952568054\n",
      "\tbatch 530 loss 0.04576124995946884\n",
      "\tbatch 540 loss 0.004776469897478819\n",
      "\tbatch 550 loss 0.0012715867487713695\n",
      "\tbatch 560 loss 0.0037262910045683384\n",
      "\tbatch 570 loss 0.006753575522452593\n",
      "\tbatch 580 loss 0.006954145152121782\n",
      "\tbatch 590 loss 0.003119219094514847\n",
      "\tbatch 600 loss 0.00021433424262795597\n",
      "\tbatch 610 loss 0.02924462780356407\n",
      "\tbatch 620 loss 0.02712278999388218\n",
      "\tbatch 630 loss 0.013276422396302223\n",
      "\tbatch 640 loss 0.0030304482206702232\n",
      "\tbatch 650 loss 0.013927504420280457\n",
      "\tbatch 660 loss 0.010620223358273506\n",
      "\tbatch 670 loss 0.0489068403840065\n",
      "\tbatch 680 loss 0.005732823628932238\n",
      "\tbatch 690 loss 0.006099114194512367\n",
      "\tbatch 700 loss 0.0007008945103734732\n",
      "\tbatch 710 loss 0.03181029483675957\n",
      "\tbatch 720 loss 0.00011564051237655804\n",
      "\tbatch 730 loss 0.005445170681923628\n",
      "\tbatch 740 loss 0.01812068372964859\n",
      "\tbatch 750 loss 0.01200170535594225\n",
      "\tbatch 760 loss 0.18750284612178802\n",
      "\tbatch 770 loss 0.023155488073825836\n",
      "\tbatch 780 loss 0.006608834955841303\n",
      "\tbatch 790 loss 0.0017209065845236182\n",
      "\tbatch 800 loss 0.03179776668548584\n",
      "\tbatch 810 loss 0.0010825918288901448\n",
      "\tbatch 820 loss 0.07129103690385818\n",
      "\tbatch 830 loss 0.0027160141617059708\n",
      "\tbatch 840 loss 0.0024646788369864225\n",
      "\tbatch 850 loss 0.050144825130701065\n",
      "\tbatch 860 loss 0.0013517832849174738\n",
      "\tbatch 870 loss 0.009594024159014225\n",
      "\tbatch 880 loss 0.00670222844928503\n",
      "\tbatch 890 loss 0.04961998015642166\n",
      "\tbatch 900 loss 0.01217680238187313\n",
      "\tbatch 910 loss 0.010145440697669983\n",
      "\tbatch 920 loss 0.03943571820855141\n",
      "\tbatch 930 loss 0.0010903971269726753\n",
      "\tbatch 940 loss 0.00332589796744287\n",
      "\tbatch 950 loss 0.0003467983042355627\n",
      "\tbatch 960 loss 0.005794147495180368\n",
      "\tbatch 970 loss 0.045438364148139954\n",
      "\tbatch 980 loss 0.00045031754416413605\n",
      "\tbatch 990 loss 0.07914677262306213\n",
      "\tbatch 1000 loss 0.016140254214406013\n",
      "\tbatch 1010 loss 0.0027870365884155035\n",
      "\tbatch 1020 loss 0.010860050097107887\n",
      "\tbatch 1030 loss 0.009540624916553497\n",
      "\tbatch 1040 loss 0.001466673449613154\n",
      "\tbatch 1050 loss 0.026258915662765503\n",
      "\tbatch 1060 loss 0.0019049133406952024\n",
      "\tbatch 1070 loss 0.0013009370304644108\n",
      "\tbatch 1080 loss 0.00835268385708332\n",
      "\tbatch 1090 loss 0.00076328992145136\n",
      "\tbatch 1100 loss 0.041255880147218704\n",
      "\tbatch 1110 loss 0.0013055584859102964\n",
      "\tbatch 1120 loss 0.0034207783173769712\n",
      "\tbatch 1130 loss 0.03964041918516159\n",
      "\tbatch 1140 loss 0.14105713367462158\n",
      "\tbatch 1150 loss 0.0077958544716238976\n",
      "\tbatch 1160 loss 0.0005613714456558228\n",
      "\tbatch 1170 loss 0.04088610038161278\n",
      "\tbatch 1180 loss 0.11960981041193008\n",
      "\tbatch 1190 loss 0.0033460615668445826\n",
      "\tbatch 1200 loss 0.03315708041191101\n",
      "\tbatch 1210 loss 0.007888924330472946\n",
      "\tbatch 1220 loss 0.023890390992164612\n",
      "\tbatch 1230 loss 0.0047930143773555756\n",
      "\tbatch 1240 loss 0.022504139691591263\n",
      "epoch 0 average_loss 0.020987654472773284\n",
      "\tbatch 0 loss 0.1514398604631424\n",
      "\tbatch 10 loss 0.0017991604981943965\n",
      "\tbatch 20 loss 0.0012604664079844952\n",
      "\tbatch 30 loss 0.004959150217473507\n",
      "\tbatch 40 loss 0.017206571996212006\n",
      "\tbatch 50 loss 0.0013240101980045438\n",
      "\tbatch 60 loss 0.004580821376293898\n",
      "\tbatch 70 loss 0.0036168687511235476\n",
      "\tbatch 80 loss 0.03960016369819641\n",
      "\tbatch 90 loss 0.0037227459251880646\n",
      "\tbatch 100 loss 0.0036884467117488384\n",
      "\tbatch 110 loss 0.037857018411159515\n",
      "\tbatch 120 loss 0.08957619965076447\n",
      "\tbatch 130 loss 0.00367079209536314\n",
      "\tbatch 140 loss 0.0077355168759822845\n",
      "\tbatch 150 loss 0.0012255857000127435\n",
      "\tbatch 160 loss 0.04976942762732506\n",
      "\tbatch 170 loss 0.012492619454860687\n",
      "\tbatch 180 loss 0.012113171629607677\n",
      "\tbatch 190 loss 0.0005496855592355132\n",
      "\tbatch 200 loss 0.029768722131848335\n",
      "\tbatch 210 loss 0.0004516132757999003\n",
      "\tbatch 220 loss 0.003966868855059147\n",
      "\tbatch 230 loss 0.0015960207674652338\n",
      "\tbatch 240 loss 0.003869275562465191\n",
      "\tbatch 250 loss 0.003156515071168542\n",
      "\tbatch 260 loss 0.013471325859427452\n",
      "\tbatch 270 loss 0.00030160226742736995\n",
      "\tbatch 280 loss 0.09670590609312057\n",
      "\tbatch 290 loss 0.016900289803743362\n",
      "\tbatch 300 loss 0.002017686842009425\n",
      "\tbatch 310 loss 0.0735924169421196\n",
      "\tbatch 320 loss 0.0218611191958189\n",
      "\tbatch 330 loss 0.004164594691246748\n",
      "\tbatch 340 loss 0.17220279574394226\n",
      "\tbatch 350 loss 0.006862129084765911\n",
      "\tbatch 360 loss 0.015001166611909866\n",
      "\tbatch 370 loss 0.010253825224936008\n",
      "\tbatch 380 loss 0.027923911809921265\n",
      "\tbatch 390 loss 0.03854414448142052\n",
      "\tbatch 400 loss 0.0005913262721151114\n",
      "\tbatch 410 loss 0.042467813938856125\n",
      "\tbatch 420 loss 0.07508336007595062\n",
      "\tbatch 430 loss 0.020707523450255394\n",
      "\tbatch 440 loss 0.0009236764162778854\n",
      "\tbatch 450 loss 0.0005819366779178381\n",
      "\tbatch 460 loss 0.032849475741386414\n",
      "\tbatch 470 loss 0.021684441715478897\n",
      "\tbatch 480 loss 0.02093261480331421\n",
      "\tbatch 490 loss 0.0010497333714738488\n",
      "\tbatch 500 loss 0.014560588635504246\n",
      "\tbatch 510 loss 0.04367499053478241\n",
      "\tbatch 520 loss 0.02115265652537346\n",
      "\tbatch 530 loss 0.019364017993211746\n",
      "\tbatch 540 loss 0.005608884152024984\n",
      "\tbatch 550 loss 0.005143899470567703\n",
      "\tbatch 560 loss 0.04075908660888672\n",
      "\tbatch 570 loss 0.0023222786840051413\n",
      "\tbatch 580 loss 0.018362581729888916\n",
      "\tbatch 590 loss 0.012501022778451443\n",
      "\tbatch 600 loss 0.004926063120365143\n",
      "\tbatch 610 loss 0.10401195287704468\n",
      "\tbatch 620 loss 0.03523262217640877\n",
      "\tbatch 630 loss 0.029120083898305893\n",
      "\tbatch 640 loss 0.0042304485104978085\n",
      "\tbatch 650 loss 0.027881233021616936\n",
      "\tbatch 660 loss 0.00107488757930696\n",
      "\tbatch 670 loss 0.027370046824216843\n",
      "\tbatch 680 loss 0.004795646760612726\n",
      "\tbatch 690 loss 0.010973078198730946\n",
      "\tbatch 700 loss 0.0029109944589436054\n",
      "\tbatch 710 loss 0.005410906858742237\n",
      "\tbatch 720 loss 0.0004242698778398335\n",
      "\tbatch 730 loss 0.001684490591287613\n",
      "\tbatch 740 loss 0.0342494435608387\n",
      "\tbatch 750 loss 0.0003492618852760643\n",
      "\tbatch 760 loss 0.011984909884631634\n",
      "\tbatch 770 loss 0.00040793480002321303\n",
      "\tbatch 780 loss 0.031877920031547546\n",
      "\tbatch 790 loss 0.0066500999964773655\n",
      "\tbatch 800 loss 0.02649926207959652\n",
      "\tbatch 810 loss 0.0010062308283522725\n",
      "\tbatch 820 loss 0.0409349724650383\n",
      "\tbatch 830 loss 0.0011135237291455269\n",
      "\tbatch 840 loss 0.0031276638619601727\n",
      "\tbatch 850 loss 0.03511229157447815\n",
      "\tbatch 860 loss 0.0008015785715542734\n",
      "\tbatch 870 loss 0.002675495808944106\n",
      "\tbatch 880 loss 0.10989578813314438\n",
      "\tbatch 890 loss 0.009026361629366875\n",
      "\tbatch 900 loss 0.03386833891272545\n",
      "\tbatch 910 loss 0.0195491723716259\n",
      "\tbatch 920 loss 0.00068998261122033\n",
      "\tbatch 930 loss 0.0008517496753484011\n",
      "\tbatch 940 loss 0.0004282228765077889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 950 loss 0.0064349304884672165\n",
      "\tbatch 960 loss 0.0032135802321135998\n",
      "\tbatch 970 loss 0.016183990985155106\n",
      "\tbatch 980 loss 0.002677003387361765\n",
      "\tbatch 990 loss 0.009330140426754951\n",
      "\tbatch 1000 loss 0.00017117650713771582\n",
      "\tbatch 1010 loss 0.0008926375885494053\n",
      "\tbatch 1020 loss 0.0004773670225404203\n",
      "\tbatch 1030 loss 0.007473781239241362\n",
      "\tbatch 1040 loss 0.003880465170368552\n",
      "\tbatch 1050 loss 0.006108866073191166\n",
      "\tbatch 1060 loss 0.019980864599347115\n",
      "\tbatch 1070 loss 0.000797164801042527\n",
      "\tbatch 1080 loss 0.02466067112982273\n",
      "\tbatch 1090 loss 0.0016776267439126968\n",
      "\tbatch 1100 loss 0.014327759854495525\n",
      "\tbatch 1110 loss 4.82356917927973e-05\n",
      "\tbatch 1120 loss 0.021505219861865044\n",
      "\tbatch 1130 loss 0.0060120197013020515\n",
      "\tbatch 1140 loss 0.07269079238176346\n",
      "\tbatch 1150 loss 0.012068012729287148\n",
      "\tbatch 1160 loss 0.005085302051156759\n",
      "\tbatch 1170 loss 0.04823828116059303\n",
      "\tbatch 1180 loss 0.056873027235269547\n",
      "\tbatch 1190 loss 0.21684500575065613\n",
      "\tbatch 1200 loss 0.030679766088724136\n",
      "\tbatch 1210 loss 0.02079669199883938\n",
      "\tbatch 1220 loss 0.015122733078897\n",
      "\tbatch 1230 loss 0.0035091545432806015\n",
      "\tbatch 1240 loss 0.004516346845775843\n",
      "epoch 1 average_loss 0.02109710658574113\n",
      "\tbatch 0 loss 0.0029525170102715492\n",
      "\tbatch 10 loss 0.04598723724484444\n",
      "\tbatch 20 loss 0.0036411620676517487\n",
      "\tbatch 30 loss 0.019210210070014\n",
      "\tbatch 40 loss 0.018547607585787773\n",
      "\tbatch 50 loss 0.0005844766856171191\n",
      "\tbatch 60 loss 0.003305918537080288\n",
      "\tbatch 70 loss 0.00026162699214182794\n",
      "\tbatch 80 loss 0.03220868110656738\n",
      "\tbatch 90 loss 0.0007142752874642611\n",
      "\tbatch 100 loss 0.0018472577212378383\n",
      "\tbatch 110 loss 0.011440047062933445\n",
      "\tbatch 120 loss 0.004278567619621754\n",
      "\tbatch 130 loss 0.004549731966108084\n",
      "\tbatch 140 loss 0.009761102497577667\n",
      "\tbatch 150 loss 0.10090029984712601\n",
      "\tbatch 160 loss 0.003815633710473776\n",
      "\tbatch 170 loss 0.010964518412947655\n",
      "\tbatch 180 loss 0.05571110546588898\n",
      "\tbatch 190 loss 0.0016045545926317573\n",
      "\tbatch 200 loss 0.014625747688114643\n",
      "\tbatch 210 loss 0.004129823762923479\n",
      "\tbatch 220 loss 0.00014332009595818818\n",
      "\tbatch 230 loss 0.03424651548266411\n",
      "\tbatch 240 loss 0.0005302369245328009\n",
      "\tbatch 250 loss 0.001409633900038898\n",
      "\tbatch 260 loss 0.038764480501413345\n",
      "\tbatch 270 loss 0.0011753810103982687\n",
      "\tbatch 280 loss 0.003406867617741227\n",
      "\tbatch 290 loss 0.0009674632456153631\n",
      "\tbatch 300 loss 0.004365076310932636\n",
      "\tbatch 310 loss 0.0027146257925778627\n",
      "\tbatch 320 loss 0.005045410245656967\n",
      "\tbatch 330 loss 1.5487657947232947e-05\n",
      "\tbatch 340 loss 0.0003196138422936201\n",
      "\tbatch 350 loss 0.03165207803249359\n",
      "\tbatch 360 loss 0.001750458963215351\n",
      "\tbatch 370 loss 0.012521377764642239\n",
      "\tbatch 380 loss 0.00030811398755759\n",
      "\tbatch 390 loss 0.03713156655430794\n",
      "\tbatch 400 loss 5.6933535233838484e-05\n",
      "\tbatch 410 loss 0.001538067706860602\n",
      "\tbatch 420 loss 0.0026516043581068516\n",
      "\tbatch 430 loss 0.00140290311537683\n",
      "\tbatch 440 loss 8.511312626069412e-05\n",
      "\tbatch 450 loss 0.0008478195522911847\n",
      "\tbatch 460 loss 0.00018736449419520795\n",
      "\tbatch 470 loss 0.0003388531331438571\n",
      "\tbatch 480 loss 0.007243102882057428\n",
      "\tbatch 490 loss 0.0040737041272223\n",
      "\tbatch 500 loss 0.011209428310394287\n",
      "\tbatch 510 loss 0.002326789777725935\n",
      "\tbatch 520 loss 0.015497603453695774\n",
      "\tbatch 530 loss 0.039423760026693344\n",
      "\tbatch 540 loss 0.0009797915117815137\n",
      "\tbatch 550 loss 0.0020092192571610212\n",
      "\tbatch 560 loss 0.0034128306433558464\n",
      "\tbatch 570 loss 0.00039345331606455147\n",
      "\tbatch 580 loss 0.0003550937690306455\n",
      "\tbatch 590 loss 0.0008475486538372934\n",
      "\tbatch 600 loss 0.001401074230670929\n",
      "\tbatch 610 loss 0.0047475663013756275\n",
      "\tbatch 620 loss 0.005643744952976704\n",
      "\tbatch 630 loss 0.022867165505886078\n",
      "\tbatch 640 loss 0.0671258494257927\n",
      "\tbatch 650 loss 0.006894636433571577\n",
      "\tbatch 660 loss 0.013935416005551815\n",
      "\tbatch 670 loss 0.03268856555223465\n",
      "\tbatch 680 loss 0.01102867815643549\n",
      "\tbatch 690 loss 0.022996755316853523\n",
      "\tbatch 700 loss 0.0010454413713887334\n",
      "\tbatch 710 loss 0.0011381786316633224\n",
      "\tbatch 720 loss 0.0008477503433823586\n",
      "\tbatch 730 loss 0.01824789308011532\n",
      "\tbatch 740 loss 0.00021026084141340107\n",
      "\tbatch 750 loss 0.003891415661200881\n",
      "\tbatch 760 loss 0.02520136721432209\n",
      "\tbatch 770 loss 0.0019052099669352174\n",
      "\tbatch 780 loss 0.029962239786982536\n",
      "\tbatch 790 loss 0.0011743733193725348\n",
      "\tbatch 800 loss 0.047129563987255096\n",
      "\tbatch 810 loss 0.0010153405601158738\n",
      "\tbatch 820 loss 0.013093270361423492\n",
      "\tbatch 830 loss 0.0003545322106219828\n",
      "\tbatch 840 loss 0.0012749646557494998\n",
      "\tbatch 850 loss 0.016813520342111588\n",
      "\tbatch 860 loss 0.0052970172837376595\n",
      "\tbatch 870 loss 0.005174858961254358\n",
      "\tbatch 880 loss 0.00037134141894057393\n",
      "\tbatch 890 loss 0.09912082552909851\n",
      "\tbatch 900 loss 0.0023466770071536303\n",
      "\tbatch 910 loss 0.0022600875236094\n",
      "\tbatch 920 loss 0.01826309971511364\n",
      "\tbatch 930 loss 0.00014946800365578383\n",
      "\tbatch 940 loss 0.004256803542375565\n",
      "\tbatch 950 loss 0.03990764915943146\n",
      "\tbatch 960 loss 0.0010778650175780058\n",
      "\tbatch 970 loss 0.05925719067454338\n",
      "\tbatch 980 loss 0.012990946881473064\n",
      "\tbatch 990 loss 0.005333757493644953\n",
      "\tbatch 1000 loss 0.0006712818867526948\n",
      "\tbatch 1010 loss 0.002296749735251069\n",
      "\tbatch 1020 loss 0.0008251040708273649\n",
      "\tbatch 1030 loss 0.004246262367814779\n",
      "\tbatch 1040 loss 0.0010211952030658722\n",
      "\tbatch 1050 loss 0.00043941635522060096\n",
      "\tbatch 1060 loss 0.00022058912145439535\n",
      "\tbatch 1070 loss 0.008338402025401592\n",
      "\tbatch 1080 loss 0.0006439631688408554\n",
      "\tbatch 1090 loss 0.00013182501425035298\n",
      "\tbatch 1100 loss 0.07438191771507263\n",
      "\tbatch 1110 loss 2.549893451941898e-06\n",
      "\tbatch 1120 loss 0.0001182995765702799\n",
      "\tbatch 1130 loss 0.15100912749767303\n",
      "\tbatch 1140 loss 0.02965143322944641\n",
      "\tbatch 1150 loss 0.002472815103828907\n",
      "\tbatch 1160 loss 0.022069541737437248\n",
      "\tbatch 1170 loss 0.04226517677307129\n",
      "\tbatch 1180 loss 0.000490571022965014\n",
      "\tbatch 1190 loss 0.0006665904074907303\n",
      "\tbatch 1200 loss 0.008430449292063713\n",
      "\tbatch 1210 loss 0.0015436758985742927\n",
      "\tbatch 1220 loss 0.008192918263375759\n",
      "\tbatch 1230 loss 0.1649450957775116\n",
      "\tbatch 1240 loss 0.012762357480823994\n",
      "epoch 2 average_loss 0.016099871297792924\n",
      "\tbatch 0 loss 0.0073661645874381065\n",
      "\tbatch 10 loss 0.047538790851831436\n",
      "\tbatch 20 loss 0.000595752673689276\n",
      "\tbatch 30 loss 0.012420162558555603\n",
      "\tbatch 40 loss 0.0020708958618342876\n",
      "\tbatch 50 loss 0.0007629707688465714\n",
      "\tbatch 60 loss 0.0006632671575061977\n",
      "\tbatch 70 loss 0.005410013720393181\n",
      "\tbatch 80 loss 0.03196083754301071\n",
      "\tbatch 90 loss 0.0008802260854281485\n",
      "\tbatch 100 loss 0.0008030087919905782\n",
      "\tbatch 110 loss 0.0029756310395896435\n",
      "\tbatch 120 loss 0.01630881056189537\n",
      "\tbatch 130 loss 0.00037017211434431374\n",
      "\tbatch 140 loss 0.02268928289413452\n",
      "\tbatch 150 loss 0.0030475170351564884\n",
      "\tbatch 160 loss 0.05253511294722557\n",
      "\tbatch 170 loss 0.02339688315987587\n",
      "\tbatch 180 loss 0.044033803045749664\n",
      "\tbatch 190 loss 0.008194955065846443\n",
      "\tbatch 200 loss 0.017065608873963356\n",
      "\tbatch 210 loss 0.002537228399887681\n",
      "\tbatch 220 loss 0.0040549966506659985\n",
      "\tbatch 230 loss 0.07328203320503235\n",
      "\tbatch 240 loss 0.0006597949541173875\n",
      "\tbatch 250 loss 2.510805643396452e-05\n",
      "\tbatch 260 loss 0.002025008900091052\n",
      "\tbatch 270 loss 0.0003452587698120624\n",
      "\tbatch 280 loss 0.014008712023496628\n",
      "\tbatch 290 loss 0.002598879160359502\n",
      "\tbatch 300 loss 0.055809322744607925\n",
      "\tbatch 310 loss 0.00174056610558182\n",
      "\tbatch 320 loss 0.0019729407504200935\n",
      "\tbatch 330 loss 0.0016634666826575994\n",
      "\tbatch 340 loss 0.10305964946746826\n",
      "\tbatch 350 loss 0.007638884242624044\n",
      "\tbatch 360 loss 0.006789927836507559\n",
      "\tbatch 370 loss 0.0057979063130915165\n",
      "\tbatch 380 loss 0.0005883455160073936\n",
      "\tbatch 390 loss 0.04363957792520523\n",
      "\tbatch 400 loss 0.006205377634614706\n",
      "\tbatch 410 loss 0.02386895939707756\n",
      "\tbatch 420 loss 0.005189801100641489\n",
      "\tbatch 430 loss 0.03457830101251602\n",
      "\tbatch 440 loss 0.002571313176304102\n",
      "\tbatch 450 loss 0.0036166829522699118\n",
      "\tbatch 460 loss 0.00047719699796289206\n",
      "\tbatch 470 loss 0.006027391646057367\n",
      "\tbatch 480 loss 0.010015842504799366\n",
      "\tbatch 490 loss 0.00026286914362572134\n",
      "\tbatch 500 loss 0.004356546327471733\n",
      "\tbatch 510 loss 0.010294974781572819\n",
      "\tbatch 520 loss 0.07218232750892639\n",
      "\tbatch 530 loss 0.0006247436976991594\n",
      "\tbatch 540 loss 0.0010397684527561069\n",
      "\tbatch 550 loss 0.006918290164321661\n",
      "\tbatch 560 loss 0.0020138751715421677\n",
      "\tbatch 570 loss 0.0022368761710822582\n",
      "\tbatch 580 loss 0.03874265402555466\n",
      "\tbatch 590 loss 0.001089006313122809\n",
      "\tbatch 600 loss 0.0009299990488216281\n",
      "\tbatch 610 loss 0.009789119474589825\n",
      "\tbatch 620 loss 0.050225548446178436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 630 loss 0.0357147678732872\n",
      "\tbatch 640 loss 0.006369595415890217\n",
      "\tbatch 650 loss 0.019047999754548073\n",
      "\tbatch 660 loss 0.001944890827871859\n",
      "\tbatch 670 loss 0.11950507014989853\n",
      "\tbatch 680 loss 0.001207337831147015\n",
      "\tbatch 690 loss 0.06602872908115387\n",
      "\tbatch 700 loss 0.008662901818752289\n",
      "\tbatch 710 loss 0.04196261242032051\n",
      "\tbatch 720 loss 0.0006458720890805125\n",
      "\tbatch 730 loss 0.0016963653033599257\n",
      "\tbatch 740 loss 0.00023387221153825521\n",
      "\tbatch 750 loss 0.0007856457377783954\n",
      "\tbatch 760 loss 0.0021331184543669224\n",
      "\tbatch 770 loss 4.4641419663093984e-05\n",
      "\tbatch 780 loss 0.0009531439282000065\n",
      "\tbatch 790 loss 0.003846038831397891\n",
      "\tbatch 800 loss 0.011279890313744545\n",
      "\tbatch 810 loss 0.0020242880564182997\n",
      "\tbatch 820 loss 0.018092555925250053\n",
      "\tbatch 830 loss 0.00528448261320591\n",
      "\tbatch 840 loss 0.00036100868601351976\n",
      "\tbatch 850 loss 0.12841761112213135\n",
      "\tbatch 860 loss 0.001978750806301832\n",
      "\tbatch 870 loss 0.0636153295636177\n",
      "\tbatch 880 loss 0.025184473022818565\n",
      "\tbatch 890 loss 0.0010728188790380955\n",
      "\tbatch 900 loss 0.07437661290168762\n",
      "\tbatch 910 loss 0.0036894772201776505\n",
      "\tbatch 920 loss 0.0012584527721628547\n",
      "\tbatch 930 loss 0.0019364663166925311\n",
      "\tbatch 940 loss 0.007161491550505161\n",
      "\tbatch 950 loss 0.0009101541945710778\n",
      "\tbatch 960 loss 0.020761752501130104\n",
      "\tbatch 970 loss 0.030687130987644196\n",
      "\tbatch 980 loss 0.0010756280971691012\n",
      "\tbatch 990 loss 0.010485787875950336\n",
      "\tbatch 1000 loss 0.0002530159254092723\n",
      "\tbatch 1010 loss 0.0022048132959753275\n",
      "\tbatch 1020 loss 0.00010346212366130203\n",
      "\tbatch 1030 loss 0.0029038540087640285\n",
      "\tbatch 1040 loss 0.004619825165718794\n",
      "\tbatch 1050 loss 0.04193560779094696\n",
      "\tbatch 1060 loss 0.007382996845990419\n",
      "\tbatch 1070 loss 0.00781717337667942\n",
      "\tbatch 1080 loss 0.047046203166246414\n",
      "\tbatch 1090 loss 0.018099935725331306\n",
      "\tbatch 1100 loss 0.006557495798915625\n",
      "\tbatch 1110 loss 6.365656008711085e-05\n",
      "\tbatch 1120 loss 0.014431868679821491\n",
      "\tbatch 1130 loss 0.008845124393701553\n",
      "\tbatch 1140 loss 0.016826780512928963\n",
      "\tbatch 1150 loss 5.378944115363993e-05\n",
      "\tbatch 1160 loss 0.029841339215636253\n",
      "\tbatch 1170 loss 0.015248422510921955\n",
      "\tbatch 1180 loss 0.0010866690427064896\n",
      "\tbatch 1190 loss 0.0005440667155198753\n",
      "\tbatch 1200 loss 0.1623024046421051\n",
      "\tbatch 1210 loss 0.009310330264270306\n",
      "\tbatch 1220 loss 0.013356958515942097\n",
      "\tbatch 1230 loss 0.007751838769763708\n",
      "\tbatch 1240 loss 0.018923070281744003\n",
      "epoch 3 average_loss 0.01819645231502218\n",
      "\tbatch 0 loss 0.0008673485135659575\n",
      "\tbatch 10 loss 0.015239600092172623\n",
      "\tbatch 20 loss 0.0010286616161465645\n",
      "\tbatch 30 loss 0.0008559192065149546\n",
      "\tbatch 40 loss 0.00129981548525393\n",
      "\tbatch 50 loss 0.0012505104532465339\n",
      "\tbatch 60 loss 0.0005364136304706335\n",
      "\tbatch 70 loss 0.00016904674703255296\n",
      "\tbatch 80 loss 0.022207701578736305\n",
      "\tbatch 90 loss 0.008547065779566765\n",
      "\tbatch 100 loss 0.0011322750942781568\n",
      "\tbatch 110 loss 0.10529875755310059\n",
      "\tbatch 120 loss 0.05331787094473839\n",
      "\tbatch 130 loss 0.00016671544290147722\n",
      "\tbatch 140 loss 0.0020048737060278654\n",
      "\tbatch 150 loss 0.025263682007789612\n",
      "\tbatch 160 loss 0.06320680677890778\n",
      "\tbatch 170 loss 0.020334385335445404\n",
      "\tbatch 180 loss 0.0043922727927565575\n",
      "\tbatch 190 loss 0.00018869769701268524\n",
      "\tbatch 200 loss 0.015745066106319427\n",
      "\tbatch 210 loss 0.053117766976356506\n",
      "\tbatch 220 loss 0.0014495966024696827\n",
      "\tbatch 230 loss 0.002679924014955759\n",
      "\tbatch 240 loss 0.0002787882986012846\n",
      "\tbatch 250 loss 0.0002135664108209312\n",
      "\tbatch 260 loss 0.010193459689617157\n",
      "\tbatch 270 loss 0.0003130969125777483\n",
      "\tbatch 280 loss 0.010850469581782818\n",
      "\tbatch 290 loss 0.004245145246386528\n",
      "\tbatch 300 loss 0.0009467154741287231\n",
      "\tbatch 310 loss 0.011773565784096718\n",
      "\tbatch 320 loss 0.0021367815788835287\n",
      "\tbatch 330 loss 4.650118717108853e-05\n",
      "\tbatch 340 loss 0.040294405072927475\n",
      "\tbatch 350 loss 0.0021147369407117367\n",
      "\tbatch 360 loss 0.0051536462269723415\n",
      "\tbatch 370 loss 0.006441127974539995\n",
      "\tbatch 380 loss 0.0015496679116040468\n",
      "\tbatch 390 loss 0.040773529559373856\n",
      "\tbatch 400 loss 4.5889119064668193e-05\n",
      "\tbatch 410 loss 0.00024732810561545193\n",
      "\tbatch 420 loss 0.09238849580287933\n",
      "\tbatch 430 loss 0.0003213080926798284\n",
      "\tbatch 440 loss 0.0004746382182929665\n",
      "\tbatch 450 loss 0.007933699525892735\n",
      "\tbatch 460 loss 0.002226180164143443\n",
      "\tbatch 470 loss 0.0029965138528496027\n",
      "\tbatch 480 loss 0.0011374744353815913\n",
      "\tbatch 490 loss 0.0007748822681605816\n",
      "\tbatch 500 loss 0.017504828050732613\n",
      "\tbatch 510 loss 0.013367968611419201\n",
      "\tbatch 520 loss 0.002261426765471697\n",
      "\tbatch 530 loss 0.0017407066188752651\n",
      "\tbatch 540 loss 0.0007453619036823511\n",
      "\tbatch 550 loss 0.0012215101160109043\n",
      "\tbatch 560 loss 0.0160499420017004\n",
      "\tbatch 570 loss 0.0011000881204381585\n",
      "\tbatch 580 loss 0.0062147569842636585\n",
      "\tbatch 590 loss 0.005861716810613871\n",
      "\tbatch 600 loss 0.003366375109180808\n",
      "\tbatch 610 loss 0.004803069867193699\n",
      "\tbatch 620 loss 0.10844312608242035\n",
      "\tbatch 630 loss 0.007281152531504631\n",
      "\tbatch 640 loss 0.09179873019456863\n",
      "\tbatch 650 loss 0.0015984022757038474\n",
      "\tbatch 660 loss 0.0007629106403328478\n",
      "\tbatch 670 loss 0.03759254887700081\n",
      "\tbatch 680 loss 0.0013322696322575212\n",
      "\tbatch 690 loss 0.003859287593513727\n",
      "\tbatch 700 loss 0.016191594302654266\n",
      "\tbatch 710 loss 0.000826022180262953\n",
      "\tbatch 720 loss 0.00020117819076403975\n",
      "\tbatch 730 loss 0.001869340194389224\n",
      "\tbatch 740 loss 0.0006103843334130943\n",
      "\tbatch 750 loss 0.00833976361900568\n",
      "\tbatch 760 loss 0.020309682935476303\n",
      "\tbatch 770 loss 0.008107463829219341\n",
      "\tbatch 780 loss 0.0012057689018547535\n",
      "\tbatch 790 loss 0.0033854630310088396\n",
      "\tbatch 800 loss 0.046544186770915985\n",
      "\tbatch 810 loss 0.007971817627549171\n",
      "\tbatch 820 loss 0.07266760617494583\n",
      "\tbatch 830 loss 0.002095871139317751\n",
      "\tbatch 840 loss 0.00023598253028467298\n",
      "\tbatch 850 loss 0.027318721637129784\n",
      "\tbatch 860 loss 9.157106978818774e-05\n",
      "\tbatch 870 loss 0.0009674152242951095\n",
      "\tbatch 880 loss 0.007809692528098822\n",
      "\tbatch 890 loss 0.02942744642496109\n",
      "\tbatch 900 loss 0.009121867828071117\n",
      "\tbatch 910 loss 0.0017274817219004035\n",
      "\tbatch 920 loss 0.0016535429749637842\n",
      "\tbatch 930 loss 0.00048411358147859573\n",
      "\tbatch 940 loss 0.0018431621138006449\n",
      "\tbatch 950 loss 0.0002564630121923983\n",
      "\tbatch 960 loss 5.379628419177607e-05\n",
      "\tbatch 970 loss 0.009895293973386288\n",
      "\tbatch 980 loss 0.0039091832004487514\n",
      "\tbatch 990 loss 0.008664527907967567\n",
      "\tbatch 1000 loss 2.3700900783296674e-05\n",
      "\tbatch 1010 loss 0.00044216879177838564\n",
      "\tbatch 1020 loss 0.000438682793173939\n",
      "\tbatch 1030 loss 0.0010552402818575501\n",
      "\tbatch 1040 loss 0.0021072716917842627\n",
      "\tbatch 1050 loss 0.00046508043305948377\n",
      "\tbatch 1060 loss 0.00017316167941316962\n",
      "\tbatch 1070 loss 0.0020948401652276516\n",
      "\tbatch 1080 loss 0.02649679221212864\n",
      "\tbatch 1090 loss 0.0010778815485537052\n",
      "\tbatch 1100 loss 0.00852386374026537\n",
      "\tbatch 1110 loss 0.00016228049935307354\n",
      "\tbatch 1120 loss 0.0067719826474785805\n",
      "\tbatch 1130 loss 0.008574492298066616\n",
      "\tbatch 1140 loss 0.009532593190670013\n",
      "\tbatch 1150 loss 0.00042833664338104427\n",
      "\tbatch 1160 loss 0.001073490479029715\n",
      "\tbatch 1170 loss 0.00799762737005949\n",
      "\tbatch 1180 loss 0.0003976568696089089\n",
      "\tbatch 1190 loss 0.0018781963735818863\n",
      "\tbatch 1200 loss 0.03467489033937454\n",
      "\tbatch 1210 loss 0.012783825397491455\n",
      "\tbatch 1220 loss 0.04456467926502228\n",
      "\tbatch 1230 loss 0.0006809849292039871\n",
      "\tbatch 1240 loss 0.0026168515905737877\n",
      "epoch 4 average_loss 0.014206884400756098\n",
      "\tbatch 0 loss 0.001203443156555295\n",
      "\tbatch 10 loss 0.003983873873949051\n",
      "\tbatch 20 loss 0.00024462860892526805\n",
      "\tbatch 30 loss 0.0031008508522063494\n",
      "\tbatch 40 loss 0.0059089441783726215\n",
      "\tbatch 50 loss 0.0028218275401741266\n",
      "\tbatch 60 loss 0.0014575014356523752\n",
      "\tbatch 70 loss 0.003526478772982955\n",
      "\tbatch 80 loss 0.21305851638317108\n",
      "\tbatch 90 loss 0.0037487586960196495\n",
      "\tbatch 100 loss 0.02083115465939045\n",
      "\tbatch 110 loss 0.018702490255236626\n",
      "\tbatch 120 loss 0.0028762342408299446\n",
      "\tbatch 130 loss 0.002351990668103099\n",
      "\tbatch 140 loss 0.0006518131704069674\n",
      "\tbatch 150 loss 0.011927340179681778\n",
      "\tbatch 160 loss 0.005552534479647875\n",
      "\tbatch 170 loss 0.02405451610684395\n",
      "\tbatch 180 loss 0.004407003987580538\n",
      "\tbatch 190 loss 0.0010272993240505457\n",
      "\tbatch 200 loss 0.023270299658179283\n",
      "\tbatch 210 loss 0.00025289447512477636\n",
      "\tbatch 220 loss 0.034538134932518005\n",
      "\tbatch 230 loss 0.005758774466812611\n",
      "\tbatch 240 loss 0.0021013508085161448\n",
      "\tbatch 250 loss 0.0005087111494503915\n",
      "\tbatch 260 loss 0.03443203493952751\n",
      "\tbatch 270 loss 0.002047780668362975\n",
      "\tbatch 280 loss 0.0033960631117224693\n",
      "\tbatch 290 loss 0.000284099776763469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 300 loss 0.0007516921032220125\n",
      "\tbatch 310 loss 0.014229564927518368\n",
      "\tbatch 320 loss 0.013932289555668831\n",
      "\tbatch 330 loss 0.001379472902044654\n",
      "\tbatch 340 loss 0.01267811469733715\n",
      "\tbatch 350 loss 0.015899308025836945\n",
      "\tbatch 360 loss 0.00448173051699996\n",
      "\tbatch 370 loss 0.06696595251560211\n",
      "\tbatch 380 loss 0.00020281992328818887\n",
      "\tbatch 390 loss 0.012413102202117443\n",
      "\tbatch 400 loss 0.0001958195643965155\n",
      "\tbatch 410 loss 0.007164696231484413\n",
      "\tbatch 420 loss 0.08765026926994324\n",
      "\tbatch 430 loss 0.002924408530816436\n",
      "\tbatch 440 loss 0.001080355141311884\n",
      "\tbatch 450 loss 0.00500802556052804\n",
      "\tbatch 460 loss 0.010601325891911983\n",
      "\tbatch 470 loss 0.0053403046913445\n",
      "\tbatch 480 loss 0.0042649307288229465\n",
      "\tbatch 490 loss 6.965412467252463e-05\n",
      "\tbatch 500 loss 0.006202416494488716\n",
      "\tbatch 510 loss 0.008850239217281342\n",
      "\tbatch 520 loss 0.020309515297412872\n",
      "\tbatch 530 loss 0.016390420496463776\n",
      "\tbatch 540 loss 0.004164920188486576\n",
      "\tbatch 550 loss 0.002498619956895709\n",
      "\tbatch 560 loss 0.004159271251410246\n",
      "\tbatch 570 loss 0.012629009783267975\n",
      "\tbatch 580 loss 0.0004180913674645126\n",
      "\tbatch 590 loss 0.0005559439305216074\n",
      "\tbatch 600 loss 0.000716237467713654\n",
      "\tbatch 610 loss 0.0005196356796659529\n",
      "\tbatch 620 loss 0.016913335770368576\n",
      "\tbatch 630 loss 0.022663692012429237\n",
      "\tbatch 640 loss 0.011684104800224304\n",
      "\tbatch 650 loss 0.018511101603507996\n",
      "\tbatch 660 loss 0.0014332524733617902\n",
      "\tbatch 670 loss 0.00714729493483901\n",
      "\tbatch 680 loss 0.0010272624203935266\n",
      "\tbatch 690 loss 0.007251254748553038\n",
      "\tbatch 700 loss 0.000962829333730042\n",
      "\tbatch 710 loss 0.00238873902708292\n",
      "\tbatch 720 loss 0.0004032283613923937\n",
      "\tbatch 730 loss 0.0003860686847474426\n",
      "\tbatch 740 loss 0.002033994533121586\n",
      "\tbatch 750 loss 0.03385385870933533\n",
      "\tbatch 760 loss 0.03710632398724556\n",
      "\tbatch 770 loss 0.00021735024347435683\n",
      "\tbatch 780 loss 0.004210378043353558\n",
      "\tbatch 790 loss 0.0027406434528529644\n",
      "\tbatch 800 loss 0.021280184388160706\n",
      "\tbatch 810 loss 0.00032342327176593244\n",
      "\tbatch 820 loss 0.0093972347676754\n",
      "\tbatch 830 loss 0.0032040439546108246\n",
      "\tbatch 840 loss 0.0020180041901767254\n",
      "\tbatch 850 loss 0.016360433772206306\n",
      "\tbatch 860 loss 0.00037160422652959824\n",
      "\tbatch 870 loss 0.0028063999488949776\n",
      "\tbatch 880 loss 0.022023966535925865\n",
      "\tbatch 890 loss 0.0004581898683682084\n",
      "\tbatch 900 loss 0.0021385187283158302\n",
      "\tbatch 910 loss 0.0009784420253708959\n",
      "\tbatch 920 loss 0.0019498715410009027\n",
      "\tbatch 930 loss 0.002483887830749154\n",
      "\tbatch 940 loss 0.006475335918366909\n",
      "\tbatch 950 loss 0.0003915024863090366\n",
      "\tbatch 960 loss 0.0024383780546486378\n",
      "\tbatch 970 loss 0.012958988547325134\n",
      "\tbatch 980 loss 0.02183368429541588\n",
      "\tbatch 990 loss 0.017947833985090256\n",
      "\tbatch 1000 loss 2.0803190636797808e-05\n",
      "\tbatch 1010 loss 5.1558650739025325e-05\n",
      "\tbatch 1020 loss 5.802083614980802e-05\n",
      "\tbatch 1030 loss 0.0011841634986922145\n",
      "\tbatch 1040 loss 0.13487014174461365\n",
      "\tbatch 1050 loss 0.09008762240409851\n",
      "\tbatch 1060 loss 0.00036954873939976096\n",
      "\tbatch 1070 loss 0.0019772008527070284\n",
      "\tbatch 1080 loss 0.011700035072863102\n",
      "\tbatch 1090 loss 0.0017193384701386094\n",
      "\tbatch 1100 loss 0.04673042520880699\n",
      "\tbatch 1110 loss 0.00013153476174920797\n",
      "\tbatch 1120 loss 0.0078098829835653305\n",
      "\tbatch 1130 loss 0.0007167537114582956\n",
      "\tbatch 1140 loss 0.029297465458512306\n",
      "\tbatch 1150 loss 0.0003521729668136686\n",
      "\tbatch 1160 loss 0.0016707124887034297\n",
      "\tbatch 1170 loss 0.017396867275238037\n",
      "\tbatch 1180 loss 0.0003666676639113575\n",
      "\tbatch 1190 loss 0.0018815795192494988\n",
      "\tbatch 1200 loss 0.008577434346079826\n",
      "\tbatch 1210 loss 0.02353850193321705\n",
      "\tbatch 1220 loss 0.003982194233685732\n",
      "\tbatch 1230 loss 0.0062411874532699585\n",
      "\tbatch 1240 loss 0.014787320047616959\n",
      "epoch 5 average_loss 0.016503451069533184\n",
      "\tbatch 0 loss 0.001046065823175013\n",
      "\tbatch 10 loss 0.03099055588245392\n",
      "\tbatch 20 loss 0.00444299029186368\n",
      "\tbatch 30 loss 0.007445517927408218\n",
      "\tbatch 40 loss 0.00201961281709373\n",
      "\tbatch 50 loss 0.002484440803527832\n",
      "\tbatch 60 loss 0.00173158326651901\n",
      "\tbatch 70 loss 8.223071927204728e-05\n",
      "\tbatch 80 loss 0.06926082819700241\n",
      "\tbatch 90 loss 0.00019942078506574035\n",
      "\tbatch 100 loss 0.00014685973292216659\n",
      "\tbatch 110 loss 0.001520841964520514\n",
      "\tbatch 120 loss 0.02904699742794037\n",
      "\tbatch 130 loss 0.004230437334626913\n",
      "\tbatch 140 loss 0.04849043861031532\n",
      "\tbatch 150 loss 0.00032102628028951585\n",
      "\tbatch 160 loss 0.0006605000235140324\n",
      "\tbatch 170 loss 0.013429651968181133\n",
      "\tbatch 180 loss 0.019389377906918526\n",
      "\tbatch 190 loss 0.002131464658305049\n",
      "\tbatch 200 loss 0.01321314089000225\n",
      "\tbatch 210 loss 0.00399096729233861\n",
      "\tbatch 220 loss 0.003791209077462554\n",
      "\tbatch 230 loss 0.004958139266818762\n",
      "\tbatch 240 loss 0.00032329323585145175\n",
      "\tbatch 250 loss 0.00019642717961687595\n",
      "\tbatch 260 loss 0.0025698156096041203\n",
      "\tbatch 270 loss 1.5504156181123108e-05\n",
      "\tbatch 280 loss 0.0013717621332034469\n",
      "\tbatch 290 loss 0.05236431211233139\n",
      "\tbatch 300 loss 0.0008207056671380997\n",
      "\tbatch 310 loss 0.0012941049644723535\n",
      "\tbatch 320 loss 0.03679102659225464\n",
      "\tbatch 330 loss 2.092763315886259e-05\n",
      "\tbatch 340 loss 0.023855574429035187\n",
      "\tbatch 350 loss 0.007365275174379349\n",
      "\tbatch 360 loss 0.0003610893909353763\n",
      "\tbatch 370 loss 0.017118720337748528\n",
      "\tbatch 380 loss 0.00014366654795594513\n",
      "\tbatch 390 loss 0.06746628880500793\n",
      "\tbatch 400 loss 1.965624869626481e-05\n",
      "\tbatch 410 loss 0.019523123279213905\n",
      "\tbatch 420 loss 0.0036085203755646944\n",
      "\tbatch 430 loss 0.004818648565560579\n",
      "\tbatch 440 loss 0.007647617254406214\n",
      "\tbatch 450 loss 0.0005370210274122655\n",
      "\tbatch 460 loss 0.00028802690212614834\n",
      "\tbatch 470 loss 0.0020424663089215755\n",
      "\tbatch 480 loss 0.001434512552805245\n",
      "\tbatch 490 loss 0.007410417310893536\n",
      "\tbatch 500 loss 0.0012300099479034543\n",
      "\tbatch 510 loss 0.004293838050216436\n",
      "\tbatch 520 loss 0.006959910970181227\n",
      "\tbatch 530 loss 0.02054765447974205\n",
      "\tbatch 540 loss 0.039387188851833344\n",
      "\tbatch 550 loss 0.0015636506723240018\n",
      "\tbatch 560 loss 0.009299155324697495\n",
      "\tbatch 570 loss 0.008102240040898323\n",
      "\tbatch 580 loss 0.04611679166555405\n",
      "\tbatch 590 loss 0.0005532210343517363\n",
      "\tbatch 600 loss 0.0029665788169950247\n",
      "\tbatch 610 loss 0.006423419341444969\n",
      "\tbatch 620 loss 0.02627553604543209\n",
      "\tbatch 630 loss 0.017791319638490677\n",
      "\tbatch 640 loss 0.012808560393750668\n",
      "\tbatch 650 loss 0.003022395307198167\n",
      "\tbatch 660 loss 0.0006844336166977882\n",
      "\tbatch 670 loss 0.04437483847141266\n",
      "\tbatch 680 loss 0.01276757288724184\n",
      "\tbatch 690 loss 0.03641246259212494\n",
      "\tbatch 700 loss 0.002450627041980624\n",
      "\tbatch 710 loss 0.00129292206838727\n",
      "\tbatch 720 loss 0.007830686867237091\n",
      "\tbatch 730 loss 0.0005182159366086125\n",
      "\tbatch 740 loss 6.697662320220843e-05\n",
      "\tbatch 750 loss 0.14787699282169342\n",
      "\tbatch 760 loss 0.025150755420327187\n",
      "\tbatch 770 loss 0.0007911420543678105\n",
      "\tbatch 780 loss 0.0004988601431250572\n",
      "\tbatch 790 loss 0.0013068525586277246\n",
      "\tbatch 800 loss 0.007592092268168926\n",
      "\tbatch 810 loss 0.03031594306230545\n",
      "\tbatch 820 loss 0.03918856754899025\n",
      "\tbatch 830 loss 0.0006850698264315724\n",
      "\tbatch 840 loss 0.000231132420594804\n",
      "\tbatch 850 loss 0.08137551695108414\n",
      "\tbatch 860 loss 0.00020704889902845025\n",
      "\tbatch 870 loss 0.0009771105833351612\n",
      "\tbatch 880 loss 0.029971111565828323\n",
      "\tbatch 890 loss 0.00570977246388793\n",
      "\tbatch 900 loss 0.015791049227118492\n",
      "\tbatch 910 loss 0.0027602396439760923\n",
      "\tbatch 920 loss 0.008670471608638763\n",
      "\tbatch 930 loss 0.0002571166551206261\n",
      "\tbatch 940 loss 0.0008148355409502983\n",
      "\tbatch 950 loss 4.049112249049358e-05\n",
      "\tbatch 960 loss 0.0005019950331188738\n",
      "\tbatch 970 loss 0.06611725687980652\n",
      "\tbatch 980 loss 0.008412668481469154\n",
      "\tbatch 990 loss 0.007259779144078493\n",
      "\tbatch 1000 loss 1.7734188077156432e-05\n",
      "\tbatch 1010 loss 0.014444793574512005\n",
      "\tbatch 1020 loss 0.000367084052413702\n",
      "\tbatch 1030 loss 0.001830554916523397\n",
      "\tbatch 1040 loss 0.002292351797223091\n",
      "\tbatch 1050 loss 0.08342455327510834\n",
      "\tbatch 1060 loss 8.821379014989361e-05\n",
      "\tbatch 1070 loss 0.009741825982928276\n",
      "\tbatch 1080 loss 0.02388697676360607\n",
      "\tbatch 1090 loss 0.005843363702297211\n",
      "\tbatch 1100 loss 0.016106490045785904\n",
      "\tbatch 1110 loss 0.0001796837168512866\n",
      "\tbatch 1120 loss 0.0004714417445939034\n",
      "\tbatch 1130 loss 0.0017203539609909058\n",
      "\tbatch 1140 loss 0.050345998257398605\n",
      "\tbatch 1150 loss 0.0008453564951196313\n",
      "\tbatch 1160 loss 0.0005379518843255937\n",
      "\tbatch 1170 loss 0.061855070292949677\n",
      "\tbatch 1180 loss 0.026262391358613968\n",
      "\tbatch 1190 loss 0.00011124309821752831\n",
      "\tbatch 1200 loss 0.03997286036610603\n",
      "\tbatch 1210 loss 0.006486411206424236\n",
      "\tbatch 1220 loss 0.005300481803715229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 1230 loss 0.0038315104320645332\n",
      "\tbatch 1240 loss 0.0005506178713403642\n",
      "epoch 6 average_loss 0.013904853290579377\n",
      "\tbatch 0 loss 0.012121232226490974\n",
      "\tbatch 10 loss 0.033800408244132996\n",
      "\tbatch 20 loss 0.0069511570036411285\n",
      "\tbatch 30 loss 0.0002729026891756803\n",
      "\tbatch 40 loss 0.0007401747279800475\n",
      "\tbatch 50 loss 0.004256666637957096\n",
      "\tbatch 60 loss 0.0017091204645112157\n",
      "\tbatch 70 loss 0.0001689913624431938\n",
      "\tbatch 80 loss 0.005420614033937454\n",
      "\tbatch 90 loss 0.004984643775969744\n",
      "\tbatch 100 loss 0.016351763159036636\n",
      "\tbatch 110 loss 0.04360486939549446\n",
      "\tbatch 120 loss 0.008018516935408115\n",
      "\tbatch 130 loss 0.0004011509008705616\n",
      "\tbatch 140 loss 0.0035243243910372257\n",
      "\tbatch 150 loss 6.987035158090293e-05\n",
      "\tbatch 160 loss 0.00029327371157705784\n",
      "\tbatch 170 loss 0.005563891027122736\n",
      "\tbatch 180 loss 0.0074262553825974464\n",
      "\tbatch 190 loss 2.2267318854574114e-05\n",
      "\tbatch 200 loss 0.02960420772433281\n",
      "\tbatch 210 loss 0.00735767325386405\n",
      "\tbatch 220 loss 0.007310427259653807\n",
      "\tbatch 230 loss 0.0005112327053211629\n",
      "\tbatch 240 loss 0.002151674823835492\n",
      "\tbatch 250 loss 0.003335443791002035\n",
      "\tbatch 260 loss 0.0016869305400177836\n",
      "\tbatch 270 loss 0.00014394409663509578\n",
      "\tbatch 280 loss 0.0001980152737814933\n",
      "\tbatch 290 loss 0.0009389295009896159\n",
      "\tbatch 300 loss 0.019139278680086136\n",
      "\tbatch 310 loss 0.0011712339473888278\n",
      "\tbatch 320 loss 0.005684036761522293\n",
      "\tbatch 330 loss 0.00022429347154684365\n",
      "\tbatch 340 loss 0.015279280953109264\n",
      "\tbatch 350 loss 0.02282784692943096\n",
      "\tbatch 360 loss 0.0004184790013823658\n",
      "\tbatch 370 loss 0.028077224269509315\n",
      "\tbatch 380 loss 0.004853078629821539\n",
      "\tbatch 390 loss 0.06145201623439789\n",
      "\tbatch 400 loss 0.0007011992856860161\n",
      "\tbatch 410 loss 0.057073358446359634\n",
      "\tbatch 420 loss 0.004317466635257006\n",
      "\tbatch 430 loss 0.00167999102268368\n",
      "\tbatch 440 loss 0.001199142774567008\n",
      "\tbatch 450 loss 0.00015575405268464237\n",
      "\tbatch 460 loss 0.0014278603484854102\n",
      "\tbatch 470 loss 0.0005911877378821373\n",
      "\tbatch 480 loss 0.024339336901903152\n",
      "\tbatch 490 loss 0.005207274574786425\n",
      "\tbatch 500 loss 0.06670938432216644\n",
      "\tbatch 510 loss 0.020318957045674324\n",
      "\tbatch 520 loss 0.016654424369335175\n",
      "\tbatch 530 loss 0.09590618312358856\n",
      "\tbatch 540 loss 0.0005353703163564205\n",
      "\tbatch 550 loss 0.0013585895067080855\n",
      "\tbatch 560 loss 0.0017750696279108524\n",
      "\tbatch 570 loss 0.0064306301064789295\n",
      "\tbatch 580 loss 0.00015957499272190034\n",
      "\tbatch 590 loss 0.00015535710554104298\n",
      "\tbatch 600 loss 0.0009815329685807228\n",
      "\tbatch 610 loss 0.022044211626052856\n",
      "\tbatch 620 loss 0.0008045329013839364\n",
      "\tbatch 630 loss 0.0062780422158539295\n",
      "\tbatch 640 loss 0.004672604613006115\n",
      "\tbatch 650 loss 0.05970776826143265\n",
      "\tbatch 660 loss 0.03987586125731468\n",
      "\tbatch 670 loss 0.045998528599739075\n",
      "\tbatch 680 loss 0.0008375203469768167\n",
      "\tbatch 690 loss 0.001853552763350308\n",
      "\tbatch 700 loss 0.0011517577804625034\n",
      "\tbatch 710 loss 0.000972202280536294\n",
      "\tbatch 720 loss 0.0008621260058134794\n",
      "\tbatch 730 loss 0.007061721757054329\n",
      "\tbatch 740 loss 0.0023291693069040775\n",
      "\tbatch 750 loss 0.0012697306228801608\n",
      "\tbatch 760 loss 0.0008296641753986478\n",
      "\tbatch 770 loss 0.00018919762806035578\n",
      "\tbatch 780 loss 0.022331805899739265\n",
      "\tbatch 790 loss 0.0007121009402908385\n",
      "\tbatch 800 loss 0.008431034162640572\n",
      "\tbatch 810 loss 0.04810985177755356\n",
      "\tbatch 820 loss 0.005455581471323967\n",
      "\tbatch 830 loss 0.003083920804783702\n",
      "\tbatch 840 loss 0.001717925420962274\n",
      "\tbatch 850 loss 0.012020625174045563\n",
      "\tbatch 860 loss 0.000362765887985006\n",
      "\tbatch 870 loss 0.000915052427444607\n",
      "\tbatch 880 loss 0.0010723633458837867\n",
      "\tbatch 890 loss 0.004402526654303074\n",
      "\tbatch 900 loss 0.016065655276179314\n",
      "\tbatch 910 loss 0.001378614455461502\n",
      "\tbatch 920 loss 0.0002674570423550904\n",
      "\tbatch 930 loss 0.04356750100851059\n",
      "\tbatch 940 loss 0.0006122589111328125\n",
      "\tbatch 950 loss 0.00015971918764989823\n",
      "\tbatch 960 loss 0.0051469337195158005\n",
      "\tbatch 970 loss 0.0015779566019773483\n",
      "\tbatch 980 loss 0.0003134711005259305\n",
      "\tbatch 990 loss 0.010750319808721542\n",
      "\tbatch 1000 loss 0.0003197489131707698\n",
      "\tbatch 1010 loss 0.0006025009788572788\n",
      "\tbatch 1020 loss 0.00011641932360362262\n",
      "\tbatch 1030 loss 0.0024870934430509806\n",
      "\tbatch 1040 loss 0.0014407499693334103\n",
      "\tbatch 1050 loss 0.0019303926965221763\n",
      "\tbatch 1060 loss 8.584257011534646e-05\n",
      "\tbatch 1070 loss 0.001207180437631905\n",
      "\tbatch 1080 loss 0.003720176639035344\n",
      "\tbatch 1090 loss 0.0003870739892590791\n",
      "\tbatch 1100 loss 0.013764607720077038\n",
      "\tbatch 1110 loss 7.599216132803122e-06\n",
      "\tbatch 1120 loss 0.00041052306187339127\n",
      "\tbatch 1130 loss 0.020444396883249283\n",
      "\tbatch 1140 loss 0.06862761080265045\n",
      "\tbatch 1150 loss 0.0003292752371635288\n",
      "\tbatch 1160 loss 0.001153104822151363\n",
      "\tbatch 1170 loss 0.003609211416915059\n",
      "\tbatch 1180 loss 0.035862334072589874\n",
      "\tbatch 1190 loss 0.03354404866695404\n",
      "\tbatch 1200 loss 0.008567560464143753\n",
      "\tbatch 1210 loss 0.00383997755125165\n",
      "\tbatch 1220 loss 0.004698796197772026\n",
      "\tbatch 1230 loss 0.011381849646568298\n",
      "\tbatch 1240 loss 0.01640165038406849\n",
      "epoch 7 average_loss 0.013303300033476989\n",
      "\tbatch 0 loss 0.001981184585019946\n",
      "\tbatch 10 loss 0.009655307047069073\n",
      "\tbatch 20 loss 0.0008091520285233855\n",
      "\tbatch 30 loss 0.0025476699229329824\n",
      "\tbatch 40 loss 0.03155005723237991\n",
      "\tbatch 50 loss 0.0002699399192351848\n",
      "\tbatch 60 loss 0.0006440300494432449\n",
      "\tbatch 70 loss 0.00315773393958807\n",
      "\tbatch 80 loss 0.005110304802656174\n",
      "\tbatch 90 loss 0.0005658812588080764\n",
      "\tbatch 100 loss 0.0025864511262625456\n",
      "\tbatch 110 loss 0.04578359052538872\n",
      "\tbatch 120 loss 0.003297714050859213\n",
      "\tbatch 130 loss 3.753606142709032e-05\n",
      "\tbatch 140 loss 0.0008927450980991125\n",
      "\tbatch 150 loss 0.06121639907360077\n",
      "\tbatch 160 loss 0.004046676214784384\n",
      "\tbatch 170 loss 0.0013449619291350245\n",
      "\tbatch 180 loss 0.020710619166493416\n",
      "\tbatch 190 loss 0.00945307593792677\n",
      "\tbatch 200 loss 0.056816328316926956\n",
      "\tbatch 210 loss 0.001803736318834126\n",
      "\tbatch 220 loss 6.0676753491861746e-05\n",
      "\tbatch 230 loss 0.012225482612848282\n",
      "\tbatch 240 loss 0.0014487247681245208\n",
      "\tbatch 250 loss 0.00032586566521786153\n",
      "\tbatch 260 loss 0.04112435504794121\n",
      "\tbatch 270 loss 0.0029056754428893328\n",
      "\tbatch 280 loss 5.1317878387635574e-05\n",
      "\tbatch 290 loss 0.0005505273002199829\n",
      "\tbatch 300 loss 0.020907670259475708\n",
      "\tbatch 310 loss 0.00680603738874197\n",
      "\tbatch 320 loss 0.010002972558140755\n",
      "\tbatch 330 loss 0.006088486406952143\n",
      "\tbatch 340 loss 0.17542442679405212\n",
      "\tbatch 350 loss 0.044524844735860825\n",
      "\tbatch 360 loss 0.0005775376921519637\n",
      "\tbatch 370 loss 0.05407491326332092\n",
      "\tbatch 380 loss 0.0009089555242098868\n",
      "\tbatch 390 loss 0.0048519703559577465\n",
      "\tbatch 400 loss 0.0038234323728829622\n",
      "\tbatch 410 loss 0.03861750662326813\n",
      "\tbatch 420 loss 0.014069785363972187\n",
      "\tbatch 430 loss 0.002004652749747038\n",
      "\tbatch 440 loss 0.00014840572839602828\n",
      "\tbatch 450 loss 0.0009876486146822572\n",
      "\tbatch 460 loss 0.001295431749895215\n",
      "\tbatch 470 loss 0.027169954031705856\n",
      "\tbatch 480 loss 0.0018436252139508724\n",
      "\tbatch 490 loss 0.0009414439555257559\n",
      "\tbatch 500 loss 0.0009257913916371763\n",
      "\tbatch 510 loss 0.012620680965483189\n",
      "\tbatch 520 loss 0.07623258978128433\n",
      "\tbatch 530 loss 0.0005680857575498521\n",
      "\tbatch 540 loss 0.004759429022669792\n",
      "\tbatch 550 loss 0.0002305283269379288\n",
      "\tbatch 560 loss 0.07402089238166809\n",
      "\tbatch 570 loss 0.00019591348245739937\n",
      "\tbatch 580 loss 0.0018681911751627922\n",
      "\tbatch 590 loss 0.0007602194091305137\n",
      "\tbatch 600 loss 0.004184134304523468\n",
      "\tbatch 610 loss 0.03031127341091633\n",
      "\tbatch 620 loss 0.05060303956270218\n",
      "\tbatch 630 loss 0.006795137654989958\n",
      "\tbatch 640 loss 0.032610245048999786\n",
      "\tbatch 650 loss 0.0006868597702123225\n",
      "\tbatch 660 loss 0.00793488696217537\n",
      "\tbatch 670 loss 0.02052134834229946\n",
      "\tbatch 680 loss 0.0008381731458939612\n",
      "\tbatch 690 loss 0.00019662776321638376\n",
      "\tbatch 700 loss 0.008097550831735134\n",
      "\tbatch 710 loss 0.0002625758061185479\n",
      "\tbatch 720 loss 0.0001063570671249181\n",
      "\tbatch 730 loss 0.0034113319125026464\n",
      "\tbatch 740 loss 0.010552342981100082\n",
      "\tbatch 750 loss 0.0014064050046727061\n",
      "\tbatch 760 loss 0.0008740808116272092\n",
      "\tbatch 770 loss 1.8255417671753094e-05\n",
      "\tbatch 780 loss 0.0005491909687407315\n",
      "\tbatch 790 loss 0.002491801744326949\n",
      "\tbatch 800 loss 0.0037480066530406475\n",
      "\tbatch 810 loss 0.0005059833638370037\n",
      "\tbatch 820 loss 0.0025954588782042265\n",
      "\tbatch 830 loss 0.001263958984054625\n",
      "\tbatch 840 loss 4.103780520381406e-05\n",
      "\tbatch 850 loss 0.036279331892728806\n",
      "\tbatch 860 loss 5.3581345127895474e-05\n",
      "\tbatch 870 loss 0.000186805467819795\n",
      "\tbatch 880 loss 0.014272810891270638\n",
      "\tbatch 890 loss 0.00043630332220345736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 900 loss 0.0032595344819128513\n",
      "\tbatch 910 loss 0.00010529771680012345\n",
      "\tbatch 920 loss 0.00021013298828620464\n",
      "\tbatch 930 loss 0.025810452178120613\n",
      "\tbatch 940 loss 0.0007019496406428516\n",
      "\tbatch 950 loss 0.012208775617182255\n",
      "\tbatch 960 loss 0.002325819805264473\n",
      "\tbatch 970 loss 0.0005464724963530898\n",
      "\tbatch 980 loss 0.00039145926712080836\n",
      "\tbatch 990 loss 0.006105025298893452\n",
      "\tbatch 1000 loss 0.00017453008331358433\n",
      "\tbatch 1010 loss 0.0013544878456741571\n",
      "\tbatch 1020 loss 0.000144765610457398\n",
      "\tbatch 1030 loss 0.0005856896168552339\n",
      "\tbatch 1040 loss 0.025576164945960045\n",
      "\tbatch 1050 loss 0.02508794330060482\n",
      "\tbatch 1060 loss 0.0013669382315129042\n",
      "\tbatch 1070 loss 0.0024635917507112026\n",
      "\tbatch 1080 loss 0.021544765681028366\n",
      "\tbatch 1090 loss 0.0037093700375407934\n",
      "\tbatch 1100 loss 0.0013096362818032503\n",
      "\tbatch 1110 loss 5.8331275795353577e-05\n",
      "\tbatch 1120 loss 0.00028152886079624295\n",
      "\tbatch 1130 loss 0.0004021920030936599\n",
      "\tbatch 1140 loss 0.04061605781316757\n",
      "\tbatch 1150 loss 0.0011650598607957363\n",
      "\tbatch 1160 loss 0.021825410425662994\n",
      "\tbatch 1170 loss 0.02031014859676361\n",
      "\tbatch 1180 loss 0.00044079931103624403\n",
      "\tbatch 1190 loss 0.001584428595378995\n",
      "\tbatch 1200 loss 0.012595108710229397\n",
      "\tbatch 1210 loss 0.010928058996796608\n",
      "\tbatch 1220 loss 0.002508173231035471\n",
      "\tbatch 1230 loss 0.001125493785366416\n",
      "\tbatch 1240 loss 0.0014055394567549229\n",
      "epoch 8 average_loss 0.011627219778916334\n",
      "\tbatch 0 loss 0.012652523815631866\n",
      "\tbatch 10 loss 0.015324480831623077\n",
      "\tbatch 20 loss 0.0077003007754683495\n",
      "\tbatch 30 loss 0.010056453756988049\n",
      "\tbatch 40 loss 0.02698778361082077\n",
      "\tbatch 50 loss 0.0006279076915234327\n",
      "\tbatch 60 loss 0.0019375287229195237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11416/779682806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmit_train_x_ims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmy_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0minput_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmit_train_x_ims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meff_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmit_train_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eff_model.to(device)\n",
    "#ce_loss = torch.nn.CrossEntropyLoss(torch.tensor([1,10.0,10.0,10,10]).to(device))\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "my_opt = torch.optim.Adam(eff_model.parameters())\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "for epoch_index in range(num_epochs):\n",
    "    train_loss_sum = 0\n",
    "    train_loss_weight = 0\n",
    "    for batch_index in range(0,mit_train_x_ims.shape[0],batch_size):\n",
    "        my_opt.zero_grad()\n",
    "        input_batch = torch.tensor(mit_train_x_ims[batch_index:batch_index+batch_size][:,np.newaxis,:,:].astype(np.float32)).to(device)\n",
    "        pred = eff_model(input_batch)\n",
    "        target = torch.tensor(mit_train_y[batch_index:batch_index+batch_size].astype(np.int64)).to(device)\n",
    "        batch_loss = ce_loss(pred,target)\n",
    "        batch_loss.backward()\n",
    "        my_opt.step()\n",
    "        if batch_index//batch_size % 10 == 0:\n",
    "            print(\"\\tbatch {} loss {}\".format(batch_index//batch_size,batch_loss))\n",
    "        train_loss_sum += batch_loss.detach().cpu().numpy() * mit_train_x_ims[batch_index:batch_index+batch_size].shape[0]\n",
    "        train_loss_weight += mit_train_x_ims[batch_index:batch_index+batch_size].shape[0]\n",
    "    print(\"epoch {} average_loss {}\".format(epoch_index, train_loss_sum/train_loss_weight))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bfbc854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10253"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_batch = input_batch.cpu()\n",
    "#pred = pred.cpu()\n",
    "#target = target.cpu()\n",
    "#batch_loss = batch_loss.cpu()\n",
    "del input_batch,pred,target,batch_loss\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07d5aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "batch 10\n",
      "batch 20\n",
      "batch 30\n",
      "batch 40\n",
      "batch 50\n",
      "batch 60\n",
      "batch 70\n",
      "batch 80\n",
      "batch 90\n",
      "batch 100\n",
      "batch 110\n",
      "batch 120\n",
      "batch 130\n",
      "batch 140\n",
      "batch 150\n",
      "batch 160\n",
      "batch 170\n",
      "batch 180\n",
      "batch 190\n",
      "batch 200\n",
      "batch 210\n",
      "batch 220\n",
      "batch 230\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "eff_model.to(device)\n",
    "test_preds = []\n",
    "batch_size = 32\n",
    "for batch_index in range(0,mit_val_x.shape[0],batch_size):\n",
    "    input_batch = torch.tensor(mit_val_x_ims[batch_index:batch_index+batch_size][:,np.newaxis,:,:].astype(np.float32)).to(device)\n",
    "    test_pred = eff_model(input_batch)\n",
    "    test_preds.append(test_pred.detach().cpu().numpy())\n",
    "    if batch_index // batch_size % 10 == 0:\n",
    "        print(\"batch {}\".format(batch_index//batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15e1dd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "550cfa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_cat = test_preds[0]\n",
    "for i in range(1,len(test_preds)):\n",
    "    test_pred_cat = np.concatenate((test_pred_cat,test_preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df32a4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7554, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14a7a3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       4, 0, 0, 0, 0, 4, 4, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 4, 0, 2, 4, 0,\n",
       "       0, 0, 0, 2, 0, 0, 2, 0, 3, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n",
       "       0, 0, 0, 0, 3, 2, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 4, 0, 2, 0, 0, 0,\n",
       "       1, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 4, 2, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 3, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "       0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 3, 0, 2,\n",
       "       4, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0,\n",
       "       0, 0, 0, 2, 0, 2, 4, 0, 1, 0, 0, 0, 4, 0, 1, 0, 0, 3, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 3, 0, 2, 1, 0, 0, 4,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3,\n",
       "       0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 4, 0, 0, 0, 0, 0, 4, 0, 0, 2, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 4, 0, 0, 2,\n",
       "       0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 3, 2, 0, 1, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 4, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 4, 1, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 4, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0,\n",
       "       0, 2, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0, 0, 1,\n",
       "       0, 0, 0, 4, 0, 0, 3, 0, 0, 4, 0, 1, 0, 1, 0, 0, 3, 0, 0, 4, 0, 0,\n",
       "       0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0,\n",
       "       0, 1, 4, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 4, 0, 0, 0, 0, 3, 2, 0, 2, 0, 2, 0, 0, 0, 1, 0, 4, 4, 0, 4,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 3, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 2, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 3, 0,\n",
       "       0, 2, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 2, 0,\n",
       "       4, 0, 2, 0, 0, 0, 2, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 4, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 3,\n",
       "       0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "       4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0,\n",
       "       0, 4, 3, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 4, 0, 0, 0, 0,\n",
       "       1, 4, 0, 3, 0, 0, 3, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 4, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 4, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_cat.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a6b3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e816ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[751,  33,   5,  42,  10],\n",
       "       [  6,  14,   0,   1,   0],\n",
       "       [  9,   1,  56,   0,   2],\n",
       "       [  1,   0,   0,   5,   0],\n",
       "       [ 10,   0,   1,   0,  53]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(mit_train_y_test,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8a79dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(eff_model.state_dict(),\"init_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f561eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6102,  100,   40,    2,   13],\n",
       "       [ 160,   19,   10,    0,    1],\n",
       "       [ 400,   12,   48,    3,    3],\n",
       "       [  52,    1,    7,    0,    0],\n",
       "       [ 487,   18,   23,    0,   53]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e225ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8236695790309769"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86aeb108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6227,   20,    5,    1,    4],\n",
       "       [ 169,   19,    2,    0,    0],\n",
       "       [ 375,    7,   62,   12,   10],\n",
       "       [  49,    1,    4,    6,    0],\n",
       "       [ 548,   11,    4,    0,   18]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reg cross entropy\n",
    "sklearn.metrics.confusion_matrix(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2729393b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8382314005824728"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d61cd175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6210,    9,   28,    3,    7],\n",
       "       [ 170,   16,    2,    1,    1],\n",
       "       [ 405,    1,   49,    9,    2],\n",
       "       [  54,    0,    1,    5,    0],\n",
       "       [ 563,    0,    2,    0,   16]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reg cross entropy for 18 epochs\n",
    "sklearn.metrics.confusion_matrix(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc955d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97370cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833465713529256"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "863b9dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6165,   43,   28,    6,   15],\n",
       "       [ 150,   24,   14,    0,    2],\n",
       "       [ 401,    8,   55,    0,    2],\n",
       "       [  51,    0,    8,    1,    0],\n",
       "       [ 553,    1,   10,    0,   17]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reg cross entropy 10 epochs no pretraining\n",
    "sklearn.metrics.confusion_matrix(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c573e451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8289647868678846"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(mit_val_y,test_pred_cat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42ecf0ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7016/2686397780.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af0cf7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6165   43   28    6   15]\n",
      " [ 150   24   14    0    2]\n",
      " [ 401    8   55    0    2]\n",
      " [  51    0    8    1    0]\n",
      " [ 553    1   10    0   17]]\n",
      "accuracy: 0.8289647868678846\n",
      "precision: 0.45026856461657827\n",
      "recall: 0.2551129143797956\n",
      "f1: 0.2725778921950314\n"
     ]
    }
   ],
   "source": [
    "average_metric = 'macro'\n",
    "true = mit_val_y\n",
    "pred = test_pred_cat.argmax(1)\n",
    "print(sklearn.metrics.confusion_matrix(y_true=true, y_pred=pred))\n",
    "print(f\"accuracy: {sklearn.metrics.accuracy_score(true, pred)}\")\n",
    "print(f\"precision: {sklearn.metrics.precision_score(true, pred, average=average_metric)}\")\n",
    "print(f\"recall: {sklearn.metrics.recall_score(true, pred, average=average_metric)}\")\n",
    "print(f\"f1: {sklearn.metrics.f1_score(true, pred, average=average_metric)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "934997ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(eff_model.state_dict(),\"model_80000train_10epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "263daa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(eff_model.state_dict(),\"model_80000train_10epoch_celoss.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a2a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(eff_model.state_dict(),\"model_80000train_18epoch_celoss.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4326b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(eff_model.state_dict(),\"model_80000train_10epoch_celoss_nopretrain.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9367fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79df24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_test = np.genfromtxt(\"Data/kaggle/mitbih_test.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38aac406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21892, 187), (21892,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mit_test_x = mit_test[:,:-1]\n",
    "mit_test_y = mit_test[:,-1]\n",
    "mit_test_x.shape,mit_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6a66711",
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_test_x_ims = gaftransformer.transform(mit_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff3832a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "batch 10\n",
      "batch 20\n",
      "batch 30\n",
      "batch 40\n",
      "batch 50\n",
      "batch 60\n",
      "batch 70\n",
      "batch 80\n",
      "batch 90\n",
      "batch 100\n",
      "batch 110\n",
      "batch 120\n",
      "batch 130\n",
      "batch 140\n",
      "batch 150\n",
      "batch 160\n",
      "batch 170\n",
      "batch 180\n",
      "batch 190\n",
      "batch 200\n",
      "batch 210\n",
      "batch 220\n",
      "batch 230\n",
      "batch 240\n",
      "batch 250\n",
      "batch 260\n",
      "batch 270\n",
      "batch 280\n",
      "batch 290\n",
      "batch 300\n",
      "batch 310\n",
      "batch 320\n",
      "batch 330\n",
      "batch 340\n",
      "batch 350\n",
      "batch 360\n",
      "batch 370\n",
      "batch 380\n",
      "batch 390\n",
      "batch 400\n",
      "batch 410\n",
      "batch 420\n",
      "batch 430\n",
      "batch 440\n",
      "batch 450\n",
      "batch 460\n",
      "batch 470\n",
      "batch 480\n",
      "batch 490\n",
      "batch 500\n",
      "batch 510\n",
      "batch 520\n",
      "batch 530\n",
      "batch 540\n",
      "batch 550\n",
      "batch 560\n",
      "batch 570\n",
      "batch 580\n",
      "batch 590\n",
      "batch 600\n",
      "batch 610\n",
      "batch 620\n",
      "batch 630\n",
      "batch 640\n",
      "batch 650\n",
      "batch 660\n",
      "batch 670\n",
      "batch 680\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "eff_model.to(device)\n",
    "test_preds = []\n",
    "batch_size = 32\n",
    "for batch_index in range(0,mit_test_x.shape[0],batch_size):\n",
    "    input_batch = torch.tensor(mit_test_x_ims[batch_index:batch_index+batch_size][:,np.newaxis,:,:].astype(np.float32)).to(device)\n",
    "    test_pred = eff_model(input_batch)\n",
    "    test_preds.append(test_pred.detach().cpu().numpy())\n",
    "    if batch_index // batch_size % 10 == 0:\n",
    "        print(\"batch {}\".format(batch_index//batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df9ee633",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_cat = test_preds[0]\n",
    "for i in range(1,len(test_preds)):\n",
    "    test_pred_cat = np.concatenate((test_pred_cat,test_preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d42bfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17847   112    91    10    58]\n",
      " [  453    72    29     0     2]\n",
      " [ 1232    36   171     1     8]\n",
      " [  151     0     9     2     0]\n",
      " [ 1546     3    15     0    44]]\n",
      "accuracy: 0.8284304768865339\n",
      "precision: 0.45062400348066767\n",
      "recall: 0.2544683375587347\n",
      "f1: 0.27200384129832245\n"
     ]
    }
   ],
   "source": [
    "#no pretraining\n",
    "average_metric = 'macro'\n",
    "true = mit_test_y\n",
    "pred = test_pred_cat.argmax(1)\n",
    "print(sklearn.metrics.confusion_matrix(y_true=true, y_pred=pred))\n",
    "print(f\"accuracy: {sklearn.metrics.accuracy_score(true, pred)}\")\n",
    "print(f\"precision: {sklearn.metrics.precision_score(true, pred, average=average_metric)}\")\n",
    "print(f\"recall: {sklearn.metrics.recall_score(true, pred, average=average_metric)}\")\n",
    "print(f\"f1: {sklearn.metrics.f1_score(true, pred, average=average_metric)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "261d111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15882  1139   385   421   291]\n",
      " [  445    71    21    14     5]\n",
      " [ 1140   104   147    25    32]\n",
      " [  120     8    19    14     1]\n",
      " [ 1328    90    49    26   115]]\n",
      "accuracy: 0.7413210305134296\n",
      "precision: 0.2827316683104611\n",
      "recall: 0.2527482329004428\n",
      "f1: 0.2452710874763005\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "#no pretraining weighted ce\n",
    "average_metric = 'macro'\n",
    "true = mit_test_y\n",
    "pred = test_pred_cat.argmax(1)\n",
    "print(sklearn.metrics.confusion_matrix(y_true=true, y_pred=pred))\n",
    "print(f\"accuracy: {sklearn.metrics.accuracy_score(true, pred)}\")\n",
    "print(f\"precision: {sklearn.metrics.precision_score(true, pred, average=average_metric)}\")\n",
    "print(f\"recall: {sklearn.metrics.recall_score(true, pred, average=average_metric)}\")\n",
    "print(f\"f1: {sklearn.metrics.f1_score(true, pred, average=average_metric)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f57c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
